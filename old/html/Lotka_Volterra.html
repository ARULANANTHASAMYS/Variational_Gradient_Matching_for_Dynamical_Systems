
<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   <!--
This HTML was auto-generated from MATLAB code.
To make changes, update the MATLAB code and republish this document.
      --><title>Variational Gradient Matching for Dynamical Systems: Lotka-Volterra</title><meta name="generator" content="MATLAB 9.2"><link rel="schema.DC" href="http://purl.org/dc/elements/1.1/"><meta name="DC.date" content="2018-04-13"><meta name="DC.source" content="Lotka_Volterra.m"><style type="text/css">
html,body,div,span,applet,object,iframe,h1,h2,h3,h4,h5,h6,p,blockquote,pre,a,abbr,acronym,address,big,cite,code,del,dfn,em,font,img,ins,kbd,q,s,samp,small,strike,strong,sub,sup,tt,var,b,u,i,center,dl,dt,dd,ol,ul,li,fieldset,form,label,legend,table,caption,tbody,tfoot,thead,tr,th,td{margin:0;padding:0;border:0;outline:0;font-size:100%;vertical-align:baseline;background:transparent}body{line-height:1}ol,ul{list-style:none}blockquote,q{quotes:none}blockquote:before,blockquote:after,q:before,q:after{content:'';content:none}:focus{outine:0}ins{text-decoration:none}del{text-decoration:line-through}table{border-collapse:collapse;border-spacing:0}

html { min-height:100%; margin-bottom:1px; }
html body { height:100%; margin:0px; font-family:Arial, Helvetica, sans-serif; font-size:10px; color:#000; line-height:140%; background:#fff none; overflow-y:scroll; }
html body td { vertical-align:top; text-align:left; }

h1 { padding:0px; margin:0px 0px 25px; font-family:Arial, Helvetica, sans-serif; font-size:1.5em; color:#d55000; line-height:100%; font-weight:normal; }
h2 { padding:0px; margin:0px 0px 8px; font-family:Arial, Helvetica, sans-serif; font-size:1.2em; color:#000; font-weight:bold; line-height:140%; border-bottom:1px solid #d6d4d4; display:block; }
h3 { padding:0px; margin:0px 0px 5px; font-family:Arial, Helvetica, sans-serif; font-size:1.1em; color:#000; font-weight:bold; line-height:140%; }

a { color:#005fce; text-decoration:none; }
a:hover { color:#005fce; text-decoration:underline; }
a:visited { color:#004aa0; text-decoration:none; }

p { padding:0px; margin:0px 0px 20px; }
img { padding:0px; margin:0px 0px 20px; border:none; }
p img, pre img, tt img, li img, h1 img, h2 img { margin-bottom:0px; } 

ul { padding:0px; margin:0px 0px 20px 23px; list-style:square; }
ul li { padding:0px; margin:0px 0px 7px 0px; }
ul li ul { padding:5px 0px 0px; margin:0px 0px 7px 23px; }
ul li ol li { list-style:decimal; }
ol { padding:0px; margin:0px 0px 20px 0px; list-style:decimal; }
ol li { padding:0px; margin:0px 0px 7px 23px; list-style-type:decimal; }
ol li ol { padding:5px 0px 0px; margin:0px 0px 7px 0px; }
ol li ol li { list-style-type:lower-alpha; }
ol li ul { padding-top:7px; }
ol li ul li { list-style:square; }

.content { font-size:1.2em; line-height:140%; padding: 20px; }

pre, code { font-size:12px; }
tt { font-size: 1.2em; }
pre { margin:0px 0px 20px; }
pre.codeinput { padding:10px; border:1px solid #d3d3d3; background:#f7f7f7; }
pre.codeoutput { padding:10px 11px; margin:0px 0px 20px; color:#4c4c4c; }
pre.error { color:red; }

@media print { pre.codeinput, pre.codeoutput { word-wrap:break-word; width:100%; } }

span.keyword { color:#0000FF }
span.comment { color:#228B22 }
span.string { color:#A020F0 }
span.untermstring { color:#B20000 }
span.syscmd { color:#B28C00 }

.footer { width:auto; padding:10px 0px; margin:25px 0px 0px; border-top:1px dotted #878787; font-size:0.8em; line-height:140%; font-style:italic; color:#878787; text-align:left; float:none; }
.footer p { margin:0px; }
.footer a { color:#878787; }
.footer a:hover { color:#878787; text-decoration:underline; }
.footer a:visited { color:#878787; }

table th { padding:7px 5px; text-align:left; vertical-align:middle; border: 1px solid #d6d4d4; font-weight:bold; }
table td { padding:7px 5px; text-align:left; vertical-align:top; border:1px solid #d6d4d4; }





  </style></head><body><div class="content"><h1>Variational Gradient Matching for Dynamical Systems: Lotka-Volterra</h1><!--introduction--><p><img vspace="5" hspace="5" src="cover_pic.png" alt=""> </p><p>Authors: <b>Nico Stephan Gorbach</b> and <b>Stefan Bauer</b>, email: <a href="mailto:nico.gorbach@gmail.com">nico.gorbach@gmail.com</a></p><p>Instructional code for the NIPS (2018) paper " <b>Scalable Variational Inference for Dynamical Systems</b> " by Nico S. Gorbach, Stefan Bauer and Joachim M. Buhmann. The paper is available at <a href="https://papers.nips.cc/paper/7066-scalable-variational-inference-for-dynamical-systems.pdf">https://papers.nips.cc/paper/7066-scalable-variational-inference-for-dynamical-systems.pdf</a>. Please cite our paper if you use our program for a further publication. Part of the derivation below is described in Wenk et al. (2018).</p><p>Example dynamical system used in this code: Lotka-Volterra system with <b>half</b> of the time points <b>unobserved</b>. The ODE parameters are also unobserved.</p><!--/introduction--><h2>Contents</h2><div><ul><li><a href="#1">Advantages of Variational Gradient Matching</a></li><li><a href="#4">Simulation Settings</a></li><li><a href="#5">User Input</a></li><li><a href="#11">Import ODEs</a></li><li><a href="#13">Mass Action Dynamical Systems</a></li><li><a href="#14">Simulate Trajectory Observations</a></li><li><a href="#19">Prior on States and State Derivatives</a></li><li><a href="#20">Matching Gradients</a></li><li><a href="#21">State Couplings in ODEs</a></li><li><a href="#22">Rewrite ODEs as Linear Combination in Parameters</a></li><li><a href="#23">Posterior over ODE Parameters</a></li><li><a href="#24">Rewrite ODEs as Linear Combination in Individual States</a></li><li><a href="#25">Posterior over Individual States</a></li><li><a href="#26">Mean-field Variational Inference</a></li><li><a href="#27">Fitting observations of state trajectories</a></li><li><a href="#28">Coordinate Ascent Variational Gradient Matching</a></li><li><a href="#33">Time Taken</a></li><li><a href="#34">References</a></li><li><a href="#35">Subroutines</a></li></ul></div><h2 id="1">Advantages of Variational Gradient Matching</h2><p>The essential idea of gradient matching (Calderhead et al., 2002) is to match the gradient governed by the ODEs with that inferred from the observations. In contrast to previous approaches gradient matching introduces a prior over states instead of a prior over ODE parameters. The advantages of gradients matching is two-fold:</p><div><ol><li>A prior over the functional form of state dynamics as opposed to ODE parameters facilitates a more expert-aware estimation of ODE parameters since experts can provide a better <i>a priori</i> description of state dynamics than ODE parameters.</li><li>Gradient matching yields a global gradient as opposed to a local one which offers significant computational advantages and provides access to a rich source of sophisticated optimization tools.</li></ol></div><p>Clear workspace and close figures</p><pre class="codeinput">clear <span class="string">all</span>; close <span class="string">all</span>;
</pre><h2 id="4">Simulation Settings</h2><pre class="codeinput">simulation.state_obs_variance = @(mean)(bsxfun(@times,[0.5^2,0.5^2],<span class="keyword">...</span>
    ones(size(mean))));                                                    <span class="comment">% observation noise</span>
simulation.ode_param = [2,1,4,1];                                          <span class="comment">% true ODE parameters [2 1 4 1] is used as a benchmark in many publications;</span>
simulation.final_time = 2;                                                 <span class="comment">% end time for integration</span>
simulation.int_interval = 0.01;                                            <span class="comment">% integration interval</span>
simulation.time_samp = 0:0.1:simulation.final_time;                        <span class="comment">% sample times for observations</span>
simulation.init_val = [5 3];                                               <span class="comment">% state values at first time point</span>
simulation.state_obs_idx = [1,1];                                          <span class="comment">% indices of states that are directly observed (Boolean)</span>
</pre><h2 id="5">User Input</h2><p><h4> Kernel </h4></p><p>Kernel parameters <img src="Lotka_Volterra_eq06401276552089372917.png" alt="$\phi$" style="width:6px;height:10px;">:</p><pre class="codeinput">kernel.param = [10,0.2];                                                   <span class="comment">% set values of rbf kernel parameters</span>
</pre><p>Error variance on state derivatives (i.e. <img src="Lotka_Volterra_eq17096441642737911057.png" alt="$\gamma$" style="width:6px;height:8px;">):</p><pre class="codeinput">state.derivative_variance = [6,6];                                         <span class="comment">% gamma for gradient matching model</span>
</pre><p><h4> Estimation times </h4></p><pre class="codeinput">time.est = 0:0.1:4;                                                        <span class="comment">% estimation times</span>
</pre><p><h4> Optimization settings </h4></p><pre class="codeinput">opt_settings.pseudo_inv_type = <span class="string">'Moore-Penrose'</span>;                            <span class="comment">% Type of pseudo inverse; options: 'Moore-Penrose' or 'modified Moore-Penrose'</span>
opt_settings.coord_ascent_numb_iter = 200;                                 <span class="comment">% number of coordinate ascent iterations</span>
opt_settings.clamp_obs_state_to_GP_fit = false;                            <span class="comment">% The observed state trajectories are clamped to the trajectories determined by standard GP regression (Boolean)</span>
</pre><p><h4> Symbols </h4></p><p>States <img src="Lotka_Volterra_eq08291690262771002032.png" alt="$\mathbf{x}$" style="width:7px;height:5px;">:</p><pre class="codeinput">symbols.state = {<span class="string">'[prey]'</span>,<span class="string">'[predator]'</span>};                                   <span class="comment">% symbols of states in 'ODEs.txt' file</span>
</pre><p>ODE parameters <img src="Lotka_Volterra_eq08288499342375314727.png" alt="$\theta$" style="width:5px;height:8px;">:</p><pre class="codeinput">symbols.param = {<span class="string">'[\theta_1]'</span>,<span class="string">'[\theta_2]'</span>,<span class="string">'[\theta_3]'</span>,<span class="string">'[\theta_4]'</span>};     <span class="comment">% symbols of parameters in 'ODEs.txt' file</span>
</pre><h2 id="11">Import ODEs</h2><pre class="codeinput">ode = import_odes(symbols);
</pre><pre class="codeinput">disp(<span class="string">'ODEs:'</span>); disp(ode.raw)
</pre><pre class="codeoutput">ODEs:
    '[\theta_1].*[prey] - [\theta_2].*[prey].*[predator]'
    '-[\theta_3].*[predator] + [\theta_4].*[prey].*[predator]'

</pre><h2 id="13">Mass Action Dynamical Systems</h2><p>A deterministic dynamical system is represented by a set of <img src="Lotka_Volterra_eq03845174387838694102.png" alt="$K$" style="width:10px;height:8px;"> ordinary differential equations (ODEs) with model parameters <img src="Lotka_Volterra_eq06597877416883810229.png" alt="$\theta \in R^d$" style="width:32px;height:10px;"> that describe the evolution of <img src="Lotka_Volterra_eq03845174387838694102.png" alt="$K$" style="width:10px;height:8px;"> states <img src="Lotka_Volterra_eq16690292594929342180.png" alt="$\mathbf{x}(t) = [x_1(t),\ldots, x_K(t)]^T$" style="width:120px;height:13px;"> such that:</p><p><img src="Lotka_Volterra_eq14568623149868123203.png" alt="$\dot{\mathbf{x}}(t) = \frac{d \mathbf{x}(t)}{d t} = \mathbf{f}(\mathbf{x}(t),\theta) \qquad (1)$" style="width:147px;height:16px;">.</p><p>A sequence of observations, <img src="Lotka_Volterra_eq17565748849429239454.png" alt="$\mathbf{y}(t)$" style="width:19px;height:11px;">, is usually contaminated by measurement error which we assume to be normally distributed with zero mean and variance for each of the <img src="Lotka_Volterra_eq03845174387838694102.png" alt="$K$" style="width:10px;height:8px;"> states, i.e. <img src="Lotka_Volterra_eq13983079546768783123.png" alt="$\mathbf{E}\sim \mathcal{N}(\mathbf{E};\mathbf{0},\mathbf{D})$" style="width:75px;height:12px;">, with <img src="Lotka_Volterra_eq02125912780065424198.png" alt="$\mathbf{D}_{ik}=\sigma_k ^2 \delta_{ik}$" style="width:55px;height:13px;">. For <img src="Lotka_Volterra_eq03672095713503266041.png" alt="$N$" style="width:10px;height:8px;"> distinct time points the overall system may therefore be summarized as:</p><p><img src="Lotka_Volterra_eq05988414051423361030.png" alt="$\mathbf{Y} = \mathbf{X} + \mathbf{E}$" style="width:56px;height:9px;">,</p><p>where</p><p><img src="Lotka_Volterra_eq15195809455161680973.png" alt="$\mathbf{X} = [\mathbf{x}(t_1),\ldots,\mathbf{x}(t_N)] = [\mathbf{x}_1,\ldots,\mathbf{x}_K]^T$" style="width:181px;height:13px;">,</p><p><img src="Lotka_Volterra_eq07638385370877036024.png" alt="$\mathbf{Y} = [\mathbf{y}(t_1),\ldots,\mathbf{y}(t_N)] = [\mathbf{y}_1,\ldots,\mathbf{y}_K]^T$" style="width:182px;height:13px;">,</p><p>and <img src="Lotka_Volterra_eq15380491105506292187.png" alt="$\mathbf{x}_k = [x_k(t_1),\ldots,x_k(t_N)]^T$" style="width:122px;height:13px;"> is the <img src="Lotka_Volterra_eq15636846968047188835.png" alt="$k$" style="width:6px;height:8px;">'th state sequence and <img src="Lotka_Volterra_eq10667114650506665719.png" alt="$\mathbf{y}_k = [y_k(t_1),$" style="width:59px;height:11px;"> <img src="Lotka_Volterra_eq11698417834938640281.png" alt="$\ldots,y_k(t_N)]^T$" style="width:58px;height:13px;"> are the observations. Given the observations <img src="Lotka_Volterra_eq00013651220649516337.png" alt="$\mathbf{Y}$" style="width:10px;height:8px;"> and the description of the dynamical system (1), the aim is to estimate both state variables <img src="Lotka_Volterra_eq03397130480831257552.png" alt="$\mathbf{X}$" style="width:9px;height:8px;"> and parameters <img src="Lotka_Volterra_eq08288499342375314727.png" alt="$\theta$" style="width:5px;height:8px;">.</p><p>We consider only dynamical systems that are locally linear with respect to ODE parameters <img src="Lotka_Volterra_eq08288499342375314727.png" alt="$\theta$" style="width:5px;height:8px;"> and individual states <img src="Lotka_Volterra_eq00398522576481313565.png" alt="$\mathbf{x}_u$" style="width:11px;height:7px;">. Such ODEs include mass-action kinetics and are given by:</p><p><img src="Lotka_Volterra_eq07850966414768814632.png" alt="$f_{k}(\mathbf{x}(t),\theta) = \sum_{i=1} \theta_{ki} \prod_{j \in \mathcal{M}_{ki}} x_j \qquad (2)$" style="width:185px;height:13px;">,</p><p>with <img src="Lotka_Volterra_eq10975547451157895511.png" alt="$\mathcal{M}_{ki} \subseteq \{ 1, \dots, K\}$" style="width:86px;height:11px;"> describing the state variables in each factor of the equation (i.e. the functions are linear in parameters and contain arbitrary large products of monomials of the states).</p><h2 id="14">Simulate Trajectory Observations</h2><p><h4> Generate ground truth by numerical integration </h4></p><pre class="codeinput">[state,time,ode] = generate_ground_truth(time,state,ode,symbols,simulation);
</pre><p><h4> Generate state observations </h4></p><pre class="codeinput">[state,time,obs_to_state_relation] = generate_state_obs(state,time,simulation);
</pre><p><h4> Symbols </h4></p><pre class="codeinput">state.sym.mean = sym(<span class="string">'x%d%d'</span>,[length(time.est),length(ode.system)]);
state.sym.variance = sym(<span class="string">'sigma%d%d'</span>,[length(time.est),length(ode.system)]);
ode_param.sym.mean = sym(<span class="string">'param%d'</span>,[length(symbols.param),1]); assume(ode_param.sym.mean,<span class="string">'real'</span>);
</pre><p><h4> Setup plots </h4></p><p>Only the state dynamics are (partially) observed.</p><pre class="codeinput">[h_states,h_param,p] = setup_plots(state,time,simulation,symbols);

tic; <span class="comment">%start timer</span>
</pre><h2 id="19">Prior on States and State Derivatives</h2><p>Gradient matching with Gaussian processes assumes a joint Gaussian process prior on states and their derivatives:</p><p><img src="Lotka_Volterra_eq01057761807000221839.png" alt="$\left(\begin{array}{c} \mathbf{X} \\ \dot{\mathbf{X}} \end{array}\right)  \sim \mathcal{N} \left( \begin{array}{c} \mathbf{X} \\ \dot{\mathbf{X}} \end{array}; \begin{array}{c}  \mathbf{0} \\ \mathbf{0}  \end{array}, \begin{array}{cc}  \mathbf{C}_{\phi} &amp; \mathbf{C}_{\phi}' \\  '\mathbf{C}_{\phi} &amp; \mathbf{C}_{\phi}''  \end{array}  \right) \qquad (3)$" style="width:216px;height:28px;">,</p><p><img src="Lotka_Volterra_eq15132385546029468189.png" alt="$\mathrm{cov}(x_k(t), x_k(t)) = C_{\phi_k}(t,t')$" style="width:131px;height:12px;"></p><p><img src="Lotka_Volterra_eq17058345339069568247.png" alt="$\mathrm{cov}(\dot{x}_k(t), x_k(t)) = \frac{\partial C_{\phi_k}(t,t') }{\partial t} =: C_{\phi_k}'(t,t')$" style="width:185px;height:18px;"></p><p><img src="Lotka_Volterra_eq08196297352138716370.png" alt="$\mathrm{cov}(x_k(t), \dot{x}_k(t)) = \frac{\partial C_{\phi_k}(t,t') }{\partial t'} =: {'C_{\phi_k}(t,t')}$" style="width:187px;height:17px;"></p><p><img src="Lotka_Volterra_eq02914213731374756582.png" alt="$\mathrm{cov}(\dot{x}_k(t), \dot{x}_k(t)) = \frac{\partial C_{\phi_k}(t,t') }{\partial t \partial t'} =: C_{\phi_k}''(t,t')$" style="width:185px;height:18px;">.</p><h2 id="20">Matching Gradients</h2><p>Given the joint distribution over states and their derivatives (3) as well as the ODEs (2), we therefore have two expressions for the state derivatives:</p><p><img src="Lotka_Volterra_eq06786540181160975064.png" alt="$\dot{\mathbf{X}} = \mathbf{F} + \epsilon_1, \epsilon_1 \sim \mathcal{N}\left(\epsilon_1;\mathbf{0}, \mathbf{I}\gamma \right)$" style="width:139px;height:13px;"></p><p><img src="Lotka_Volterra_eq00641387202370649337.png" alt="$\dot{\mathbf{X}} = {'\mathbf{C}_{\phi}} \mathbf{C}_{\phi}^{-1} ~\mathbf{X} + \epsilon_2, \epsilon_2 \sim \mathcal{N}\left(\epsilon_2;\mathbf{0}, \mathbf{A} \right)$" style="width:179px;height:16px;"></p><p>where <img src="Lotka_Volterra_eq15438906814634469781.png" alt="$\mathbf{F} := \mathbf{f}(\mathbf{X},\theta)$" style="width:58px;height:11px;">, <img src="Lotka_Volterra_eq02616189247499740046.png" alt="$\mathbf{A} := \mathbf{C}_{\phi}'' -  {'\mathbf{C}_{\phi}} \mathbf{C}_{\phi}^{-1} \mathbf{C}_{\phi}'$" style="width:105px;height:15px;"> and <img src="Lotka_Volterra_eq17096441642737911057.png" alt="$\gamma$" style="width:6px;height:8px;"> is the error variance in the ODEs. Note that, in a deterministic system, the output of the ODEs <img src="Lotka_Volterra_eq10805855639155619100.png" alt="$\mathbf{F}$" style="width:8px;height:8px;"> should equal the state derivatives <img src="Lotka_Volterra_eq07315144976585928416.png" alt="$\dot{\mathbf{X}}$" style="width:9px;height:11px;">. However, in the first equation above we relax this contraint by adding stochasticity to the state derivatives <img src="Lotka_Volterra_eq07315144976585928416.png" alt="$\dot{\mathbf{X}}$" style="width:9px;height:11px;"> in order to compensate for a potential model mismatch. The second equation above is obtained by deriving the conditional distribution for <img src="Lotka_Volterra_eq07315144976585928416.png" alt="$\dot{\mathbf{X}}$" style="width:9px;height:11px;"> from the joint distribution in equation (3). Equating the two expressions in the equations above we can eliminate the unknown state derivatives <img src="Lotka_Volterra_eq07315144976585928416.png" alt="$\dot{\mathbf{X}}$" style="width:9px;height:11px;">:</p><p><img src="Lotka_Volterra_eq16675609090530047740.png" alt="$\mathbf{F} = {'\mathbf{C}_{\phi}} \mathbf{C}_{\phi}^{-1} ~\mathbf{X} + \epsilon_0 \qquad (4)$" style="width:130px;height:15px;">,</p><p>with <img src="Lotka_Volterra_eq08944609086354323641.png" alt="$\epsilon_0 := \epsilon_2 - \epsilon_1$" style="width:57px;height:7px;">.</p><pre class="codeinput">[dC_times_invC,inv_C,A_plus_gamma_inv] = kernel_function(kernel,state,time.est);
</pre><h2 id="21">State Couplings in ODEs</h2><pre class="codeinput">coupling_idx = find_state_couplings_in_odes(ode,symbols);
</pre><h2 id="22">Rewrite ODEs as Linear Combination in Parameters</h2><p>We rewrite the ODEs in equation (2) as a linear combination in the parameters:</p><p><img src="Lotka_Volterra_eq16609810645451920803.png" alt="$\mathbf{B}_{\theta k} \theta + \mathbf{b}_{\theta k} \stackrel{!}{=} \mathbf{f}_k(\mathbf{X},\theta) \qquad (5)$" style="width:139px;height:15px;">,</p><p>where matrices <img src="Lotka_Volterra_eq09863200650503750796.png" alt="$\mathbf{B}_{\theta k}$" style="width:17px;height:10px;"> and <img src="Lotka_Volterra_eq05105382836198979729.png" alt="$\mathbf{b}_{\theta k}$" style="width:15px;height:10px;"> are defined such that the ODEs <img src="Lotka_Volterra_eq15737021493207115936.png" alt="$\mathbf{f}_k(\mathbf{X},\theta)$" style="width:36px;height:11px;"> are expressed as a linear combination in <img src="Lotka_Volterra_eq08288499342375314727.png" alt="$\theta$" style="width:5px;height:8px;">.</p><pre class="codeinput">[ode_param.lin_comb.B,ode_param.lin_comb.b] = rewrite_odes_as_linear_combination_in_parameters(ode,symbols);
</pre><h2 id="23">Posterior over ODE Parameters</h2><p>Inserting (5) into (4) and solving for <img src="Lotka_Volterra_eq08288499342375314727.png" alt="$\theta$" style="width:5px;height:8px;"> yields:</p><p><img src="Lotka_Volterra_eq05180355945512982290.png" alt="$\theta = \mathbf{B}_{\theta}^+ \left( {'\mathbf{C}_{\phi}} \mathbf{C}_{\phi}^{-1} \mathbf{X} - \mathbf{b}_{\theta} + \epsilon_0 \right)$" style="width:142px;height:20px;">,</p><p>where <img src="Lotka_Volterra_eq00263976522217680460.png" alt="$\mathbf{B}_{\theta}^+$" style="width:15px;height:13px;"> denotes the pseudo-inverse of <img src="Lotka_Volterra_eq14264513074604781781.png" alt="$\mathbf{B}_{\theta}$" style="width:13px;height:10px;">.</p><p>Since <img src="Lotka_Volterra_eq15121298810044302100.png" alt="$\mathbf{C}_{\phi}$" style="width:14px;height:11px;"> is block diagonal we can rewrite the expression above as:</p><p><img src="Lotka_Volterra_eq00688257075730191683.png" alt="$\theta = \left( \mathbf{B}_{\theta} \mathbf{B}_{\theta}^T \right)^{-1} \mathbf{B}_{\theta}^T  \left( \sum_k {'\mathbf{C}_{\phi_k}} \mathbf{C}_{\boldmath\phi_k}^{-1} \mathbf{X}_k - \mathbf{b}_{\theta k} + \boldmath\epsilon_0^{(k)} \right)$" style="width:230px;height:20px;">,</p><p><img src="Lotka_Volterra_eq09210081259629302610.png" alt="$= \left( \mathbf{B}_{\theta} \mathbf{B}_{\theta}^T \right)^{-1} \left( \sum_k \mathbf{B}_{\theta k}^T \left( {'\mathbf{C}_{\phi_k}} \mathbf{C}_{\boldmath\phi_k}^{-1} \mathbf{X}_k - \mathbf{b}_{\theta k} + \boldmath\epsilon_0^{(k)} \right) \right)$" style="width:237px;height:20px;">,</p><p>where we subsitute the Moore-Penrose inverse for the pseudo-inverse (i.e. <img src="Lotka_Volterra_eq12469254312080902996.png" alt="$\mathbf{B}_{\theta}^+ = \left( \mathbf{B}_{\theta} \mathbf{B}_{\theta}^T \right)^{-1} \mathbf{B}_{\theta}^T$" style="width:96px;height:16px;">).</p><p>We can therefore derive the posterior distribution over ODE parameters:</p><p><img src="Lotka_Volterra_eq16007501702514714848.png" alt="$p(\theta \mid \mathbf{X}, \phi, \gamma) = \mathcal{N}\left(\theta ; \mathbf{B}_{\theta}^+ ~ \left( {'\mathbf{C}_{\phi}} \mathbf{C}_{\phi}^{-1} \mathbf{X} - \mathbf{b}_{\theta} \right), ~ \mathbf{B}_{\theta}^+ ~ (\mathbf{A} + \mathbf{I}\gamma) ~ \mathbf{B}_{\theta}^{+T} \right)$" style="width:312px;height:20px;"></p><p><img src="Lotka_Volterra_eq13159839905936607294.png" alt="$= \mathcal{N}\left(\theta ; \left( \mathbf{B}_{\theta} \mathbf{B}_{\theta}^T \right)^{-1} \left( \sum_k \mathbf{B}_{\theta k}^T ~ \left( {'\mathbf{C}_{\phi k}} \mathbf{C}_{\phi k}^{-1} \mathbf{X}_k - \mathbf{b}_{\theta k} \right) \right), ~ \mathbf{B}_{\theta}^+ ~ (\mathbf{A} + \mathbf{I}\gamma) ~ \mathbf{B}_{\theta}^{+T} \right)$" style="width:345px;height:20px;"></p><p><img src="Lotka_Volterra_eq03198135326067574098.png" alt="$= \prod_k \mathcal{N}\left(\theta ; \left( \mathbf{B}_{\theta} \mathbf{B}_{\theta}^T \right)^{-1} \left( \mathbf{B}_{\theta k}^T ~ \left( {'\mathbf{C}_{\phi k}} \mathbf{C}_{\phi k}^{-1} \mathbf{X}_k - \mathbf{b}_{\theta k} \right) \right), ~ \mathbf{B}_{\theta k}^+ ~ (\mathbf{A}_k + \mathbf{I}\gamma) ~ \mathbf{B}_{\theta k}^{+T} \right) \qquad (6)$" style="width:390px;height:20px;"></p><h2 id="24">Rewrite ODEs as Linear Combination in Individual States</h2><p>We rewrite the expression <img src="Lotka_Volterra_eq17411081488619577558.png" alt="$\mathbf{f}(\mathbf{X},\theta) - {'\mathbf{C}}_{\phi} \mathbf{C}_{\phi}^{-1} \mathbf{X}$" style="width:93px;height:15px;"> in equation (4) as a linear combination in the individual state <img src="Lotka_Volterra_eq00398522576481313565.png" alt="$\mathbf{x}_u$" style="width:11px;height:7px;">:</p><p><img src="Lotka_Volterra_eq17696499692546749583.png" alt="$\mathbf{R}_{uk} \mathbf{x}_u + \mathbf{r}_{uk} \stackrel{!}{=} \mathbf{f}_k(\mathbf{X},\theta)$" style="width:109px;height:15px;">.</p><p>where matrices <img src="Lotka_Volterra_eq06228382002088498691.png" alt="$\mathbf{R}_{uk}$" style="width:18px;height:10px;"> and <img src="Lotka_Volterra_eq02299362643576640155.png" alt="$\mathbf{r}_{uk}$" style="width:14px;height:7px;"> are defined such that the ODE <img src="Lotka_Volterra_eq15737021493207115936.png" alt="$\mathbf{f}_k(\mathbf{X},\theta)$" style="width:36px;height:11px;"> is expressed as a linear combination in the individual state <img src="Lotka_Volterra_eq00398522576481313565.png" alt="$\mathbf{x}_u$" style="width:11px;height:7px;">.</p><pre class="codeinput">[state.lin_comb.R,state.lin_comb.r] = rewrite_odes_as_linear_combination_in_ind_states(ode,symbols,coupling_idx.states);
</pre><h2 id="25">Posterior over Individual States</h2><p>Given the linear combination of the ODEs w.r.t. an individual state, we define the matrices <img src="Lotka_Volterra_eq11780124863314593273.png" alt="$\mathbf{B}_u$" style="width:13px;height:10px;"> and <img src="Lotka_Volterra_eq15590211095018081680.png" alt="$\mathbf{b}_u$" style="width:11px;height:10px;"> such that the expression <img src="Lotka_Volterra_eq17411081488619577558.png" alt="$\mathbf{f}(\mathbf{X},\theta) - {'\mathbf{C}}_{\phi} \mathbf{C}_{\phi}^{-1} \mathbf{X}$" style="width:93px;height:15px;"> is rewritten as a linear combination in an individual state:</p><p><img src="Lotka_Volterra_eq15637579348524886787.png" alt="$\mathbf{B}_{u} \mathbf{x}_u + \mathbf{b}_{u} \stackrel{!}{=} \mathbf{f}(\mathbf{X},\theta) \qquad (7)$" style="width:134px;height:15px;">.</p><p>Inserting (7) into (4) and solving for <img src="Lotka_Volterra_eq00398522576481313565.png" alt="$\mathbf{x}_u$" style="width:11px;height:7px;"> yields:</p><p><img src="Lotka_Volterra_eq18427597317046955935.png" alt="$\mathbf{x}_u = \mathbf{B}_{u}^+ \left( \epsilon_0 -\mathbf{b}_{u} \right)$" style="width:86px;height:12px;">,</p><p>Since <img src="Lotka_Volterra_eq15121298810044302100.png" alt="$\mathbf{C}_{\phi}$" style="width:14px;height:11px;"> is block diagonal we can rewrite the expression above as:</p><p><img src="Lotka_Volterra_eq16753383490539703582.png" alt="$\mathbf{x}_u = \left( \mathbf{B}_{u} \mathbf{B}_{u}^T \right)^{-1} \mathbf{B}_{u}^T \sum_k \left(\epsilon_0^{(k)} -\mathbf{b}_{uk} \right)$" style="width:169px;height:20px;"></p><p><img src="Lotka_Volterra_eq01015579618441042371.png" alt="$= \left( \mathbf{B}_{u} \mathbf{B}_{u}^T \right)^{-1} \sum_k \mathbf{B}_{uk}^T \left(\epsilon_0^{(k)} -\mathbf{b}_{uk} \right)$" style="width:157px;height:20px;">,</p><p>where <img src="Lotka_Volterra_eq03266229480980241917.png" alt="$\mathbf{B}_{u}^+$" style="width:15px;height:12px;"> denotes the pseudo-inverse of <img src="Lotka_Volterra_eq11755194738169897586.png" alt="$\mathbf{B}_{u}$" style="width:13px;height:10px;">. We can therefore derive the posterior distribution over an individual state <img src="Lotka_Volterra_eq00398522576481313565.png" alt="$\mathbf{x}_u$" style="width:11px;height:7px;">:</p><p><img src="Lotka_Volterra_eq16518861028658924937.png" alt="$p(\mathbf{x}_u \mid \mathbf{X}_{-u}, \phi, \gamma) = \mathcal{N}\left(\mathbf{x}_u ; -\mathbf{B}_{u}^+ \mathbf{b}_u, ~\mathbf{B}_u^{+} ~ (\mathbf{A} + \mathbf{I}\gamma) ~ \mathbf{B}_u^{+T} \right)$" style="width:262px;height:14px;"></p><p><img src="Lotka_Volterra_eq16865926342553571159.png" alt="$= \mathcal{N}\left(\mathbf{x}_u ; \left( \mathbf{B}_{u} \mathbf{B}_{u}^T \right)^{-1} \left( - \sum_k \mathbf{B}_{uk}^T \mathbf{b}_{uk} \right), ~\mathbf{B}_{u}^{+} ~ (\mathbf{A} + \mathbf{I}\gamma) ~ \mathbf{B}_u^{+T} \right) \qquad (8)$" style="width:312px;height:20px;">,</p><p>with <img src="Lotka_Volterra_eq11492802563980187967.png" alt="$\mathbf{X}_{-u}$" style="width:20px;height:10px;"> denoting the set of all states except state <img src="Lotka_Volterra_eq00398522576481313565.png" alt="$\mathbf{x}_u$" style="width:11px;height:7px;">.</p><h2 id="26">Mean-field Variational Inference</h2><p>To infer the parameters <img src="Lotka_Volterra_eq08288499342375314727.png" alt="$\theta$" style="width:5px;height:8px;">, we want to find the maximum a posteriori estimate (MAP):</p><p><img src="Lotka_Volterra_eq02662686890869235014.png" alt="$\theta^* := arg \max_{\theta} ~ \ln p(\theta \mid \mathbf{Y},\phi,\gamma, \sigma)$" style="width:161px;height:11px;"></p><p><img src="Lotka_Volterra_eq10332986432290330183.png" alt="$= arg\max_{\theta} ~ \ln \int  p(\theta,\mathbf{X} \mid \mathbf{Y},\phi,\gamma,\boldmath\sigma) d\mathbf{X}$" style="width:185px;height:13px;"></p><p><img src="Lotka_Volterra_eq14165579452795694147.png" alt="$= arg\max_{\theta} ~ \ln \int p(\theta \mid \mathbf{X},\phi,\gamma) p(\mathbf{X} \mid \mathbf{Y}, \phi,\boldmath\sigma) d\mathbf{X} \qquad (9)$" style="width:259px;height:13px;">.</p><p>However, the integral above is intractable due to the strong couplings induced by the nonlinear ODEs <img src="Lotka_Volterra_eq15427371409597902919.png" alt="$\mathbf{f}$" style="width:5px;height:8px;"> which appear in the term <img src="Lotka_Volterra_eq12256005313017053686.png" alt="$p(\theta \mid \mathbf{X},\phi,\gamma)$" style="width:60px;height:11px;">.</p><p>We use mean-field variational inference to establish variational lower bounds that are analytically tractable by decoupling state variables from the ODE parameters as well as decoupling the state variables from each other. Note that, since the ODEs described by equation (2) are <b>locally linear</b>, both conditional distributions <img src="Lotka_Volterra_eq07986901429347053731.png" alt="$p(\theta \mid \mathbf{X},\mathbf{Y},\phi,\gamma,\boldmath\sigma)$" style="width:87px;height:11px;"> (equation (6)) and <img src="Lotka_Volterra_eq04901024764408510344.png" alt="$p(\mathbf{x}_u \mid \theta, \mathbf{X}_{-u},\mathbf{Y},\phi,\gamma,\boldmath\sigma)$" style="width:114px;height:11px;"> (equation (8)) are analytically tractable and Gaussian distributed as mentioned previously.</p><p>The decoupling is induced by designing a variational distribution <img src="Lotka_Volterra_eq12543915506178644472.png" alt="$Q(\theta,\mathbf{X})$" style="width:36px;height:11px;"> which is restricted to the family of factorial distributions:</p><p><img src="Lotka_Volterra_eq06559016290108276412.png" alt="$\mathcal{Q} := \bigg{\{} Q : Q(\theta,\mathbf{X}) = q(\theta) \prod_u q(\mathbf{x}_u) \bigg{\}}$" style="width:174px;height:27px;">.</p><p>The particular form of <img src="Lotka_Volterra_eq10956578154268717519.png" alt="$q(\theta)$" style="width:19px;height:11px;"> and <img src="Lotka_Volterra_eq03894946144229571455.png" alt="$q(\mathbf{x}_u)$" style="width:25px;height:11px;"> are designed to be Gaussian distributed which places them in the same family as the true full conditional distributions. To find the optimal factorial distribution we minimize the Kullback-Leibler divergence between the variational and the true posterior distribution:</p><p><img src="Lotka_Volterra_eq06043919411795453470.png" alt="$\hat{Q} := arg \min_{Q(\theta,\mathbf{X}) \in \mathcal{Q}} \mathrm{KL} \left[ Q(\theta,\mathbf{X}) \mid \mid p(\theta,\mathbf{X} \mid \mathbf{Y},\phi, \gamma,\boldmath\sigma) \right] \qquad (10)$" style="width:304px;height:15px;">,</p><p>where <img src="Lotka_Volterra_eq03753133506936905529.png" alt="$\hat{Q}$" style="width:8px;height:13px;"> is the proxy distribution. The proxy distribution that minimizes the KL-divergence (10) depends on the true full conditionals and is given by:</p><p><img src="Lotka_Volterra_eq03833571776588842047.png" alt="$\hat{q}({\theta}) \propto \exp \left(~ E_{Q_{-\theta}} \ln p(\theta \mid \mathbf{X},\mathbf{Y},\phi,\gamma,\boldmath\sigma) ~\right) \qquad (11)$" style="width:234px;height:12px;"></p><p><img src="Lotka_Volterra_eq09451345083692260152.png" alt="$\hat{q}(\mathbf{x}_u) \propto \exp\left( ~ E_{Q_{-u}} \ln p(\mathbf{x}_u \mid \theta, \mathbf{X}_{-u},\mathbf{Y},\phi,\gamma,\sigma) ~ \right) \qquad (12)$" style="width:268px;height:12px;">.</p><h2 id="27">Fitting observations of state trajectories</h2><p>We fit the observations of state trajectories by standard GP regression. The data-informed distribution <img src="Lotka_Volterra_eq12925047211459032679.png" alt="$p(\mathbf{X} \mid \mathbf{Y}, \phi,\boldmath\sigma)$" style="width:65px;height:11px;"> in euqation (9) can be determined analytically using Gaussian process regression with the GP prior <img src="Lotka_Volterra_eq03083741424957926896.png" alt="$p(\mathbf{X} \mid \phi) = \prod_k \mathcal{N}(\mathbf{x}_k ; \mathbf{0},\mathbf{C}_{\phi})$" style="width:132px;height:12px;">:</p><p><img src="Lotka_Volterra_eq08911582832004885651.png" alt="$p(\mathbf{X} \mid \mathbf{Y}, \phi,\gamma) = \prod_k \mathcal{N}(\mathbf{x}_k ; \boldmath\mu_k(\mathbf{y}_k),\boldmath\Sigma_k)$" style="width:181px;height:12px;">,</p><p>where <img src="Lotka_Volterra_eq06586455063503371300.png" alt="$\boldmath\mu_k(\mathbf{y}_k) := \sigma_k^{-2} \left(\sigma_k^{-2} \mathbf{I} + \mathbf{C}_{\boldmath\phi_k}^{-1} \right)^{-1} \mathbf{y}_k$" style="width:177px;height:23px;"> and <img src="Lotka_Volterra_eq05050976928104673176.png" alt="$\boldmath\Sigma_k ^{-1}:=\sigma_k^{-2} \mathbf{I} + \mathbf{C}_{\boldmath\phi_k}^{-1}$" style="width:101px;height:15px;">.</p><pre class="codeinput">[mu,inv_sigma] = fitting_state_observations(state,inv_C,obs_to_state_relation,simulation);
</pre><h2 id="28">Coordinate Ascent Variational Gradient Matching</h2><p>We minimize the KL-divergence in equation (10) by coordinate descent (where each step is analytically tractable) by iterating between determining the proxy for the distribution over ODE parameters <img src="Lotka_Volterra_eq13051043552279614619.png" alt="$\hat{q}(\theta)$" style="width:19px;height:11px;"> and the proxies for the distribution over individual states <img src="Lotka_Volterra_eq16039342152573661646.png" alt="$\hat{q}(\mathbf{x}_u)$" style="width:25px;height:11px;">.</p><pre class="codeinput">state.proxy.mean = mu;                                                     <span class="comment">% Initialize the state estimation by the GP regression posterior</span>
<span class="keyword">for</span> i = 1:opt_settings.coord_ascent_numb_iter
</pre><p><h4> Proxy for ODE parameters </h4></p><p>Expanding the proxy distribution in equation (11) for <img src="Lotka_Volterra_eq08288499342375314727.png" alt="$\theta$" style="width:5px;height:8px;"> yields:</p><p><img src="Lotka_Volterra_eq10150089782601658287.png" alt="$\hat{q}(\theta) \stackrel{(a)}{\propto} \exp \left( ~E_{Q_{-\theta}} \ln p(\theta \mid \mathbf{X},\mathbf{Y},\phi,\gamma,\sigma) ~ \right)$" style="width:192px;height:17px;"></p><p><img src="Lotka_Volterra_eq18136870603909550621.png" alt="$\stackrel{(b)}{\propto} \exp \left( ~E_{Q_{-\theta}}  \ln \mathcal{N} \left( \theta; \mathbf{B}_{\theta}^+ ~ \left( '\mathbf{C}_{\phi} \mathbf{C}_{\phi}^{-1} \mathbf{X} - \mathbf{b}_{\theta} \right), ~ \mathbf{B}_{\theta}^+ ~ (\mathbf{A} + \mathbf{I}\gamma) ~ \mathbf{B}_{\theta}^{+T} \right) ~\right)$" style="width:324px;height:21px;"></p><p><img src="Lotka_Volterra_eq01404657084286892155.png" alt="$= \exp \left( ~E_{Q_{-\theta}} \mathcal{N}\left(\theta ; \left( \mathbf{B}_{\theta} \mathbf{B}_{\theta}^T \right)^{-1} \left( \sum_k \mathbf{B}_{\theta k}^T ~ \left( {'\mathbf{C}_{\phi k}} \mathbf{C}_{\phi k}^{-1} \mathbf{X}_k - \mathbf{b}_{\theta k} \right) \right), ~ \mathbf{B}_{\theta}^+ ~ (\mathbf{A} + \mathbf{I}\gamma) ~ \mathbf{B}_{\theta}^{+T} \right) ~\right)$" style="width:409px;height:20px;">,</p><p>which can be normalized analytically due to its exponential quadratic form. In (a) we recall that the ODE parameters depend only indirectly on the observations <img src="Lotka_Volterra_eq00013651220649516337.png" alt="$\mathbf{Y}$" style="width:10px;height:8px;"> through the states <img src="Lotka_Volterra_eq03397130480831257552.png" alt="$\mathbf{X}$" style="width:9px;height:8px;"> and in (b) we substitute <img src="Lotka_Volterra_eq12256005313017053686.png" alt="$p(\theta \mid \mathbf{X},\phi,\gamma)$" style="width:60px;height:11px;"> by its density given in equation (6).</p><pre class="codeinput">    [param_proxy_mean,param_proxy_inv_cov] = proxy_for_ode_parameters(state.proxy.mean,dC_times_invC,<span class="keyword">...</span>
        ode_param.lin_comb,symbols,A_plus_gamma_inv,opt_settings);

    <span class="keyword">if</span> i==1 || ~mod(i,20); plot_results(h_states,h_param,state,time,simulation,param_proxy_mean,p,<span class="string">'not_final'</span>); <span class="keyword">end</span>
</pre><img vspace="5" hspace="5" src="Lotka_Volterra_05.png" style="width:1200px;height:500px;" alt=""> <img vspace="5" hspace="5" src="Lotka_Volterra_06.png" style="width:1200px;height:500px;" alt=""> <img vspace="5" hspace="5" src="Lotka_Volterra_07.png" style="width:1200px;height:500px;" alt=""> <img vspace="5" hspace="5" src="Lotka_Volterra_08.png" style="width:1200px;height:500px;" alt=""> <img vspace="5" hspace="5" src="Lotka_Volterra_09.png" style="width:1200px;height:500px;" alt=""> <img vspace="5" hspace="5" src="Lotka_Volterra_10.png" style="width:1200px;height:500px;" alt=""> <img vspace="5" hspace="5" src="Lotka_Volterra_11.png" style="width:1200px;height:500px;" alt=""> <img vspace="5" hspace="5" src="Lotka_Volterra_12.png" style="width:1200px;height:500px;" alt=""> <img vspace="5" hspace="5" src="Lotka_Volterra_13.png" style="width:1200px;height:500px;" alt=""> <img vspace="5" hspace="5" src="Lotka_Volterra_14.png" style="width:1200px;height:500px;" alt=""> <img vspace="5" hspace="5" src="Lotka_Volterra_15.png" style="width:1200px;height:500px;" alt=""> <p><h4> Proxy for individual states </h4></p><p>Expanding the proxy distribution in equation (12) over the individual state <img src="Lotka_Volterra_eq00398522576481313565.png" alt="$\mathbf{x}_u$" style="width:11px;height:7px;">:</p><p><img src="Lotka_Volterra_eq01033372200965865608.png" alt="$\hat{q}(\mathbf{x}_u) \stackrel{(a)}{\propto} \exp \left( ~ E_{Q_{-u}}  \ln ( p(\mathbf{x}_u \mid \theta, \mathbf{X}_{-u},\phi,\gamma) p(\mathbf{x}_u \mid\mathbf{Y},\phi,\sigma) ) ~ \right)$" style="width:274px;height:17px;"></p><p><img src="Lotka_Volterra_eq09094123556733585003.png" alt="$\stackrel{(b)}{\propto} \exp\big( ~ E_{Q_{-u}} \ln \mathcal{N}\left(\mathbf{x}_u ; -\mathbf{B}_{u}^+ \mathbf{b}_u, ~\mathbf{B}_u^{+} ~ (\mathbf{A} + \mathbf{I}\gamma) ~ \mathbf{B}_u^{+T} \right) + E_{Q_{-u}} \ln \mathcal{N}\left(\mathbf{x}_u ; \boldmath\mu_u(\mathbf{Y}), \Sigma_u \right) \big)$" style="width:385px;height:18px;"></p><p><img src="Lotka_Volterra_eq03469240357617729935.png" alt="$= \exp\big( ~ E_{Q_{-u}} \ln \mathcal{N}\left(\mathbf{x}_u ; \left( \mathbf{B}_{u} \mathbf{B}_{u}^T \right)^{-1} \left( - \sum_k \mathbf{B}_{uk}^T \mathbf{b}_{uk} \right), ~\mathbf{B}_{u}^{+} ~ (\mathbf{A} + \mathbf{I}\gamma) ~ \mathbf{B}_u^{+T} \right) + E_{Q_{-u}} \ln \mathcal{N}\left(\mathbf{x}_u ; \boldmath\mu_u(\mathbf{Y}), \Sigma_u \right) \big)$" style="width:478px;height:20px;">,</p><p>which, once more, can be normalized analytically due to its exponential quadratic form. In (a) we decompose the full conditional into an ODE-informed distribution and a data-informed distribution and in (b) we substitute the ODE-informed distribution <img src="Lotka_Volterra_eq05375239652389534508.png" alt="$p(\mathbf{x}_u \mid \theta, \mathbf{X}_{-u},\phi,\gamma)$" style="width:88px;height:11px;"> with its density given by equation (8).</p><pre class="codeinput">    [state.proxy.mean,state.proxy.inv_cov] = proxy_for_ind_states(state.lin_comb,state.proxy.mean,<span class="keyword">...</span>
        param_proxy_mean',dC_times_invC,coupling_idx.states,symbols,mu,inv_sigma,state.obs_idx,<span class="keyword">...</span>
        A_plus_gamma_inv,opt_settings);
</pre><pre class="codeinput"><span class="keyword">end</span>
</pre><p><h4> Final result </h4></p><pre class="codeinput">plot_results(h_states,h_param,state,time,simulation,param_proxy_mean,p,<span class="string">'final'</span>);
</pre><img vspace="5" hspace="5" src="Lotka_Volterra_16.png" style="width:1200px;height:500px;" alt=""> <h2 id="33">Time Taken</h2><pre class="codeinput">disp([<span class="string">'time taken: '</span> num2str(toc) <span class="string">' seconds'</span>])
</pre><pre class="codeoutput">time taken: 73.6314 seconds
</pre><h2 id="34">References</h2><div><ul><li><b>Gorbach, N.S.</b> , <b>Bauer, S.</b> and Buhmann, J.M., Scalable Variational Inference for Dynamical Systems. 2017a. Neural Information Processing Systems (NIPS). <a href="https://papers.nips.cc/paper/7066-scalable-variational-inference-for-dynamical-systems.pdf">https://papers.nips.cc/paper/7066-scalable-variational-inference-for-dynamical-systems.pdf</a>, arxiv: <a href="https://arxiv.org/abs/1705.07079">https://arxiv.org/abs/1705.07079</a>.</li><li><b>Bauer, S.</b> , <b>Gorbach, N.S.</b> and Buhmann, J.M., Efficient and Flexible Inference for Stochastic Differential Equations. 2017b. Neural Information Processing Systems (NIPS). <a href="https://papers.nips.cc/paper/7274-efficient-and-flexible-inference-for-stochastic-systems.pdf">https://papers.nips.cc/paper/7274-efficient-and-flexible-inference-for-stochastic-systems.pdf</a></li><li>Wenk, P., Gotovos, A., Bauer, S., Gorbach, N.S., Krause, A. and Buhmann, J.M., Fast Gaussian Process Based Gradient Matching for Parameters Identification in Systems of Nonlinear ODEs. 2018. In submission to Conference on Uncertainty in Artificial Intelligence (UAI).</li><li>Calderhead, B., Girolami, M. and Lawrence. N.D., 2002. Accelerating Bayesian inference over nonlinear differential equation models. <i>In Advances in Neural Information Processing Systems (NIPS)</i> . 22.</li></ul></div><p>The authors in bold font have contributed equally to their respective papers.</p><h2 id="35">Subroutines</h2><p><h4> Kernel function </h4></p><p>Gradient matching with Gaussian processes assumes a joint Gaussian process prior on states and their derivatives:</p><p><img src="Lotka_Volterra_eq09462475681373695039.png" alt="$\left(\begin{array}{c} \mathbf{X} \\ \dot{\mathbf{X}} \end{array}\right)  \sim \mathcal{N} \left( \begin{array}{c} \mathbf{X} \\ \dot{\mathbf{X}} \end{array}; \begin{array}{c}  \mathbf{0} \\ \mathbf{0}  \end{array}, \begin{array}{cc}  \mathbf{C}_{\phi} &amp; \mathbf{C}_{\phi}' \\  '\mathbf{C}_{\phi} &amp; \mathbf{C}_{\phi}''  \end{array}  \right)$" style="width:177px;height:28px;">,</p><p><img src="Lotka_Volterra_eq15132385546029468189.png" alt="$\mathrm{cov}(x_k(t), x_k(t)) = C_{\phi_k}(t,t')$" style="width:131px;height:12px;"></p><p><img src="Lotka_Volterra_eq17058345339069568247.png" alt="$\mathrm{cov}(\dot{x}_k(t), x_k(t)) = \frac{\partial C_{\phi_k}(t,t') }{\partial t} =: C_{\phi_k}'(t,t')$" style="width:185px;height:18px;"></p><p><img src="Lotka_Volterra_eq08196297352138716370.png" alt="$\mathrm{cov}(x_k(t), \dot{x}_k(t)) = \frac{\partial C_{\phi_k}(t,t') }{\partial t'} =: {'C_{\phi_k}(t,t')}$" style="width:187px;height:17px;"></p><p><img src="Lotka_Volterra_eq02914213731374756582.png" alt="$\mathrm{cov}(\dot{x}_k(t), \dot{x}_k(t)) = \frac{\partial C_{\phi_k}(t,t') }{\partial t \partial t'} =: C_{\phi_k}''(t,t')$" style="width:185px;height:18px;">.</p><pre class="codeinput"><span class="keyword">function</span> [dC_times_invC,inv_C,A_plus_gamma_inv] = kernel_function(kernel,state,time_est)
</pre><pre class="codeinput">kernel.param_sym = sym(<span class="string">'rbf_param%d'</span>,[1,2]); assume(kernel.param_sym,<span class="string">'real'</span>);
kernel.time1 = sym(<span class="string">'time1'</span>); assume(kernel.time1,<span class="string">'real'</span>); kernel.time2 = sym(<span class="string">'time2'</span>); assume(kernel.time2,<span class="string">'real'</span>);
kernel.func = kernel.param_sym(1).*exp(-(kernel.time1-kernel.time2).^2./(kernel.param_sym(2).^2));                      <span class="comment">% RBF kernel</span>
kernel.name = <span class="string">'rbf'</span>;
</pre><p>kernel derivatives</p><pre class="codeinput"><span class="keyword">for</span> i = 1:length(kernel)
    kernel.func_d = diff(kernel.func,kernel.time1);
    kernel.func_dd = diff(kernel.func_d,kernel.time2);
    GP.fun = matlabFunction(kernel.func,<span class="string">'Vars'</span>,{kernel.time1,kernel.time2,kernel.param_sym});
    GP.fun_d = matlabFunction(kernel.func_d,<span class="string">'Vars'</span>,{kernel.time1,kernel.time2,kernel.param_sym});
    GP.fun_dd = matlabFunction(kernel.func_dd,<span class="string">'Vars'</span>,{kernel.time1,kernel.time2,kernel.param_sym});
<span class="keyword">end</span>
</pre><p>populate GP covariance matrix</p><pre class="codeinput"><span class="keyword">for</span> t=1:length(time_est)
    C(t,:)=GP.fun(time_est(t),time_est,kernel.param);
    dC(t,:)=GP.fun_d(time_est(t),time_est,kernel.param);
    Cd(t,:)=GP.fun_d(time_est,time_est(t),kernel.param);
    ddC(t,:)=GP.fun_dd(time_est(t),time_est,kernel.param);
<span class="keyword">end</span>
</pre><p>GP covariance scaling</p><pre class="codeinput">[~,D] = eig(C); perturb = abs(max(diag(D))-min(diag(D))) / 10000;
<span class="keyword">if</span> any(diag(D)&lt;1e-6); C(logical(eye(size(C,1)))) = C(logical(eye(size(C,1)))) + perturb.*rand(size(C,1),1); <span class="keyword">end</span>
[~,D] = eig(C);
<span class="keyword">if</span> any(diag(D)&lt;0); error(<span class="string">'C has negative eigenvalues!'</span>); <span class="keyword">elseif</span> any(diag(D)&lt;1e-6); warning(<span class="string">'C is badly scaled'</span>); <span class="keyword">end</span>
inv_C = inv_chol(chol(C,<span class="string">'lower'</span>));

dC_times_invC = dC * inv_C;
</pre><p>plot samples from GP prior</p><pre class="codeinput">figure(3);
hold <span class="string">on</span>; plot(time_est,mvnrnd(zeros(1,length(time_est)),C(:,:,1),3),<span class="string">'LineWidth'</span>,2);
h1 = gca; h1.FontSize = 20; h1.XLabel.String = <span class="string">'time'</span>; h1.YLabel.String = <span class="string">'state value'</span>;
h1.Title.String = [kernel.name <span class="string">' kernel'</span>];
</pre><img vspace="5" hspace="5" src="Lotka_Volterra_04.png" style="width:560px;height:420px;" alt=""> <p>determine <img src="Lotka_Volterra_eq15134421922901189392.png" alt="$\mathbf{A} + \mathbf{I}\gamma$" style="width:34px;height:11px;">:</p><pre class="codeinput">A = ddC - dC_times_invC * Cd;
A_plus_gamma = A + state.derivative_variance(1) .* eye(size(A));
A_plus_gamma = 0.5.*(A_plus_gamma+A_plus_gamma');            <span class="comment">% ensure that A plus gamma is symmetric</span>
A_plus_gamma_inv = inv_chol(chol(A_plus_gamma,<span class="string">'lower'</span>));
</pre><pre class="codeinput"><span class="keyword">end</span>
</pre><p><h4> Fitting state observations </h4></p><p>We fit the observations of state trajectories by standard GP regression.</p><p><img src="Lotka_Volterra_eq08911582832004885651.png" alt="$p(\mathbf{X} \mid \mathbf{Y}, \phi,\gamma) = \prod_k \mathcal{N}(\mathbf{x}_k ; \boldmath\mu_k(\mathbf{y}_k),\boldmath\Sigma_k)$" style="width:181px;height:12px;">,</p><p>where <img src="Lotka_Volterra_eq06586455063503371300.png" alt="$\boldmath\mu_k(\mathbf{y}_k) := \sigma_k^{-2} \left(\sigma_k^{-2} \mathbf{I} + \mathbf{C}_{\boldmath\phi_k}^{-1} \right)^{-1} \mathbf{y}_k$" style="width:177px;height:23px;"> and <img src="Lotka_Volterra_eq05050976928104673176.png" alt="$\boldmath\Sigma_k ^{-1}:=\sigma_k^{-2} \mathbf{I} + \mathbf{C}_{\boldmath\phi_k}^{-1}$" style="width:101px;height:15px;">.</p><pre class="codeinput"><span class="keyword">function</span> [mu_u,inv_sigma_u,state] = fitting_state_observations(state,inv_C,obs_to_state_relation,simulation)
</pre><p>Dimensions</p><pre class="codeinput">numb_states = size(state.sym.mean,2);
numb_time_points = size(state.sym.mean,1);
</pre><p>Variance of state observations</p><pre class="codeinput">state_obs_variance = simulation.state_obs_variance(state.obs);
</pre><p>Form block-diagonal matrix out of <img src="Lotka_Volterra_eq07761394001218402071.png" alt="$\mathbf{C}_{\boldmath\phi_k}^{-1}$" style="width:18px;height:15px;"></p><pre class="codeinput">inv_C_replicas = num2cell(inv_C(:,:,ones(1,numb_states)),[1,2]);
inv_C_blkdiag = sparse(blkdiag(inv_C_replicas{:}));
</pre><p>GP posterior inverse covariance matrix: <img src="Lotka_Volterra_eq05050976928104673176.png" alt="$\boldmath\Sigma_k ^{-1}:=\sigma_k^{-2} \mathbf{I} + \mathbf{C}_{\boldmath\phi_k}^{-1}$" style="width:101px;height:15px;"></p><pre class="codeinput">dim = size(state_obs_variance,1)*size(state_obs_variance,2);
D = spdiags(reshape(state_obs_variance.^(-1),[],1),0,dim,dim) * speye(dim); <span class="comment">% covariance matrix of error term (big E)</span>
A_times_D_times_A = obs_to_state_relation' * D * obs_to_state_relation;
inv_sigma = A_times_D_times_A + inv_C_blkdiag;
</pre><p>GP posterior mean: $\boldmath\mu_k(\mathbf{y}_k) := \sigma_k^{-2} \left(\sigma_k^{-2} \mathbf{I} + \mathbf{C}_{\boldmath\phi_k}^{-1} \right)^{-1} \mathbf{y}_k</p><pre class="codeinput">mu = inv_sigma \ obs_to_state_relation' * D * reshape(state.obs,[],1);
</pre><p>Reshape GP mean</p><pre class="codeinput">mu_u = zeros(numb_time_points,numb_states);
<span class="keyword">for</span> u = 1:numb_states
    idx = (u-1)*numb_time_points+1:(u-1)*numb_time_points+numb_time_points;
    mu_u(:,u) = mu(idx);
<span class="keyword">end</span>
</pre><p>Reshape GP inverse covariance matrix</p><pre class="codeinput">inv_sigma_u = zeros(numb_time_points,numb_time_points,numb_states);
<span class="keyword">for</span> i = 1:numb_states
    idx = [(i-1)*numb_time_points+1:(i-1)*numb_time_points+numb_time_points];
    inv_sigma_u(:,:,i) = inv_sigma(idx,idx);
<span class="keyword">end</span>
</pre><pre class="codeinput"><span class="keyword">end</span>
</pre><p><h4> Find state ODE couplings </h4></p><pre class="codeinput"><span class="keyword">function</span> coupling_idx = find_state_couplings_in_odes(ode,symbols)

state_sym = sym(<span class="string">'state%d'</span>,[1,length(ode.system)]); assume(state_sym,<span class="string">'real'</span>);
<span class="keyword">for</span> k = 1:length(ode.system)
    tmp_idx = ismember(state_sym,symvar(ode.system_sym(k))); tmp_idx(:,k) = 1;
    ode_couplings_states(k,tmp_idx) = 1;
<span class="keyword">end</span>

<span class="keyword">for</span> u = 1:length(symbols.state)
    coupling_idx.states{u} = find(ode_couplings_states(:,u));
<span class="keyword">end</span>

<span class="keyword">end</span>
</pre><p><h4> Rewrite ODEs as linear combination in parameters </h4></p><p><img src="Lotka_Volterra_eq08159364397951378465.png" alt="$\mathbf{B}_{\theta k} \theta + \mathbf{b}_{\theta k} \stackrel{!}{=} \mathbf{f}_k(\mathbf{X},\theta)$" style="width:103px;height:15px;">,</p><p>where matrices <img src="Lotka_Volterra_eq09863200650503750796.png" alt="$\mathbf{B}_{\theta k}$" style="width:17px;height:10px;"> and <img src="Lotka_Volterra_eq05105382836198979729.png" alt="$\mathbf{b}_{\theta k}$" style="width:15px;height:10px;"> are defined such that the ODEs <img src="Lotka_Volterra_eq15737021493207115936.png" alt="$\mathbf{f}_k(\mathbf{X},\theta)$" style="width:36px;height:11px;"> are expressed as a linear combination in <img src="Lotka_Volterra_eq08288499342375314727.png" alt="$\theta$" style="width:5px;height:8px;">.</p><pre class="codeinput"><span class="keyword">function</span> [B,b] = rewrite_odes_as_linear_combination_in_parameters(ode,symbols)
</pre><p>Initialization of symbolic variables</p><pre class="codeinput">param_sym = sym(<span class="string">'param%d'</span>,[1,length(symbols.param)]); assume(param_sym,<span class="string">'real'</span>);
state_sym = sym(<span class="string">'state%d'</span>,[1,length(symbols.state)]); assume(state_sym,<span class="string">'real'</span>);
state0_sym = sym(<span class="string">'state0'</span>); assume(state0_sym,<span class="string">'real'</span>);
state_const_sym = sym(<span class="string">'state_const'</span>); assume(state_const_sym,<span class="string">'real'</span>);
</pre><p>Rewrite ODEs as linear combinations in parameters (global)</p><pre class="codeinput">[B_sym,b_sym] = equationsToMatrix(ode.system_sym,param_sym);
b_sym = -b_sym; <span class="comment">% See the documentation of the function "equationsToMatrix"</span>
</pre><p>Operations locally w.r.t. ODEs</p><pre class="codeinput"><span class="keyword">for</span> k = 1:length(ode.system)
    B_sym(k,B_sym(k,:)==<span class="string">'0'</span>) = state0_sym;
    <span class="keyword">for</span> i = 1:length(B_sym(k,:))
        sym_var = symvar(B_sym(k,i));
        <span class="keyword">if</span> isempty(sym_var)
            B_sym(k,i) = B_sym(k,i) + state0_sym;
        <span class="keyword">end</span>
    <span class="keyword">end</span>
    B{k} = matlabFunction(B_sym(k,:),<span class="string">'Vars'</span>,{state_sym,state0_sym,state_const_sym});
    b{k} = matlabFunction(b_sym(k,:),<span class="string">'Vars'</span>,{state_sym,state0_sym,state_const_sym});
<span class="keyword">end</span>
</pre><pre class="codeinput"><span class="keyword">end</span>
</pre><p><h4> Rewrite ODEs as linear combination in individual states </h4></p><p><img src="Lotka_Volterra_eq17696499692546749583.png" alt="$\mathbf{R}_{uk} \mathbf{x}_u + \mathbf{r}_{uk} \stackrel{!}{=} \mathbf{f}_k(\mathbf{X},\theta)$" style="width:109px;height:15px;">.</p><p>where matrices <img src="Lotka_Volterra_eq06228382002088498691.png" alt="$\mathbf{R}_{uk}$" style="width:18px;height:10px;"> and <img src="Lotka_Volterra_eq02299362643576640155.png" alt="$\mathbf{r}_{uk}$" style="width:14px;height:7px;"> are defined such that the ODEs <img src="Lotka_Volterra_eq15737021493207115936.png" alt="$\mathbf{f}_k(\mathbf{X},\theta)$" style="width:36px;height:11px;"> is rewritten as a linear combination in the individual state <img src="Lotka_Volterra_eq00398522576481313565.png" alt="$\mathbf{x}_u$" style="width:11px;height:7px;">.</p><pre class="codeinput"><span class="keyword">function</span> [R,r] = rewrite_odes_as_linear_combination_in_ind_states(ode,symbols,coupling_idx)
</pre><p>Initialization of symbolic variables</p><pre class="codeinput">param_sym = sym(<span class="string">'param%d'</span>,[1,length(symbols.param)]); assume(param_sym,<span class="string">'real'</span>);
state_sym = sym(<span class="string">'state%d'</span>,[1,length(symbols.state)]); assume(state_sym,<span class="string">'real'</span>);
state0_sym = sym(<span class="string">'state0'</span>); assume(state0_sym,<span class="string">'real'</span>);
state_const_sym = sym(<span class="string">'state_const'</span>); assume(state_const_sym,<span class="string">'real'</span>);
</pre><p>Rewrite ODEs as linear combinations in parameters (locally)</p><pre class="codeinput"><span class="keyword">for</span> u = 1:length(symbols.state)
    <span class="keyword">for</span> k = coupling_idx{u}'
        [R_sym,r_sym] = equationsToMatrix(ode.system{k}(state_sym,param_sym'),state_sym(:,u));
        r_sym = -r_sym; <span class="comment">% See the documentation of the function "equationsToMatrix"</span>

        R{u,k} = matlabFunction(R_sym,<span class="string">'Vars'</span>,{state_sym,param_sym});
        r{u,k} = matlabFunction(r_sym,<span class="string">'Vars'</span>,{state_sym,param_sym});
    <span class="keyword">end</span>
<span class="keyword">end</span>
</pre><pre class="codeinput"><span class="keyword">end</span>
</pre><p><h4> Proxy for ODE parameters </h4></p><p><img src="Lotka_Volterra_eq15329696075421497620.png" alt="$\hat{q}(\theta) {\propto} \exp \left( ~E_{Q_{-\theta}}  \ln \mathcal{N}\left(\theta ; \left( \mathbf{B}_{\theta} \mathbf{B}_{\theta}^T \right)^{-1} \left( \sum_k \mathbf{B}_{\theta k}^T ~ \left( {'\mathbf{C}_{\phi k}} \mathbf{C}_{\phi k}^{-1} \mathbf{X}_k - \mathbf{b}_{\theta k} \right) \right), ~ \mathbf{B}_{\theta}^+ ~ (\mathbf{A} + \mathbf{I}\gamma) ~ \mathbf{B}_{\theta}^{+T} \right) ~\right)$" style="width:440px;height:20px;">,</p><pre class="codeinput"><span class="keyword">function</span> [param_proxy_mean,param_proxy_inv_cov] = proxy_for_ode_parameters(state_proxy_mean,<span class="keyword">...</span>
    dC_times_invC,lin_comb,symbols,A_plus_gamma_inv,opt_settings)
</pre><p>Initialization</p><pre class="codeinput">state0 = zeros(size(dC_times_invC,1),1);
param_proxy_inv_cov = zeros(length(symbols.param));
global_scaling = zeros(length(symbols.param));
global_mean = zeros(length(symbols.param),1);
</pre><p>Iteratate through ODEs</p><pre class="codeinput"><span class="keyword">for</span> k = 1:length(symbols.state)
</pre><p>unpack matrices <img src="Lotka_Volterra_eq09421698273818199295.png" alt="$\mathbf{B}$" style="width:9px;height:8px;"> and <img src="Lotka_Volterra_eq00174205605165731489.png" alt="$\mathbf{b}$" style="width:7px;height:8px;"></p><pre class="codeinput">    B = lin_comb.B{k}(state_proxy_mean,state0,ones(size(state_proxy_mean,1),1));
    b = lin_comb.b{k}(state_proxy_mean,state0,ones(size(state_proxy_mean,1),1));
</pre><p>Local</p><pre class="codeinput">    <span class="keyword">if</span> strcmp(opt_settings.pseudo_inv_type,<span class="string">'Moore-Penrose'</span>)
        <span class="comment">% local mean: $\mathbf{B}_{\theta k}^T ~ \left( {'\mathbf{C}_{\phi_k}} \mathbf{C}_{\phi k}^{-1} \mathbf{X}_k - \mathbf{b}_{\theta k}$</span>
        local_mean = B' * (dC_times_invC * state_proxy_mean(:,k) - b);
        local_scaling = B' * B;
        local_inv_cov = B' * A_plus_gamma_inv * B;
    <span class="keyword">elseif</span> strcmp(opt_settings.pseudo_inv_type,<span class="string">'modified Moore-Penrose'</span>)
        local_mean = B' * A_plus_gamma_inv * (dC_times_invC * state_proxy_mean(:,k) - b);
        local_scaling = B' * A_plus_gamma_inv * B;
        local_inv_cov = local_scaling;
    <span class="keyword">end</span>
</pre><p>Global</p><pre class="codeinput">    global_mean = global_mean + local_mean;
    global_scaling = global_scaling + local_scaling;

    <span class="comment">% Inverse covariance of ODE param proxy distribution</span>
    param_proxy_inv_cov = param_proxy_inv_cov + local_inv_cov;
</pre><pre class="codeinput"><span class="keyword">end</span>
</pre><p>Check scaling of covariance matrix</p><pre class="codeinput">[~,D] = eig(param_proxy_inv_cov);
<span class="keyword">if</span> any(diag(D)&lt;0)
    warning(<span class="string">'param_proxy_inv_cov has negative eigenvalues!'</span>);
<span class="keyword">elseif</span> any(diag(D)&lt;1e-3)
    warning(<span class="string">'param_proxy_inv_cov is badly scaled'</span>)
    disp(<span class="string">'perturbing diagonal of param_proxy_inv_cov'</span>)
    perturb = abs(max(diag(D))-min(diag(D))) / 10000;
    param_proxy_inv_cov(logical(eye(size(param_proxy_inv_cov,1)))) = param_proxy_inv_cov(logical(eye(size(param_proxy_inv_cov,1)))) <span class="keyword">...</span>
        + perturb.*rand(size(param_proxy_inv_cov,1),1);
<span class="keyword">end</span>
</pre><p>Mean of parameter proxy distribution (option: Moore-penrose inverse example): <img src="Lotka_Volterra_eq15995969067233676168.png" alt="$\left( \mathbf{B}_{\theta} \mathbf{B}_{\theta}^T \right)^{-1} \left( \sum_k \mathbf{B}_{\theta k}^T ~ \left( {'\mathbf{C}_{\phi k}} \mathbf{C}_{\phi k}^{-1} \mathbf{X}_k - \mathbf{b}_{\theta k} \right) \right)$" style="width:200px;height:20px;"></p><pre class="codeinput">param_proxy_mean = global_scaling \ global_mean;
param_proxy_mean = abs(param_proxy_mean);      <span class="comment">% mirroring to preserve magnitude</span>
</pre><pre class="codeinput"><span class="keyword">end</span>
</pre><p><h4> Proxy for individual states </h4></p><p><img src="Lotka_Volterra_eq11126775742288154163.png" alt="$\hat{q}(\mathbf{x}_u) \propto \exp\big( ~ E_{Q_{-u}} \ln \mathcal{N}\left(\mathbf{x}_u ; \left( \mathbf{B}_{u} \mathbf{B}_{u}^T \right)^{-1} \left( - \sum_k \mathbf{B}_{uk}^T \mathbf{b}_{uk} \right), ~\mathbf{B}_{u}^{+} ~ (\mathbf{A} + \mathbf{I}\gamma) ~ \mathbf{B}_u^{+T} \right) + E_{Q_{-u}} \ln \mathcal{N}\left(\mathbf{x}_u ; \boldmath\mu_u(\mathbf{Y}), \Sigma_u \right) \big)$" style="width:507px;height:20px;">,</p><pre class="codeinput"><span class="keyword">function</span> [state_proxy_mean,state_proxy_inv_cov] = proxy_for_ind_states(lin_comb,state_proxy_mean,<span class="keyword">...</span>
    ode_param,dC_times_invC,coupling_idx,symbols,mu,inv_sigma,state_obs_idx,<span class="keyword">...</span>
    A_plus_gamma_inv,opt_settings)
</pre><p>Clamp observed states to GP fit</p><pre class="codeinput"><span class="keyword">if</span> opt_settings.clamp_obs_state_to_GP_fit
    state_enumeration = find(~state_obs_idx);
<span class="keyword">else</span>
    state_enumeration = 1:length(symbols.state);
<span class="keyword">end</span>

<span class="keyword">for</span> u = state_enumeration
</pre><p>Initialization</p><pre class="codeinput">    state_proxy_inv_cov(:,:,u) = zeros(size(dC_times_invC));
    global_scaling = zeros(size(dC_times_invC));
    global_mean = zeros(size(dC_times_invC,1),1);
</pre><p>Iteratate through ODEs</p><pre class="codeinput">    <span class="keyword">for</span> k = coupling_idx{u}'
</pre><p>unpack matrices <img src="Lotka_Volterra_eq08522258881804510052.png" alt="$\mathbf{R}$" style="width:10px;height:8px;"> and <img src="Lotka_Volterra_eq05836581178621559763.png" alt="$\mathbf{r}$" style="width:5px;height:5px;"></p><pre class="codeinput">        R = diag(lin_comb.R{u,k}(state_proxy_mean,ode_param));
        r = lin_comb.r{u,k}(state_proxy_mean,ode_param);
        <span class="keyword">if</span> size(R,1) == 1; R = R.*eye(size(dC_times_invC,1)); <span class="keyword">end</span>
        <span class="keyword">if</span> length(r)==1; r = zeros(length(global_mean),1); <span class="keyword">end</span>
</pre><p>Define matrices B and b such that <img src="Lotka_Volterra_eq18302453219156952320.png" alt="$\mathbf{B}_{uk} \mathbf{x}_u + \mathbf{b}_{uk} \stackrel{!}{=} \mathbf{f}_k(\mathbf{X},\theta) - {'\mathbf{C}}_{\phi_{k}} \mathbf{C}_{\phi_{k}}^{-1} \mathbf{X}$" style="width:174px;height:17px;"></p><pre class="codeinput">        <span class="keyword">if</span> k~=u
            B = R;
            b = r - dC_times_invC * state_proxy_mean(:,k);
        <span class="keyword">else</span>
            B = R - dC_times_invC;
            b = r;
        <span class="keyword">end</span>
</pre><p>Local</p><pre class="codeinput">        <span class="keyword">if</span> strcmp(opt_settings.pseudo_inv_type,<span class="string">'Moore-Penrose'</span>)
            <span class="comment">% local mean: $\mathbf{B}_{uk}^T \left(\epsilon_0^{(k)} -\mathbf{b}_{uk}</span>
            local_mean = -B' * b;
            local_scaling = B' * B;
            local_inv_cov = B' * A_plus_gamma_inv * B;
        <span class="keyword">elseif</span> strcmp(opt_settings.pseudo_inv_type,<span class="string">'modified Moore-Penrose'</span>)
            local_mean = -B' * A_plus_gamma_inv * b;
            local_scaling = B' * A_plus_gamma_inv * B;
            local_inv_cov = local_scaling;
        <span class="keyword">end</span>
</pre><p>Global</p><pre class="codeinput">        global_mean = global_mean + local_mean;
        global_scaling = global_scaling + local_scaling;
</pre><p>Inverse covariance for state proxy distribution</p><pre class="codeinput">        state_proxy_inv_cov(:,:,u) = state_proxy_inv_cov(:,:,u) + local_inv_cov;
</pre><pre class="codeinput">    <span class="keyword">end</span>
</pre><p>Mean of state proxy distribution (option: Moore-penrose inverse example): <img src="Lotka_Volterra_eq01005039668526812269.png" alt="$\left( \mathbf{B}_{u} \mathbf{B}_{u}^T \right)^{-1} \sum_k \mathbf{B}_{uk}^T \left(\epsilon_0^{(k)} -\mathbf{b}_{uk} \right)$" style="width:145px;height:20px;"></p><pre class="codeinput">    state_proxy_mean(:,u) = (global_scaling + inv_sigma(:,:,u)) \ (global_mean + (inv_sigma(:,:,u) * mu(:,u)));
</pre><pre class="codeinput"><span class="keyword">end</span>
</pre><pre class="codeinput"><span class="keyword">end</span>
</pre><p><h4> Import ODEs </h4></p><pre class="codeinput"><span class="keyword">function</span> ode = import_odes(symbols)
</pre><p>Path to system of ODEs</p><pre class="codeinput">path_ode = <span class="string">'./Lotka_Volterra_ODEs.txt'</span>;
</pre><p>Import ODEs</p><pre class="codeinput">ode.raw = importdata(path_ode);
ode.refined = ode.raw;
</pre><p>Refine ODEs</p><pre class="codeinput"><span class="keyword">for</span> k = 1:length(ode.refined)
<span class="keyword">for</span> u = 1:length(symbols.state); ode.refined{k} = strrep(ode.refined{k},[symbols.state{u}],[<span class="string">'state(:,'</span> num2str(u) <span class="string">')'</span>]); <span class="keyword">end</span>
<span class="keyword">for</span> j = 1:length(symbols.param); ode.refined{k} = strrep(ode.refined{k},symbols.param{j},[<span class="string">'param('</span> num2str(j) <span class="string">')'</span>]); <span class="keyword">end</span>
<span class="keyword">end</span>
<span class="keyword">for</span> k = 1:length(ode.refined); ode.system{k} = str2func([<span class="string">'@(state,param)('</span> ode.refined{k} <span class="string">')'</span>]); <span class="keyword">end</span>
</pre><pre class="codeinput"><span class="keyword">end</span>
</pre><p><h4> Generate ground truth </h4></p><pre class="codeinput"><span class="keyword">function</span> [state,time,ode] = generate_ground_truth(time,state,ode,symbols,simulation)
</pre><p>Integration times</p><pre class="codeinput">time.true=0:simulation.int_interval:simulation.final_time;                 <span class="comment">% ture times</span>
Tindex=length(time.true);                                                  <span class="comment">% index time</span>
TTT=length(simulation.time_samp);                                          <span class="comment">% number of sampled points</span>
itrue=round(simulation.time_samp./simulation.int_interval+ones(1,TTT));    <span class="comment">% Index of sample time in the true time</span>
</pre><p>Symbolic computations</p><pre class="codeinput">param_sym = sym(<span class="string">'param%d'</span>,[1,length(symbols.param)]); assume(param_sym,<span class="string">'real'</span>);
state_sym = sym(<span class="string">'state%d'</span>,[1,length(symbols.state)]); assume(state_sym,<span class="string">'real'</span>);
<span class="keyword">for</span> i = 1:length(ode.system)
    ode.system_sym(i) = ode.system{i}(state_sym,param_sym);
<span class="keyword">end</span>
</pre><p>Fourth order Runge-Kutta (numerical) integration</p><pre class="codeinput">ode_system_mat = matlabFunction(ode.system_sym',<span class="string">'Vars'</span>,{state_sym',param_sym'});
[~,OutX_solver]=ode45(@(t,x) ode_system_mat(x,simulation.ode_param'), time.true, simulation.init_val);
state.true_all=OutX_solver;
state.true=state.true_all(itrue,:);
</pre><p>Pack</p><pre class="codeinput">state.obs_idx = simulation.state_obs_idx;
</pre><pre class="codeinput"><span class="keyword">end</span>
</pre><p><h4> Generate observations of states </h4></p><pre class="codeinput"><span class="keyword">function</span> [state,time,obs_to_state_relation] = generate_state_obs(state,time,simulation)
</pre><p>State observations</p><pre class="codeinput">state_obs_variance = simulation.state_obs_variance(state.true);
state.obs = state.true + sqrt(state_obs_variance) .* randn(size(state.true));
</pre><p>Mapping between states and observations</p><pre class="codeinput"><span class="keyword">if</span> length(simulation.time_samp) &lt; length(time.est)
    time.idx = munkres(pdist2(simulation.time_samp',time.est'));
    time.ind = sub2ind([length(simulation.time_samp),length(time.est)],1:length(simulation.time_samp),time.idx);
<span class="keyword">else</span>
    time.idx = munkres(pdist2(time.est',simulation.time_samp'));
    time.ind = sub2ind([length(time.est),length(simulation.time_samp)],1:length(time.est),time.idx);
<span class="keyword">end</span>
time.obs_time_to_state_time_relation = zeros(length(simulation.time_samp),length(time.est)); time.obs_time_to_state_time_relation(time.ind) = 1;
state_mat = eye(size(state.true,2));
obs_to_state_relation = sparse(kron(state_mat,time.obs_time_to_state_time_relation));
time.samp = simulation.time_samp;
</pre><pre class="codeinput"><span class="keyword">end</span>
</pre><p><h4> Setup plots </h4></p><pre class="codeinput"><span class="keyword">function</span> [h_states,h_param,p] = setup_plots(state,time,simulation,symbols)
</pre><p>Refine ODE parameter symbols</p><pre class="codeinput"><span class="keyword">for</span> i = 1:length(symbols.param); symbols.param{i} = symbols.param{i}(2:end-1); <span class="keyword">end</span>
</pre><p>Figure size and position setup</p><pre class="codeinput">figure(1); set(1, <span class="string">'Position'</span>, [0, 200, 1200, 500]);
</pre><img vspace="5" hspace="5" src="Lotka_Volterra_01.png" style="width:1200px;height:500px;" alt=""> <p>ODE parameters</p><pre class="codeinput">h_param = subplot(1,3,1); h_param.FontSize = 20; h_param.Title.String = <span class="string">'ODE parameters'</span>;
set(gca,<span class="string">'XTick'</span>,[1:length(symbols.param)]); set(gca,<span class="string">'XTickLabel'</span>,symbols.param);
hold <span class="string">on</span>; drawnow
</pre><img vspace="5" hspace="5" src="Lotka_Volterra_02.png" style="width:1200px;height:500px;" alt=""> <p>States</p><pre class="codeinput"><span class="keyword">for</span> u = 1:2
    h_states{u} = subplot(1,3,u+1); cla; p.true = plot(time.true,state.true_all(:,u),<span class="string">'LineWidth'</span>,2,<span class="string">'Color'</span>,[217,95,2]./255);
    hold <span class="string">on</span>; p.obs = plot(simulation.time_samp,state.obs(:,u),<span class="string">'*'</span>,<span class="string">'Color'</span>,[217,95,2]./255,<span class="string">'MarkerSize'</span>,10);
    h_states{u}.FontSize = 20; h_states{u}.Title.String = symbols.state{u}(2:end-1); h_states{u}.XLim = [min(time.est),max(time.est)];
    h_states{u}.XLabel.String = <span class="string">'time'</span>; hold <span class="string">on</span>;
<span class="keyword">end</span>
</pre><img vspace="5" hspace="5" src="Lotka_Volterra_03.png" style="width:1200px;height:500px;" alt=""> <pre class="codeinput"><span class="keyword">end</span>
</pre><p><h4> Plot results </h4></p><pre class="codeinput"><span class="keyword">function</span> plot_results(h_states,h_param,state,time,simulation,param_proxy_mean,p,plot_type)

<span class="keyword">for</span> u = 1:2
    <span class="keyword">if</span> strcmp(plot_type,<span class="string">'final'</span>)

        <span class="comment">% State proxy variance</span>
        state_proxy_variance = diag(state.proxy.inv_cov(:,:,u)^(-1));
        shaded_region = [state.proxy.mean(:,u)+1*sqrt(state_proxy_variance); flip(state.proxy.mean(:,u)-1*sqrt(state_proxy_variance),1)];
        f = fill(h_states{u},[time.est'; flip(time.est',1)], shaded_region, [222,235,247]/255); set(f,<span class="string">'EdgeColor'</span>,<span class="string">'None'</span>);

        <span class="comment">% Replot true states</span>
        p.true = plot(h_states{u},time.true,state.true_all(:,u),<span class="string">'LineWidth'</span>,2,<span class="string">'Color'</span>,[217,95,2]./255);

        <span class="comment">% Replot state obbservations</span>
        p.obs = plot(h_states{u},simulation.time_samp,state.obs(:,u),<span class="string">'*'</span>,<span class="string">'Color'</span>,[217,95,2]./255,<span class="string">'MarkerSize'</span>,10);

        <span class="comment">% State proxy mean (final)</span>
        hold <span class="string">on</span>; p.est = plot(h_states{u},time.est,state.proxy.mean(:,u),<span class="string">'Color'</span>,[117,112,179]./255,<span class="string">'LineWidth'</span>,2);
    <span class="keyword">else</span>
        <span class="comment">% state proxy mean (not final)</span>
        hold <span class="string">on</span>; p.est = plot(h_states{u},time.est,state.proxy.mean(:,u),<span class="string">'LineWidth'</span>,0.1,<span class="string">'Color'</span>,[0.8,0.8,0.8]);
    <span class="keyword">end</span>
    <span class="comment">% Specify legend entries</span>
    legend(h_states{u},[p.true,p.obs,p.est],{<span class="string">'true'</span>,<span class="string">'observed'</span>,<span class="string">'estimate'</span>},<span class="string">'Location'</span>,<span class="string">'southwest'</span>);
<span class="keyword">end</span>

<span class="comment">% ODE parameters</span>
cla(h_param); b = bar(h_param,1:length(param_proxy_mean),[simulation.ode_param',param_proxy_mean]);
b(1).FaceColor = [217,95,2]./255; b(2).FaceColor = [117,112,179]./255;
h_param.XLim = [0.5,length(param_proxy_mean)+0.5]; h_param.YLimMode = <span class="string">'auto'</span>;
legend(h_param,{<span class="string">'true'</span>,<span class="string">'estimate'</span>},<span class="string">'Location'</span>,<span class="string">'northwest'</span>);
drawnow

<span class="keyword">end</span>
</pre><p class="footer"><br><a href="http://www.mathworks.com/products/matlab/">Published with MATLAB&reg; R2017a</a><br></p></div><!--
##### SOURCE BEGIN #####
%% Variational Gradient Matching for Dynamical Systems: Lotka-Volterra
%
% <<cover_pic.png>>
%
% Authors: *Nico Stephan Gorbach* and *Stefan Bauer*, email: nico.gorbach@gmail.com
%
% Instructional code for the NIPS (2018) paper " *Scalable Variational Inference for Dynamical Systems* "
% by Nico S. Gorbach, Stefan Bauer and Joachim M. Buhmann.
% The paper is available at <https://papers.nips.cc/paper/7066-scalable-variational-inference-for-dynamical-systems.pdf>.
% Please cite our paper if you use our program for a further publication.
% Part of the derivation below is described in Wenk et al. (2018).
%
% Example dynamical system used in this code: Lotka-Volterra system with *half* of the time points *unobserved*. The ODE parameters are also unobserved.

%% Advantages of Variational Gradient Matching
% The essential idea of gradient matching (Calderhead et al., 2002) is to match the gradient
% governed by the ODEs with that inferred from the observations. In contrast
% to previous approaches gradient matching introduces a prior over states
% instead of a prior over ODE parameters. The advantages of gradients
% matching is two-fold:
%%
%
% # A prior over the functional form of state dynamics as opposed to ODE parameters facilitates a
% more expert-aware estimation of ODE parameters since experts can provide
% a better _a priori_ description of state dynamics than ODE parameters.
% # Gradient matching yields a global gradient as opposed to a local one which
% offers significant computational advantages and provides access to a rich
% source of sophisticated optimization tools.
%

%%
% Clear workspace and close figures
clear all; close all;

%% Simulation Settings

simulation.state_obs_variance = @(mean)(bsxfun(@times,[0.5^2,0.5^2],...
    ones(size(mean))));                                                    % observation noise
simulation.ode_param = [2,1,4,1];                                          % true ODE parameters [2 1 4 1] is used as a benchmark in many publications;
simulation.final_time = 2;                                                 % end time for integration
simulation.int_interval = 0.01;                                            % integration interval
simulation.time_samp = 0:0.1:simulation.final_time;                        % sample times for observations
simulation.init_val = [5 3];                                               % state values at first time point
simulation.state_obs_idx = [1,1];                                          % indices of states that are directly observed (Boolean)

%% User Input
%
% <html><h4> Kernel </h4></html>
%
% Kernel parameters $\phi$:
kernel.param = [10,0.2];                                                   % set values of rbf kernel parameters
%%
% Error variance on state derivatives (i.e. $\gamma$):
state.derivative_variance = [6,6];                                         % gamma for gradient matching model
%%
% <html><h4> Estimation times </h4></html>
time.est = 0:0.1:4;                                                        % estimation times
%%
% <html><h4> Optimization settings </h4></html>
opt_settings.pseudo_inv_type = 'Moore-Penrose';                            % Type of pseudo inverse; options: 'Moore-Penrose' or 'modified Moore-Penrose'
opt_settings.coord_ascent_numb_iter = 200;                                 % number of coordinate ascent iterations
opt_settings.clamp_obs_state_to_GP_fit = false;                            % The observed state trajectories are clamped to the trajectories determined by standard GP regression (Boolean)
%%
% <html><h4> Symbols </h4></html>
%
% States $\mathbf{x}$:
symbols.state = {'[prey]','[predator]'};                                   % symbols of states in 'ODEs.txt' file
%%
% ODE parameters $\theta$:
symbols.param = {'[\theta_1]','[\theta_2]','[\theta_3]','[\theta_4]'};     % symbols of parameters in 'ODEs.txt' file

%% Import ODEs
%
ode = import_odes(symbols);

%%
disp('ODEs:'); disp(ode.raw)

%% Mass Action Dynamical Systems
%
% A deterministic dynamical system is represented by a set of $K$ ordinary differential equations (ODEs) with model parameters $\theta \in R^d$ that describe the evolution of $K$ states $\mathbf{x}(t) = [x_1(t),\ldots, x_K(t)]^T$ such that:
% 
% $\dot{\mathbf{x}}(t) = \frac{d \mathbf{x}(t)}{d t} = \mathbf{f}(\mathbf{x}(t),\theta) \qquad (1)$.
% 
% A sequence of observations, $\mathbf{y}(t)$, is usually contaminated by measurement error which we assume to be normally distributed with zero mean and variance for each of the $K$ states, i.e. $\mathbf{E}\sim \mathcal{N}(\mathbf{E};\mathbf{0},\mathbf{D})$, with $\mathbf{D}_{ik}=\sigma_k ^2 \delta_{ik}$. For $N$ distinct time points the overall system may therefore be summarized as:
% 
% $\mathbf{Y} = \mathbf{X} + \mathbf{E}$,
% 
% where 
%
% $\mathbf{X} = [\mathbf{x}(t_1),\ldots,\mathbf{x}(t_N)] = [\mathbf{x}_1,\ldots,\mathbf{x}_K]^T$,
%
% $\mathbf{Y} = [\mathbf{y}(t_1),\ldots,\mathbf{y}(t_N)] = [\mathbf{y}_1,\ldots,\mathbf{y}_K]^T$,
% 
% and $\mathbf{x}_k = [x_k(t_1),\ldots,x_k(t_N)]^T$ is the $k$'th state sequence and $\mathbf{y}_k = [y_k(t_1),$ $\ldots,y_k(t_N)]^T$ are the observations. Given the observations $\mathbf{Y}$ and the description of the dynamical system (1), the aim is to estimate both state variables $\mathbf{X}$ and parameters $\theta$.
% 
% We consider only dynamical systems that are locally linear with respect to ODE parameters $\theta$ and individual states $\mathbf{x}_u$. Such ODEs include mass-action kinetics and are given by: 
%
% $f_{k}(\mathbf{x}(t),\theta) = \sum_{i=1} \theta_{ki} \prod_{j \in \mathcal{M}_{ki}} x_j \qquad (2)$,
%
% with $\mathcal{M}_{ki} \subseteq \{ 1, \dots, K\}$ describing the state variables in each factor of the equation (i.e. the functions are linear in parameters and contain arbitrary large products of monomials of the states).

%% Simulate Trajectory Observations
%%
% <html><h4> Generate ground truth by numerical integration </h4></html>
[state,time,ode] = generate_ground_truth(time,state,ode,symbols,simulation);

%%
% <html><h4> Generate state observations </h4></html>
[state,time,obs_to_state_relation] = generate_state_obs(state,time,simulation);

%%
% <html><h4> Symbols </h4></html>
state.sym.mean = sym('x%d%d',[length(time.est),length(ode.system)]);
state.sym.variance = sym('sigma%d%d',[length(time.est),length(ode.system)]);
ode_param.sym.mean = sym('param%d',[length(symbols.param),1]); assume(ode_param.sym.mean,'real');

%%
% <html><h4> Setup plots </h4></html>
%
% Only the state dynamics are (partially) observed.
[h_states,h_param,p] = setup_plots(state,time,simulation,symbols);

tic; %start timer
%% Prior on States and State Derivatives
% Gradient matching with Gaussian processes assumes a joint Gaussian process prior on states and their derivatives:
%
% $\left(\begin{array}{c}
% \mathbf{X} \\ \dot{\mathbf{X}}
% \end{array}\right)
%  \sim \mathcal{N} \left(
% \begin{array}{c}
% \mathbf{X} \\ \dot{\mathbf{X}}
% \end{array}; 
% \begin{array}{c}
%  \mathbf{0} \\ 
% \mathbf{0}
%  \end{array},
% \begin{array}{cc}
%  \mathbf{C}_{\phi} & \mathbf{C}_{\phi}' \\
%  '\mathbf{C}_{\phi} & \mathbf{C}_{\phi}'' 
%  \end{array}
%  \right) \qquad (3)$,
%
% $\mathrm{cov}(x_k(t), x_k(t)) = C_{\phi_k}(t,t')$
%
% $\mathrm{cov}(\dot{x}_k(t), x_k(t)) = \frac{\partial C_{\phi_k}(t,t') }{\partial t} =: C_{\phi_k}'(t,t')$
%
% $\mathrm{cov}(x_k(t), \dot{x}_k(t)) = \frac{\partial C_{\phi_k}(t,t') }{\partial t'} =: {'C_{\phi_k}(t,t')}$
%
% $\mathrm{cov}(\dot{x}_k(t), \dot{x}_k(t)) = \frac{\partial C_{\phi_k}(t,t') }{\partial t \partial t'} =: C_{\phi_k}''(t,t')$.

%% Matching Gradients
%
% Given the joint distribution over states and their derivatives (3) as well as the ODEs (2), we therefore have two expressions for the state derivatives:
%
% $\dot{\mathbf{X}} = \mathbf{F} + \epsilon_1, \epsilon_1 \sim \mathcal{N}\left(\epsilon_1;\mathbf{0}, \mathbf{I}\gamma \right)$
%
% $\dot{\mathbf{X}} = {'\mathbf{C}_{\phi}} \mathbf{C}_{\phi}^{-1} ~\mathbf{X} + \epsilon_2, \epsilon_2 \sim \mathcal{N}\left(\epsilon_2;\mathbf{0}, \mathbf{A} \right)$
%
% where $\mathbf{F} := \mathbf{f}(\mathbf{X},\theta)$, $\mathbf{A} := \mathbf{C}_{\phi}'' -  {'\mathbf{C}_{\phi}} \mathbf{C}_{\phi}^{-1} \mathbf{C}_{\phi}'$ and $\gamma$ is the error variance in the ODEs. Note that, in a deterministic system, the output of the ODEs $\mathbf{F}$ should equal the state derivatives $\dot{\mathbf{X}}$. However, in the first equation above we relax this contraint by adding stochasticity to the state derivatives $\dot{\mathbf{X}}$ in order to compensate for a potential model mismatch. The second equation above is obtained by deriving the conditional distribution for $\dot{\mathbf{X}}$ from the joint distribution in equation (3). Equating the two expressions in the equations above we can eliminate the unknown state derivatives $\dot{\mathbf{X}}$:
%
% $\mathbf{F} = {'\mathbf{C}_{\phi}} \mathbf{C}_{\phi}^{-1} ~\mathbf{X} + \epsilon_0 \qquad (4)$,
%
% with $\epsilon_0 := \epsilon_2 - \epsilon_1$.

[dC_times_invC,inv_C,A_plus_gamma_inv] = kernel_function(kernel,state,time.est);

%% State Couplings in ODEs

coupling_idx = find_state_couplings_in_odes(ode,symbols);

%% Rewrite ODEs as Linear Combination in Parameters
%
% We rewrite the ODEs in equation (2) as a linear combination in the parameters:
%
% $\mathbf{B}_{\theta k} \theta + \mathbf{b}_{\theta k} \stackrel{!}{=} \mathbf{f}_k(\mathbf{X},\theta) \qquad (5)$,
%
% where matrices $\mathbf{B}_{\theta k}$ and $\mathbf{b}_{\theta k}$ are defined such that the ODEs $\mathbf{f}_k(\mathbf{X},\theta)$ are expressed as a linear combination in $\theta$.

[ode_param.lin_comb.B,ode_param.lin_comb.b] = rewrite_odes_as_linear_combination_in_parameters(ode,symbols);

%% Posterior over ODE Parameters
%
% Inserting (5) into (4) and solving for $\theta$ yields:
%
% $\theta = \mathbf{B}_{\theta}^+ \left( {'\mathbf{C}_{\phi}} \mathbf{C}_{\phi}^{-1} \mathbf{X} - \mathbf{b}_{\theta} + \epsilon_0 \right)$,
% 
% where $\mathbf{B}_{\theta}^+$ denotes the pseudo-inverse of $\mathbf{B}_{\theta}$.
%
% Since $\mathbf{C}_{\phi}$ is block diagonal we can rewrite the expression above as:
%
% $\theta = \left( \mathbf{B}_{\theta} \mathbf{B}_{\theta}^T \right)^{-1} \mathbf{B}_{\theta}^T  \left( \sum_k {'\mathbf{C}_{\phi_k}} \mathbf{C}_{\boldmath\phi_k}^{-1} \mathbf{X}_k - \mathbf{b}_{\theta k} + \boldmath\epsilon_0^{(k)} \right)$,
%
% $= \left( \mathbf{B}_{\theta} \mathbf{B}_{\theta}^T \right)^{-1} \left( \sum_k \mathbf{B}_{\theta k}^T \left( {'\mathbf{C}_{\phi_k}} \mathbf{C}_{\boldmath\phi_k}^{-1} \mathbf{X}_k - \mathbf{b}_{\theta k} + \boldmath\epsilon_0^{(k)} \right) \right)$,
%
% where we subsitute the Moore-Penrose inverse for the pseudo-inverse (i.e. $\mathbf{B}_{\theta}^+ = \left( \mathbf{B}_{\theta} \mathbf{B}_{\theta}^T \right)^{-1} \mathbf{B}_{\theta}^T$).
%
% We can therefore derive the posterior distribution over ODE parameters:
% 
% $p(\theta \mid \mathbf{X}, \phi, \gamma) = \mathcal{N}\left(\theta ; \mathbf{B}_{\theta}^+ ~ \left( {'\mathbf{C}_{\phi}} \mathbf{C}_{\phi}^{-1} \mathbf{X} - \mathbf{b}_{\theta} \right), ~ \mathbf{B}_{\theta}^+ ~ (\mathbf{A} + \mathbf{I}\gamma) ~ \mathbf{B}_{\theta}^{+T} \right)$
%
% $= \mathcal{N}\left(\theta ; \left( \mathbf{B}_{\theta} \mathbf{B}_{\theta}^T \right)^{-1} \left( \sum_k \mathbf{B}_{\theta k}^T ~ \left( {'\mathbf{C}_{\phi k}} \mathbf{C}_{\phi k}^{-1} \mathbf{X}_k - \mathbf{b}_{\theta k} \right) \right), ~ \mathbf{B}_{\theta}^+ ~ (\mathbf{A} + \mathbf{I}\gamma) ~ \mathbf{B}_{\theta}^{+T} \right)$
%
% $= \prod_k \mathcal{N}\left(\theta ; \left( \mathbf{B}_{\theta} \mathbf{B}_{\theta}^T \right)^{-1} \left( \mathbf{B}_{\theta k}^T ~ \left( {'\mathbf{C}_{\phi k}} \mathbf{C}_{\phi k}^{-1} \mathbf{X}_k - \mathbf{b}_{\theta k} \right) \right), ~ \mathbf{B}_{\theta k}^+ ~ (\mathbf{A}_k + \mathbf{I}\gamma) ~ \mathbf{B}_{\theta k}^{+T} \right) \qquad (6)$

%% Rewrite ODEs as Linear Combination in Individual States
%
% We rewrite the expression $\mathbf{f}(\mathbf{X},\theta) - {'\mathbf{C}}_{\phi} \mathbf{C}_{\phi}^{-1} \mathbf{X}$ in equation (4) as a linear combination in the individual state $\mathbf{x}_u$:
%
% $\mathbf{R}_{uk} \mathbf{x}_u + \mathbf{r}_{uk} \stackrel{!}{=} \mathbf{f}_k(\mathbf{X},\theta)$.
%
% where matrices $\mathbf{R}_{uk}$ and $\mathbf{r}_{uk}$ are defined such that the ODE $\mathbf{f}_k(\mathbf{X},\theta)$ is expressed as a linear combination in the individual state $\mathbf{x}_u$.

[state.lin_comb.R,state.lin_comb.r] = rewrite_odes_as_linear_combination_in_ind_states(ode,symbols,coupling_idx.states);

%% Posterior over Individual States
%
% Given the linear combination of the ODEs w.r.t. an individual state, we define the matrices $\mathbf{B}_u$ and $\mathbf{b}_u$ such that the expression $\mathbf{f}(\mathbf{X},\theta) - {'\mathbf{C}}_{\phi} \mathbf{C}_{\phi}^{-1} \mathbf{X}$ is rewritten as a linear combination in an individual state:
%
% $\mathbf{B}_{u} \mathbf{x}_u + \mathbf{b}_{u} \stackrel{!}{=} \mathbf{f}(\mathbf{X},\theta) \qquad (7)$.
%
% Inserting (7) into (4) and solving for $\mathbf{x}_u$ yields:
%
% $\mathbf{x}_u = \mathbf{B}_{u}^+ \left( \epsilon_0 -\mathbf{b}_{u} \right)$,
%
% Since $\mathbf{C}_{\phi}$ is block diagonal we can rewrite the expression above as:
%
% $\mathbf{x}_u = \left( \mathbf{B}_{u} \mathbf{B}_{u}^T \right)^{-1} \mathbf{B}_{u}^T \sum_k \left(\epsilon_0^{(k)} -\mathbf{b}_{uk} \right)$
%
% $= \left( \mathbf{B}_{u} \mathbf{B}_{u}^T \right)^{-1} \sum_k \mathbf{B}_{uk}^T \left(\epsilon_0^{(k)} -\mathbf{b}_{uk} \right)$,
%
% where $\mathbf{B}_{u}^+$ denotes the pseudo-inverse of $\mathbf{B}_{u}$. We can therefore derive the posterior distribution over an individual state $\mathbf{x}_u$:
%
% $p(\mathbf{x}_u \mid \mathbf{X}_{-u}, \phi, \gamma) = \mathcal{N}\left(\mathbf{x}_u ; -\mathbf{B}_{u}^+ \mathbf{b}_u, ~\mathbf{B}_u^{+} ~ (\mathbf{A} + \mathbf{I}\gamma) ~ \mathbf{B}_u^{+T} \right)$
%
% $= \mathcal{N}\left(\mathbf{x}_u ; \left( \mathbf{B}_{u} \mathbf{B}_{u}^T \right)^{-1} \left( - \sum_k \mathbf{B}_{uk}^T \mathbf{b}_{uk} \right), ~\mathbf{B}_{u}^{+} ~ (\mathbf{A} + \mathbf{I}\gamma) ~ \mathbf{B}_u^{+T} \right) \qquad (8)$,
%
% with $\mathbf{X}_{-u}$ denoting the set of all states except state $\mathbf{x}_u$.

%% Mean-field Variational Inference
%
% To infer the parameters $\theta$, we want to find the maximum a posteriori estimate (MAP): 
%
% $\theta^* := arg \max_{\theta} ~ \ln p(\theta \mid \mathbf{Y},\phi,\gamma, \sigma)$
%
% $= arg\max_{\theta} ~ \ln \int  p(\theta,\mathbf{X} \mid \mathbf{Y},\phi,\gamma,\boldmath\sigma) d\mathbf{X}$
%
% $= arg\max_{\theta} ~ \ln \int p(\theta \mid \mathbf{X},\phi,\gamma) p(\mathbf{X} \mid \mathbf{Y}, \phi,\boldmath\sigma) d\mathbf{X} \qquad (9)$.
% 
% However, the integral above is intractable due to the strong couplings induced by the nonlinear ODEs $\mathbf{f}$ which appear in the term $p(\theta \mid \mathbf{X},\phi,\gamma)$. 
% 
% We use mean-field variational inference to establish variational lower bounds that are analytically tractable by decoupling state variables from the ODE parameters as well as decoupling the state variables from each other. Note that, since the ODEs described by equation (2) are *locally linear*, both conditional distributions $p(\theta \mid \mathbf{X},\mathbf{Y},\phi,\gamma,\boldmath\sigma)$ (equation (6)) and $p(\mathbf{x}_u \mid \theta, \mathbf{X}_{-u},\mathbf{Y},\phi,\gamma,\boldmath\sigma)$ (equation (8)) are analytically tractable and Gaussian distributed as mentioned previously. 
% 
% The decoupling is induced by designing a variational distribution $Q(\theta,\mathbf{X})$ which is restricted to the family of factorial distributions:
%
% $\mathcal{Q} := \bigg{\{} Q : Q(\theta,\mathbf{X}) = q(\theta) \prod_u q(\mathbf{x}_u) \bigg{\}}$.
% 
% The particular form of $q(\theta)$ and $q(\mathbf{x}_u)$ are designed to be Gaussian distributed which places them in the same family as the true full conditional distributions. To find the optimal factorial distribution we minimize the Kullback-Leibler divergence between the variational and the true posterior distribution:
%
% $\hat{Q} := arg \min_{Q(\theta,\mathbf{X}) \in \mathcal{Q}} \mathrm{KL} \left[ Q(\theta,\mathbf{X}) \mid \mid p(\theta,\mathbf{X} \mid \mathbf{Y},\phi, \gamma,\boldmath\sigma) \right] \qquad (10)$,
%
% where $\hat{Q}$ is the proxy distribution. The proxy distribution that minimizes the KL-divergence (10) depends on the true full conditionals and is given by:
%
% $\hat{q}({\theta}) \propto \exp \left(~ E_{Q_{-\theta}} \ln p(\theta \mid \mathbf{X},\mathbf{Y},\phi,\gamma,\boldmath\sigma) ~\right) \qquad (11)$
% 
% $\hat{q}(\mathbf{x}_u) \propto \exp\left( ~ E_{Q_{-u}} \ln p(\mathbf{x}_u \mid \theta, \mathbf{X}_{-u},\mathbf{Y},\phi,\gamma,\sigma) ~ \right) \qquad (12)$.

%% Fitting observations of state trajectories
%
% We fit the observations of state trajectories by standard GP regression. The data-informed distribution $p(\mathbf{X} \mid \mathbf{Y}, \phi,\boldmath\sigma)$ in euqation (9) can be determined analytically using Gaussian process regression with the GP prior $p(\mathbf{X} \mid \phi) = \prod_k \mathcal{N}(\mathbf{x}_k ; \mathbf{0},\mathbf{C}_{\phi})$:
%
% $p(\mathbf{X} \mid \mathbf{Y}, \phi,\gamma) = \prod_k \mathcal{N}(\mathbf{x}_k ; \boldmath\mu_k(\mathbf{y}_k),\boldmath\Sigma_k)$,
%
% where $\boldmath\mu_k(\mathbf{y}_k) := \sigma_k^{-2} \left(\sigma_k^{-2} \mathbf{I} + \mathbf{C}_{\boldmath\phi_k}^{-1} \right)^{-1} \mathbf{y}_k$ and $\boldmath\Sigma_k ^{-1}:=\sigma_k^{-2} \mathbf{I} + \mathbf{C}_{\boldmath\phi_k}^{-1}$.

[mu,inv_sigma] = fitting_state_observations(state,inv_C,obs_to_state_relation,simulation);

%% Coordinate Ascent Variational Gradient Matching
% 
% We minimize the KL-divergence in equation (10) by coordinate descent (where each step is analytically tractable) by iterating between determining the proxy for the distribution over ODE parameters $\hat{q}(\theta)$ and the proxies for the distribution over individual states $\hat{q}(\mathbf{x}_u)$. 

state.proxy.mean = mu;                                                     % Initialize the state estimation by the GP regression posterior
for i = 1:opt_settings.coord_ascent_numb_iter
    %%
    % <html><h4> Proxy for ODE parameters </h4></html>
    %
    % Expanding the proxy distribution in equation (11) for $\theta$ yields:
    %
    % $\hat{q}(\theta) \stackrel{(a)}{\propto} \exp \left( ~E_{Q_{-\theta}} \ln p(\theta \mid \mathbf{X},\mathbf{Y},\phi,\gamma,\sigma) ~ \right)$
    %
    % $\stackrel{(b)}{\propto} \exp \left( ~E_{Q_{-\theta}}  \ln \mathcal{N} \left( \theta; \mathbf{B}_{\theta}^+ ~ \left( '\mathbf{C}_{\phi} \mathbf{C}_{\phi}^{-1} \mathbf{X} - \mathbf{b}_{\theta} \right), ~ \mathbf{B}_{\theta}^+ ~ (\mathbf{A} + \mathbf{I}\gamma) ~ \mathbf{B}_{\theta}^{+T} \right) ~\right)$
    %
    % $= \exp \left( ~E_{Q_{-\theta}} \mathcal{N}\left(\theta ; \left( \mathbf{B}_{\theta} \mathbf{B}_{\theta}^T \right)^{-1} \left( \sum_k \mathbf{B}_{\theta k}^T ~ \left( {'\mathbf{C}_{\phi k}} \mathbf{C}_{\phi k}^{-1} \mathbf{X}_k - \mathbf{b}_{\theta k} \right) \right), ~ \mathbf{B}_{\theta}^+ ~ (\mathbf{A} + \mathbf{I}\gamma) ~ \mathbf{B}_{\theta}^{+T} \right) ~\right)$,
    %
    % which can be normalized analytically due to its exponential quadratic form. In (a) we recall that the ODE parameters depend only indirectly on the observations $\mathbf{Y}$ through the states $\mathbf{X}$ and in (b) we substitute $p(\theta \mid \mathbf{X},\phi,\gamma)$ by its density given in equation (6).

    [param_proxy_mean,param_proxy_inv_cov] = proxy_for_ode_parameters(state.proxy.mean,dC_times_invC,...
        ode_param.lin_comb,symbols,A_plus_gamma_inv,opt_settings);
    
    if i==1 || ~mod(i,20); plot_results(h_states,h_param,state,time,simulation,param_proxy_mean,p,'not_final'); end
    
    %%
    % <html><h4> Proxy for individual states </h4></html>
    %
    % Expanding the proxy distribution in equation (12) over the individual state $\mathbf{x}_u$:
    %
    % $\hat{q}(\mathbf{x}_u) \stackrel{(a)}{\propto} \exp \left( ~ E_{Q_{-u}}  \ln ( p(\mathbf{x}_u \mid \theta, \mathbf{X}_{-u},\phi,\gamma) p(\mathbf{x}_u \mid\mathbf{Y},\phi,\sigma) ) ~ \right)$
    %
    % $\stackrel{(b)}{\propto} \exp\big( ~ E_{Q_{-u}} \ln \mathcal{N}\left(\mathbf{x}_u ; -\mathbf{B}_{u}^+ \mathbf{b}_u, ~\mathbf{B}_u^{+} ~ (\mathbf{A} + \mathbf{I}\gamma) ~ \mathbf{B}_u^{+T} \right) + E_{Q_{-u}} \ln \mathcal{N}\left(\mathbf{x}_u ; \boldmath\mu_u(\mathbf{Y}), \Sigma_u \right) \big)$
    %
    % $= \exp\big( ~ E_{Q_{-u}} \ln \mathcal{N}\left(\mathbf{x}_u ; \left( \mathbf{B}_{u} \mathbf{B}_{u}^T \right)^{-1} \left( - \sum_k \mathbf{B}_{uk}^T \mathbf{b}_{uk} \right), ~\mathbf{B}_{u}^{+} ~ (\mathbf{A} + \mathbf{I}\gamma) ~ \mathbf{B}_u^{+T} \right) + E_{Q_{-u}} \ln \mathcal{N}\left(\mathbf{x}_u ; \boldmath\mu_u(\mathbf{Y}), \Sigma_u \right) \big)$,
    %
    % which, once more, can be normalized analytically due to its exponential quadratic form. In (a) we decompose the full conditional into an ODE-informed distribution and a data-informed distribution and in (b) we substitute the ODE-informed distribution $p(\mathbf{x}_u \mid \theta, \mathbf{X}_{-u},\phi,\gamma)$ with its density given by equation (8).

    [state.proxy.mean,state.proxy.inv_cov] = proxy_for_ind_states(state.lin_comb,state.proxy.mean,...
        param_proxy_mean',dC_times_invC,coupling_idx.states,symbols,mu,inv_sigma,state.obs_idx,...
        A_plus_gamma_inv,opt_settings);
end

%%
% <html><h4> Final result </h4></html>
plot_results(h_states,h_param,state,time,simulation,param_proxy_mean,p,'final');

%% Time Taken

disp(['time taken: ' num2str(toc) ' seconds'])

%% References
%
% * *Gorbach, N.S.* , *Bauer, S.* and Buhmann, J.M., Scalable Variational Inference for Dynamical Systems. 2017a. Neural Information Processing Systems (NIPS). <https://papers.nips.cc/paper/7066-scalable-variational-inference-for-dynamical-systems.pdf>, arxiv: <https://arxiv.org/abs/1705.07079>.
% * *Bauer, S.* , *Gorbach, N.S.* and Buhmann, J.M., Efficient and Flexible Inference for Stochastic Differential Equations. 2017b. Neural Information Processing Systems (NIPS). <https://papers.nips.cc/paper/7274-efficient-and-flexible-inference-for-stochastic-systems.pdf>
% * Wenk, P., Gotovos, A., Bauer, S., Gorbach, N.S., Krause, A. and Buhmann, J.M., Fast Gaussian Process Based Gradient Matching for Parameters Identification in Systems of Nonlinear ODEs. 2018. In submission to Conference on Uncertainty in Artificial Intelligence (UAI).
% * Calderhead, B., Girolami, M. and Lawrence. N.D., 2002. Accelerating Bayesian inference over nonlinear differential equation models. _In Advances in Neural Information Processing Systems (NIPS)_ . 22.
%
% The authors in bold font have contributed equally to their respective
% papers.

%% Subroutines
% <html><h4> Kernel function </h4></html>
%
% Gradient matching with Gaussian processes assumes a joint Gaussian process prior on states and their derivatives:
%
% $\left(\begin{array}{c}
% \mathbf{X} \\ \dot{\mathbf{X}}
% \end{array}\right)
%  \sim \mathcal{N} \left(
% \begin{array}{c}
% \mathbf{X} \\ \dot{\mathbf{X}}
% \end{array}; 
% \begin{array}{c}
%  \mathbf{0} \\ 
% \mathbf{0}
%  \end{array},
% \begin{array}{cc}
%  \mathbf{C}_{\phi} & \mathbf{C}_{\phi}' \\
%  '\mathbf{C}_{\phi} & \mathbf{C}_{\phi}'' 
%  \end{array}
%  \right)$,
%
% $\mathrm{cov}(x_k(t), x_k(t)) = C_{\phi_k}(t,t')$
%
% $\mathrm{cov}(\dot{x}_k(t), x_k(t)) = \frac{\partial C_{\phi_k}(t,t') }{\partial t} =: C_{\phi_k}'(t,t')$
%
% $\mathrm{cov}(x_k(t), \dot{x}_k(t)) = \frac{\partial C_{\phi_k}(t,t') }{\partial t'} =: {'C_{\phi_k}(t,t')}$
%
% $\mathrm{cov}(\dot{x}_k(t), \dot{x}_k(t)) = \frac{\partial C_{\phi_k}(t,t') }{\partial t \partial t'} =: C_{\phi_k}''(t,t')$.

function [dC_times_invC,inv_C,A_plus_gamma_inv] = kernel_function(kernel,state,time_est)

kernel.param_sym = sym('rbf_param%d',[1,2]); assume(kernel.param_sym,'real');
kernel.time1 = sym('time1'); assume(kernel.time1,'real'); kernel.time2 = sym('time2'); assume(kernel.time2,'real');
kernel.func = kernel.param_sym(1).*exp(-(kernel.time1-kernel.time2).^2./(kernel.param_sym(2).^2));                      % RBF kernel
kernel.name = 'rbf';   

%%
% kernel derivatives
for i = 1:length(kernel)
    kernel.func_d = diff(kernel.func,kernel.time1);
    kernel.func_dd = diff(kernel.func_d,kernel.time2);
    GP.fun = matlabFunction(kernel.func,'Vars',{kernel.time1,kernel.time2,kernel.param_sym});
    GP.fun_d = matlabFunction(kernel.func_d,'Vars',{kernel.time1,kernel.time2,kernel.param_sym});
    GP.fun_dd = matlabFunction(kernel.func_dd,'Vars',{kernel.time1,kernel.time2,kernel.param_sym});
end

%%
% populate GP covariance matrix
for t=1:length(time_est)
    C(t,:)=GP.fun(time_est(t),time_est,kernel.param);
    dC(t,:)=GP.fun_d(time_est(t),time_est,kernel.param);
    Cd(t,:)=GP.fun_d(time_est,time_est(t),kernel.param);
    ddC(t,:)=GP.fun_dd(time_est(t),time_est,kernel.param);
end

%%
% GP covariance scaling
[~,D] = eig(C); perturb = abs(max(diag(D))-min(diag(D))) / 10000;
if any(diag(D)<1e-6); C(logical(eye(size(C,1)))) = C(logical(eye(size(C,1)))) + perturb.*rand(size(C,1),1); end
[~,D] = eig(C);
if any(diag(D)<0); error('C has negative eigenvalues!'); elseif any(diag(D)<1e-6); warning('C is badly scaled'); end
inv_C = inv_chol(chol(C,'lower'));

dC_times_invC = dC * inv_C;

%%
% plot samples from GP prior
figure(3); 
hold on; plot(time_est,mvnrnd(zeros(1,length(time_est)),C(:,:,1),3),'LineWidth',2);
h1 = gca; h1.FontSize = 20; h1.XLabel.String = 'time'; h1.YLabel.String = 'state value';
h1.Title.String = [kernel.name ' kernel'];

%%
% determine $\mathbf{A} + \mathbf{I}\gamma$:
A = ddC - dC_times_invC * Cd;
A_plus_gamma = A + state.derivative_variance(1) .* eye(size(A));
A_plus_gamma = 0.5.*(A_plus_gamma+A_plus_gamma');            % ensure that A plus gamma is symmetric
A_plus_gamma_inv = inv_chol(chol(A_plus_gamma,'lower'));

end

%%
% <html><h4> Fitting state observations </h4></html>
%
% We fit the observations of state trajectories by standard GP regression.
%
% $p(\mathbf{X} \mid \mathbf{Y}, \phi,\gamma) = \prod_k \mathcal{N}(\mathbf{x}_k ; \boldmath\mu_k(\mathbf{y}_k),\boldmath\Sigma_k)$,
%
% where $\boldmath\mu_k(\mathbf{y}_k) := \sigma_k^{-2} \left(\sigma_k^{-2} \mathbf{I} + \mathbf{C}_{\boldmath\phi_k}^{-1} \right)^{-1} \mathbf{y}_k$ and $\boldmath\Sigma_k ^{-1}:=\sigma_k^{-2} \mathbf{I} + \mathbf{C}_{\boldmath\phi_k}^{-1}$.

function [mu_u,inv_sigma_u,state] = fitting_state_observations(state,inv_C,obs_to_state_relation,simulation)

%%
% Dimensions
numb_states = size(state.sym.mean,2);
numb_time_points = size(state.sym.mean,1);

%%
% Variance of state observations
state_obs_variance = simulation.state_obs_variance(state.obs); 

%%
% Form block-diagonal matrix out of $\mathbf{C}_{\boldmath\phi_k}^{-1}$
inv_C_replicas = num2cell(inv_C(:,:,ones(1,numb_states)),[1,2]);
inv_C_blkdiag = sparse(blkdiag(inv_C_replicas{:})); 

%%
% GP posterior inverse covariance matrix: $\boldmath\Sigma_k ^{-1}:=\sigma_k^{-2} \mathbf{I} + \mathbf{C}_{\boldmath\phi_k}^{-1}$
dim = size(state_obs_variance,1)*size(state_obs_variance,2);
D = spdiags(reshape(state_obs_variance.^(-1),[],1),0,dim,dim) * speye(dim); % covariance matrix of error term (big E)
A_times_D_times_A = obs_to_state_relation' * D * obs_to_state_relation;
inv_sigma = A_times_D_times_A + inv_C_blkdiag;

%%
% GP posterior mean: $\boldmath\mu_k(\mathbf{y}_k) := \sigma_k^{-2} \left(\sigma_k^{-2} \mathbf{I} + \mathbf{C}_{\boldmath\phi_k}^{-1} \right)^{-1} \mathbf{y}_k
mu = inv_sigma \ obs_to_state_relation' * D * reshape(state.obs,[],1);

%%
% Reshape GP mean
mu_u = zeros(numb_time_points,numb_states);
for u = 1:numb_states
    idx = (u-1)*numb_time_points+1:(u-1)*numb_time_points+numb_time_points;
    mu_u(:,u) = mu(idx);
end

%%
% Reshape GP inverse covariance matrix
inv_sigma_u = zeros(numb_time_points,numb_time_points,numb_states);
for i = 1:numb_states
    idx = [(i-1)*numb_time_points+1:(i-1)*numb_time_points+numb_time_points];
    inv_sigma_u(:,:,i) = inv_sigma(idx,idx);
end

end

%%
% <html><h4> Find state ODE couplings </h4></html>
function coupling_idx = find_state_couplings_in_odes(ode,symbols)

state_sym = sym('state%d',[1,length(ode.system)]); assume(state_sym,'real');
for k = 1:length(ode.system)
    tmp_idx = ismember(state_sym,symvar(ode.system_sym(k))); tmp_idx(:,k) = 1;
    ode_couplings_states(k,tmp_idx) = 1; 
end

for u = 1:length(symbols.state)
    coupling_idx.states{u} = find(ode_couplings_states(:,u));  
end

end

%%
% <html><h4> Rewrite ODEs as linear combination in parameters </h4></html>
%
% $\mathbf{B}_{\theta k} \theta + \mathbf{b}_{\theta k} \stackrel{!}{=} \mathbf{f}_k(\mathbf{X},\theta)$,
%
% where matrices $\mathbf{B}_{\theta k}$ and $\mathbf{b}_{\theta k}$ are defined such that the ODEs $\mathbf{f}_k(\mathbf{X},\theta)$ are expressed as a linear combination in $\theta$.

function [B,b] = rewrite_odes_as_linear_combination_in_parameters(ode,symbols)

%%
% Initialization of symbolic variables
param_sym = sym('param%d',[1,length(symbols.param)]); assume(param_sym,'real');
state_sym = sym('state%d',[1,length(symbols.state)]); assume(state_sym,'real');
state0_sym = sym('state0'); assume(state0_sym,'real');
state_const_sym = sym('state_const'); assume(state_const_sym,'real');

%%
% Rewrite ODEs as linear combinations in parameters (global)
[B_sym,b_sym] = equationsToMatrix(ode.system_sym,param_sym);
b_sym = -b_sym; % See the documentation of the function "equationsToMatrix"

%%
% Operations locally w.r.t. ODEs
for k = 1:length(ode.system)
    B_sym(k,B_sym(k,:)=='0') = state0_sym;
    for i = 1:length(B_sym(k,:))
        sym_var = symvar(B_sym(k,i));
        if isempty(sym_var)
            B_sym(k,i) = B_sym(k,i) + state0_sym;
        end
    end
    B{k} = matlabFunction(B_sym(k,:),'Vars',{state_sym,state0_sym,state_const_sym});
    b{k} = matlabFunction(b_sym(k,:),'Vars',{state_sym,state0_sym,state_const_sym});
end

end

%%
% <html><h4> Rewrite ODEs as linear combination in individual states </h4></html>
%
% $\mathbf{R}_{uk} \mathbf{x}_u + \mathbf{r}_{uk} \stackrel{!}{=} \mathbf{f}_k(\mathbf{X},\theta)$.
%
% where matrices $\mathbf{R}_{uk}$ and $\mathbf{r}_{uk}$ are defined such that the ODEs $\mathbf{f}_k(\mathbf{X},\theta)$ is rewritten as a linear combination in the individual state $\mathbf{x}_u$.

function [R,r] = rewrite_odes_as_linear_combination_in_ind_states(ode,symbols,coupling_idx)

%%
% Initialization of symbolic variables
param_sym = sym('param%d',[1,length(symbols.param)]); assume(param_sym,'real');
state_sym = sym('state%d',[1,length(symbols.state)]); assume(state_sym,'real');
state0_sym = sym('state0'); assume(state0_sym,'real');
state_const_sym = sym('state_const'); assume(state_const_sym,'real');

%%
% Rewrite ODEs as linear combinations in parameters (locally)
for u = 1:length(symbols.state)
    for k = coupling_idx{u}'
        [R_sym,r_sym] = equationsToMatrix(ode.system{k}(state_sym,param_sym'),state_sym(:,u));
        r_sym = -r_sym; % See the documentation of the function "equationsToMatrix"
        
        R{u,k} = matlabFunction(R_sym,'Vars',{state_sym,param_sym});
        r{u,k} = matlabFunction(r_sym,'Vars',{state_sym,param_sym});
    end
end

end

%%
% <html><h4> Proxy for ODE parameters </h4></html>
%
% $\hat{q}(\theta) {\propto} \exp \left( ~E_{Q_{-\theta}}  \ln \mathcal{N}\left(\theta ; \left( \mathbf{B}_{\theta} \mathbf{B}_{\theta}^T \right)^{-1} \left( \sum_k \mathbf{B}_{\theta k}^T ~ \left( {'\mathbf{C}_{\phi k}} \mathbf{C}_{\phi k}^{-1} \mathbf{X}_k - \mathbf{b}_{\theta k} \right) \right), ~ \mathbf{B}_{\theta}^+ ~ (\mathbf{A} + \mathbf{I}\gamma) ~ \mathbf{B}_{\theta}^{+T} \right) ~\right)$,

function [param_proxy_mean,param_proxy_inv_cov] = proxy_for_ode_parameters(state_proxy_mean,...
    dC_times_invC,lin_comb,symbols,A_plus_gamma_inv,opt_settings)

%%
% Initialization
state0 = zeros(size(dC_times_invC,1),1);
param_proxy_inv_cov = zeros(length(symbols.param));
global_scaling = zeros(length(symbols.param));
global_mean = zeros(length(symbols.param),1);

%%
% Iteratate through ODEs
for k = 1:length(symbols.state)
    
    %%
    % unpack matrices $\mathbf{B}$ and $\mathbf{b}$
    B = lin_comb.B{k}(state_proxy_mean,state0,ones(size(state_proxy_mean,1),1));
    b = lin_comb.b{k}(state_proxy_mean,state0,ones(size(state_proxy_mean,1),1));
    
    %%
    % Local
    if strcmp(opt_settings.pseudo_inv_type,'Moore-Penrose')
        % local mean: $\mathbf{B}_{\theta k}^T ~ \left( {'\mathbf{C}_{\phi_k}} \mathbf{C}_{\phi k}^{-1} \mathbf{X}_k - \mathbf{b}_{\theta k}$
        local_mean = B' * (dC_times_invC * state_proxy_mean(:,k) - b); 
        local_scaling = B' * B;
        local_inv_cov = B' * A_plus_gamma_inv * B;
    elseif strcmp(opt_settings.pseudo_inv_type,'modified Moore-Penrose')
        local_mean = B' * A_plus_gamma_inv * (dC_times_invC * state_proxy_mean(:,k) - b);   
        local_scaling = B' * A_plus_gamma_inv * B;
        local_inv_cov = local_scaling;
    end   
    
    %%
    % Global
    global_mean = global_mean + local_mean;
    global_scaling = global_scaling + local_scaling;
    
    % Inverse covariance of ODE param proxy distribution
    param_proxy_inv_cov = param_proxy_inv_cov + local_inv_cov;
end

%%
% Check scaling of covariance matrix
[~,D] = eig(param_proxy_inv_cov);
if any(diag(D)<0)
    warning('param_proxy_inv_cov has negative eigenvalues!');
elseif any(diag(D)<1e-3)
    warning('param_proxy_inv_cov is badly scaled')
    disp('perturbing diagonal of param_proxy_inv_cov')
    perturb = abs(max(diag(D))-min(diag(D))) / 10000;
    param_proxy_inv_cov(logical(eye(size(param_proxy_inv_cov,1)))) = param_proxy_inv_cov(logical(eye(size(param_proxy_inv_cov,1)))) ...
        + perturb.*rand(size(param_proxy_inv_cov,1),1);
end

%%
% Mean of parameter proxy distribution (option: Moore-penrose inverse example): $\left( \mathbf{B}_{\theta} \mathbf{B}_{\theta}^T \right)^{-1} \left( \sum_k \mathbf{B}_{\theta k}^T ~ \left( {'\mathbf{C}_{\phi k}} \mathbf{C}_{\phi k}^{-1} \mathbf{X}_k - \mathbf{b}_{\theta k} \right) \right)$
param_proxy_mean = global_scaling \ global_mean;
param_proxy_mean = abs(param_proxy_mean);      % mirroring to preserve magnitude

end

%%
% <html><h4> Proxy for individual states </h4></html>
%
% $\hat{q}(\mathbf{x}_u) \propto \exp\big( ~ E_{Q_{-u}} \ln \mathcal{N}\left(\mathbf{x}_u ; \left( \mathbf{B}_{u} \mathbf{B}_{u}^T \right)^{-1} \left( - \sum_k \mathbf{B}_{uk}^T \mathbf{b}_{uk} \right), ~\mathbf{B}_{u}^{+} ~ (\mathbf{A} + \mathbf{I}\gamma) ~ \mathbf{B}_u^{+T} \right) + E_{Q_{-u}} \ln \mathcal{N}\left(\mathbf{x}_u ; \boldmath\mu_u(\mathbf{Y}), \Sigma_u \right) \big)$,

function [state_proxy_mean,state_proxy_inv_cov] = proxy_for_ind_states(lin_comb,state_proxy_mean,...
    ode_param,dC_times_invC,coupling_idx,symbols,mu,inv_sigma,state_obs_idx,...
    A_plus_gamma_inv,opt_settings)

%%
% Clamp observed states to GP fit
if opt_settings.clamp_obs_state_to_GP_fit
    state_enumeration = find(~state_obs_idx);
else
    state_enumeration = 1:length(symbols.state);
end

for u = state_enumeration
    
    %%
    % Initialization
    state_proxy_inv_cov(:,:,u) = zeros(size(dC_times_invC));
    global_scaling = zeros(size(dC_times_invC));
    global_mean = zeros(size(dC_times_invC,1),1);
    
    %%
    % Iteratate through ODEs
    for k = coupling_idx{u}'
        
        %% 
        % unpack matrices $\mathbf{R}$ and $\mathbf{r}$
        R = diag(lin_comb.R{u,k}(state_proxy_mean,ode_param));
        r = lin_comb.r{u,k}(state_proxy_mean,ode_param);
        if size(R,1) == 1; R = R.*eye(size(dC_times_invC,1)); end
        if length(r)==1; r = zeros(length(global_mean),1); end
         
        %%
        % Define matrices B and b such that $\mathbf{B}_{uk} \mathbf{x}_u + \mathbf{b}_{uk} \stackrel{!}{=} \mathbf{f}_k(\mathbf{X},\theta) - {'\mathbf{C}}_{\phi_{k}} \mathbf{C}_{\phi_{k}}^{-1} \mathbf{X}$
        if k~=u
            B = R;
            b = r - dC_times_invC * state_proxy_mean(:,k);
        else
            B = R - dC_times_invC;
            b = r;
        end
        
        %%
        % Local
        if strcmp(opt_settings.pseudo_inv_type,'Moore-Penrose')
            % local mean: $\mathbf{B}_{uk}^T \left(\epsilon_0^{(k)} -\mathbf{b}_{uk}
            local_mean = -B' * b;
            local_scaling = B' * B;
            local_inv_cov = B' * A_plus_gamma_inv * B;
        elseif strcmp(opt_settings.pseudo_inv_type,'modified Moore-Penrose')
            local_mean = -B' * A_plus_gamma_inv * b;
            local_scaling = B' * A_plus_gamma_inv * B;
            local_inv_cov = local_scaling;
        end
          
        %%
        % Global
        global_mean = global_mean + local_mean;
        global_scaling = global_scaling + local_scaling;
        
        %%
        % Inverse covariance for state proxy distribution
        state_proxy_inv_cov(:,:,u) = state_proxy_inv_cov(:,:,u) + local_inv_cov;
    end
    
    %%
    % Mean of state proxy distribution (option: Moore-penrose inverse example): $\left( \mathbf{B}_{u} \mathbf{B}_{u}^T \right)^{-1} \sum_k \mathbf{B}_{uk}^T \left(\epsilon_0^{(k)} -\mathbf{b}_{uk} \right)$
    state_proxy_mean(:,u) = (global_scaling + inv_sigma(:,:,u)) \ (global_mean + (inv_sigma(:,:,u) * mu(:,u)));
end

end

%%
% <html><h4> Import ODEs </h4></html>
function ode = import_odes(symbols)

%%
% Path to system of ODEs
path_ode = './Lotka_Volterra_ODEs.txt';                                                  

%%
% Import ODEs
ode.raw = importdata(path_ode);
ode.refined = ode.raw;

%%
% Refine ODEs
for k = 1:length(ode.refined)
for u = 1:length(symbols.state); ode.refined{k} = strrep(ode.refined{k},[symbols.state{u}],['state(:,' num2str(u) ')']); end 
for j = 1:length(symbols.param); ode.refined{k} = strrep(ode.refined{k},symbols.param{j},['param(' num2str(j) ')']); end
end
for k = 1:length(ode.refined); ode.system{k} = str2func(['@(state,param)(' ode.refined{k} ')']); end

end

%%
% <html><h4> Generate ground truth </h4></html>
function [state,time,ode] = generate_ground_truth(time,state,ode,symbols,simulation)

%%
% Integration times
time.true=0:simulation.int_interval:simulation.final_time;                 % ture times
Tindex=length(time.true);                                                  % index time
TTT=length(simulation.time_samp);                                          % number of sampled points
itrue=round(simulation.time_samp./simulation.int_interval+ones(1,TTT));    % Index of sample time in the true time

%%
% Symbolic computations
param_sym = sym('param%d',[1,length(symbols.param)]); assume(param_sym,'real');
state_sym = sym('state%d',[1,length(symbols.state)]); assume(state_sym,'real');
for i = 1:length(ode.system)
    ode.system_sym(i) = ode.system{i}(state_sym,param_sym);
end

%%
% Fourth order Runge-Kutta (numerical) integration
ode_system_mat = matlabFunction(ode.system_sym','Vars',{state_sym',param_sym'});
[~,OutX_solver]=ode45(@(t,x) ode_system_mat(x,simulation.ode_param'), time.true, simulation.init_val);
state.true_all=OutX_solver;
state.true=state.true_all(itrue,:);

%%
% Pack
state.obs_idx = simulation.state_obs_idx;

end

%%
% <html><h4> Generate observations of states </h4></html>
function [state,time,obs_to_state_relation] = generate_state_obs(state,time,simulation)

%%
% State observations
state_obs_variance = simulation.state_obs_variance(state.true);
state.obs = state.true + sqrt(state_obs_variance) .* randn(size(state.true));

%%
% Mapping between states and observations
if length(simulation.time_samp) < length(time.est)
    time.idx = munkres(pdist2(simulation.time_samp',time.est'));
    time.ind = sub2ind([length(simulation.time_samp),length(time.est)],1:length(simulation.time_samp),time.idx);
else
    time.idx = munkres(pdist2(time.est',simulation.time_samp'));
    time.ind = sub2ind([length(time.est),length(simulation.time_samp)],1:length(time.est),time.idx);
end
time.obs_time_to_state_time_relation = zeros(length(simulation.time_samp),length(time.est)); time.obs_time_to_state_time_relation(time.ind) = 1;
state_mat = eye(size(state.true,2));
obs_to_state_relation = sparse(kron(state_mat,time.obs_time_to_state_time_relation));
time.samp = simulation.time_samp;

end

%%
% <html><h4> Setup plots </h4></html>
function [h_states,h_param,p] = setup_plots(state,time,simulation,symbols)

%%
% Refine ODE parameter symbols
for i = 1:length(symbols.param); symbols.param{i} = symbols.param{i}(2:end-1); end
%%
% Figure size and position setup
figure(1); set(1, 'Position', [0, 200, 1200, 500]);
%%
% ODE parameters
h_param = subplot(1,3,1); h_param.FontSize = 20; h_param.Title.String = 'ODE parameters';
set(gca,'XTick',[1:length(symbols.param)]); set(gca,'XTickLabel',symbols.param);
hold on; drawnow
%%
% States
for u = 1:2
    h_states{u} = subplot(1,3,u+1); cla; p.true = plot(time.true,state.true_all(:,u),'LineWidth',2,'Color',[217,95,2]./255); 
    hold on; p.obs = plot(simulation.time_samp,state.obs(:,u),'*','Color',[217,95,2]./255,'MarkerSize',10);
    h_states{u}.FontSize = 20; h_states{u}.Title.String = symbols.state{u}(2:end-1); h_states{u}.XLim = [min(time.est),max(time.est)];
    h_states{u}.XLabel.String = 'time'; hold on;
end

end

%%
% <html><h4> Plot results </h4></html>
function plot_results(h_states,h_param,state,time,simulation,param_proxy_mean,p,plot_type)

for u = 1:2
    if strcmp(plot_type,'final')
        
        % State proxy variance
        state_proxy_variance = diag(state.proxy.inv_cov(:,:,u)^(-1));
        shaded_region = [state.proxy.mean(:,u)+1*sqrt(state_proxy_variance); flip(state.proxy.mean(:,u)-1*sqrt(state_proxy_variance),1)];  
        f = fill(h_states{u},[time.est'; flip(time.est',1)], shaded_region, [222,235,247]/255); set(f,'EdgeColor','None');
        
        % Replot true states
        p.true = plot(h_states{u},time.true,state.true_all(:,u),'LineWidth',2,'Color',[217,95,2]./255); 
        
        % Replot state obbservations
        p.obs = plot(h_states{u},simulation.time_samp,state.obs(:,u),'*','Color',[217,95,2]./255,'MarkerSize',10);
        
        % State proxy mean (final)
        hold on; p.est = plot(h_states{u},time.est,state.proxy.mean(:,u),'Color',[117,112,179]./255,'LineWidth',2);
    else
        % state proxy mean (not final)
        hold on; p.est = plot(h_states{u},time.est,state.proxy.mean(:,u),'LineWidth',0.1,'Color',[0.8,0.8,0.8]); 
    end
    % Specify legend entries
    legend(h_states{u},[p.true,p.obs,p.est],{'true','observed','estimate'},'Location','southwest'); 
end

% ODE parameters
cla(h_param); b = bar(h_param,1:length(param_proxy_mean),[simulation.ode_param',param_proxy_mean]);
b(1).FaceColor = [217,95,2]./255; b(2).FaceColor = [117,112,179]./255;
h_param.XLim = [0.5,length(param_proxy_mean)+0.5]; h_param.YLimMode = 'auto';   
legend(h_param,{'true','estimate'},'Location','northwest');
drawnow

end

##### SOURCE END #####
--></body></html>