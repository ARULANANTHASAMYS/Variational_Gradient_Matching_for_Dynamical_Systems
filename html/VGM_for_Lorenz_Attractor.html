
<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   <!--
This HTML was auto-generated from MATLAB code.
To make changes, update the MATLAB code and republish this document.
      --><title>Variational Gradient Matching for Dynamical Systems: Lorenz Attractor</title><meta name="generator" content="MATLAB 9.2"><link rel="schema.DC" href="http://purl.org/dc/elements/1.1/"><meta name="DC.date" content="2018-05-03"><meta name="DC.source" content="VGM_for_Lorenz_Attractor.m"><style type="text/css">
html,body,div,span,applet,object,iframe,h1,h2,h3,h4,h5,h6,p,blockquote,pre,a,abbr,acronym,address,big,cite,code,del,dfn,em,font,img,ins,kbd,q,s,samp,small,strike,strong,sub,sup,tt,var,b,u,i,center,dl,dt,dd,ol,ul,li,fieldset,form,label,legend,table,caption,tbody,tfoot,thead,tr,th,td{margin:0;padding:0;border:0;outline:0;font-size:100%;vertical-align:baseline;background:transparent}body{line-height:1}ol,ul{list-style:none}blockquote,q{quotes:none}blockquote:before,blockquote:after,q:before,q:after{content:'';content:none}:focus{outine:0}ins{text-decoration:none}del{text-decoration:line-through}table{border-collapse:collapse;border-spacing:0}

html { min-height:100%; margin-bottom:1px; }
html body { height:100%; margin:0px; font-family:Arial, Helvetica, sans-serif; font-size:10px; color:#000; line-height:140%; background:#fff none; overflow-y:scroll; }
html body td { vertical-align:top; text-align:left; }

h1 { padding:0px; margin:0px 0px 25px; font-family:Arial, Helvetica, sans-serif; font-size:1.5em; color:#d55000; line-height:100%; font-weight:normal; }
h2 { padding:0px; margin:0px 0px 8px; font-family:Arial, Helvetica, sans-serif; font-size:1.2em; color:#000; font-weight:bold; line-height:140%; border-bottom:1px solid #d6d4d4; display:block; }
h3 { padding:0px; margin:0px 0px 5px; font-family:Arial, Helvetica, sans-serif; font-size:1.1em; color:#000; font-weight:bold; line-height:140%; }

a { color:#005fce; text-decoration:none; }
a:hover { color:#005fce; text-decoration:underline; }
a:visited { color:#004aa0; text-decoration:none; }

p { padding:0px; margin:0px 0px 20px; }
img { padding:0px; margin:0px 0px 20px; border:none; }
p img, pre img, tt img, li img, h1 img, h2 img { margin-bottom:0px; } 

ul { padding:0px; margin:0px 0px 20px 23px; list-style:square; }
ul li { padding:0px; margin:0px 0px 7px 0px; }
ul li ul { padding:5px 0px 0px; margin:0px 0px 7px 23px; }
ul li ol li { list-style:decimal; }
ol { padding:0px; margin:0px 0px 20px 0px; list-style:decimal; }
ol li { padding:0px; margin:0px 0px 7px 23px; list-style-type:decimal; }
ol li ol { padding:5px 0px 0px; margin:0px 0px 7px 0px; }
ol li ol li { list-style-type:lower-alpha; }
ol li ul { padding-top:7px; }
ol li ul li { list-style:square; }

.content { font-size:1.2em; line-height:140%; padding: 20px; }

pre, code { font-size:12px; }
tt { font-size: 1.2em; }
pre { margin:0px 0px 20px; }
pre.codeinput { padding:10px; border:1px solid #d3d3d3; background:#f7f7f7; }
pre.codeoutput { padding:10px 11px; margin:0px 0px 20px; color:#4c4c4c; }
pre.error { color:red; }

@media print { pre.codeinput, pre.codeoutput { word-wrap:break-word; width:100%; } }

span.keyword { color:#0000FF }
span.comment { color:#228B22 }
span.string { color:#A020F0 }
span.untermstring { color:#B20000 }
span.syscmd { color:#B28C00 }

.footer { width:auto; padding:10px 0px; margin:25px 0px 0px; border-top:1px dotted #878787; font-size:0.8em; line-height:140%; font-style:italic; color:#878787; text-align:left; float:none; }
.footer p { margin:0px; }
.footer a { color:#878787; }
.footer a:hover { color:#878787; text-decoration:underline; }
.footer a:visited { color:#878787; }

table th { padding:7px 5px; text-align:left; vertical-align:middle; border: 1px solid #d6d4d4; font-weight:bold; }
table td { padding:7px 5px; text-align:left; vertical-align:top; border:1px solid #d6d4d4; }





  </style></head><body><div class="content"><h1>Variational Gradient Matching for Dynamical Systems: Lorenz Attractor</h1><!--introduction--><!--/introduction--><h2>Contents</h2><div><ul><li><a href="#1">.</a></li><li><a href="#2"><b>Authors</b>:</a></li><li><a href="#3">Contents:</a></li><li><a href="#4">User Input: Simulation Settings</a></li><li><a href="#10">User Input: Estimation Settings</a></li><li><a href="#16">Preprocessing</a></li><li><a href="#19">Mass Action Dynamical Systems</a></li><li><a href="#21">Simulate Trajectories</a></li><li><a href="#26">Prior on States and State Derivatives</a></li><li><a href="#27">Matching Gradients</a></li><li><a href="#30">Rewrite ODEs as Linear Combination in Parameters</a></li><li><a href="#33">Posterior over ODE Parameters</a></li><li><a href="#35">Rewrite ODEs as Linear Combination in Individual States</a></li><li><a href="#38">Posterior over Individual States</a></li><li><a href="#40">Mean-field Variational Inference</a></li><li><a href="#42">Fitting observations of state trajectories</a></li><li><a href="#45">Coordinate Ascent Variational Gradient Matching</a></li><li><a href="#56">Time Taken</a></li><li><a href="#59">References</a></li></ul></div><h2 id="1">.</h2><h2 id="2"><b>Authors</b>:</h2><p><b>Nico Stephan Gorbach</b> and <b>Stefan Bauer</b>, email: <a href="mailto:nico.gorbach@gmail.com">nico.gorbach@gmail.com</a></p><h2 id="3">Contents:</h2><p>Instructional code for the NIPS (2018) paper <a href="https://papers.nips.cc/paper/7066-scalable-variational-inference-for-dynamical-systems.pdf">Scalable Variational Inference for Dynamical Systems</a> by Nico S. Gorbach, Stefan Bauer and Joachim M. Buhmann. Please cite our paper if you use our program for a further publication. The derivations in this document are also given in the doctoral thesis <a href="https://www.research-collection.ethz.ch/handle/20.500.11850/261734">https://www.research-collection.ethz.ch/handle/20.500.11850/261734</a> as well as in parts of Wenk et al. (2018).</p><p>Example dynamical system used in this code:* Lorenz attractor* system with the* y-dimension unobserved*. The ODE parameters are also unobserved.</p><h2 id="4">User Input: Simulation Settings</h2><div><ul><li><b>true ODE parameter</b></li></ul></div><pre>               Input a row vector of real numbers of size 1 x 3:</pre><pre class="codeinput">        simulation.ode_param = [10,28,8/3];
</pre><p></p><div><ul><li><b>final time for simulation</b></li></ul></div><pre>               Input a positive real number:</pre><pre class="codeinput">        simulation.final_time = 20;
</pre><p></p><div><ul><li><b>observation noise</b></li></ul></div><pre>               Input a function handle:</pre><pre class="codeinput">        simulation.state_obs_variance = @(mean)(bsxfun(@times,[2,2],ones(size(mean))));
</pre><p></p><div><ul><li><b>time interval between observations</b></li></ul></div><pre>               Input a positive real number:</pre><pre class="codeinput">        simulation.interval_between_observations = 0.1;
</pre><p></p><h2 id="10">User Input: Estimation Settings</h2><div><ul><li>*Kernel parameters *$\mathbf\phi$</li></ul></div><pre>               Input a row vector of positive real numbers of size 1 x
2:</pre><pre class="codeinput">        kernel.param = [10,0.2];
</pre><p></p><div><ul><li><b>Error variance on state derivatives (i.e.</b> <img src="VGM_for_Lorenz_Attractor_eq17096441642737911057.png" alt="$\gamma$" style="width:6px;height:8px;">*)*</li></ul></div><pre>               Input a row vector of positive real numbers of size 1 x
3:</pre><pre class="codeinput">        state.derivative_variance = [6,6,6];
</pre><p></p><div><ul><li><b>Estimation times</b></li></ul></div><pre>               Input a row vector of positive real numbers in ascending
order:</pre><pre class="codeinput">        time.est = 0:0.1:20;
</pre><p>Preliminary operations</p><pre class="codeinput">close <span class="string">all</span>; clc; addpath(<span class="string">'VGM_functions'</span>)
</pre><h2 id="16">Preprocessing</h2><pre class="codeinput">[symbols,simulation,ode,odes_path,coupling_idx,opt_settings,plot_settings] = preprocessing_Lorenz_Attractor (simulation);
</pre><pre class="codeoutput"> 
ODEs:
 
/  d x                   \
|  --- == -sigma (x - y) |
|   dt                   |
|                        |
| d y                    |
| --- == rho x - y - x z |
|  dt                    |
|                        |
|  d z                   |
|  --- == x y - lambda z |
\   dt                   /

</pre><h2 id="19">Mass Action Dynamical Systems</h2><p>A deterministic dynamical system is represented by a set of <img src="VGM_for_Lorenz_Attractor_eq03845174387838694102.png" alt="$K$" style="width:10px;height:8px;"> ordinary differential equations (ODEs) with model parameters <img src="VGM_for_Lorenz_Attractor_eq18305986736779408640.png" alt="$\mathbf\theta \in \mathcal{R}^d$" style="width:33px;height:10px;"> that describe the evolution of <img src="VGM_for_Lorenz_Attractor_eq03845174387838694102.png" alt="$K$" style="width:10px;height:8px;"> states <img src="VGM_for_Lorenz_Attractor_eq16690292594929342180.png" alt="$\mathbf{x}(t) = [x_1(t),\ldots, x_K(t)]^T$" style="width:120px;height:13px;"> such that:</p><p><img src="VGM_for_Lorenz_Attractor_eq15519211153938804350.png" alt="$\dot{\mathbf{x}}(t) = \frac{d \mathbf{x}(t)}{d t} = \mathbf{f}(\mathbf{x}(t),\mathbf\theta) \qquad (1)$" style="width:147px;height:16px;">,</p><p>A sequence of observations, <img src="VGM_for_Lorenz_Attractor_eq17565748849429239454.png" alt="$\mathbf{y}(t)$" style="width:19px;height:11px;">, is usually contaminated by measurement error which we assume to be normally distributed with zero mean and variance for each of the <img src="VGM_for_Lorenz_Attractor_eq03845174387838694102.png" alt="$K$" style="width:10px;height:8px;"> states, i.e. <img src="VGM_for_Lorenz_Attractor_eq00997383903036395961.png" alt="$\mathbf{E}\sim\mathcal{N}(\mathbf{E};\mathbf{0},\mathbf{D})$" style="width:75px;height:12px;">, with <img src="VGM_for_Lorenz_Attractor_eq02125912780065424198.png" alt="$\mathbf{D}_{ik}=\sigma_k ^2 \delta_{ik}$" style="width:55px;height:13px;">. For <img src="VGM_for_Lorenz_Attractor_eq03672095713503266041.png" alt="$N$" style="width:10px;height:8px;"> distinct time points the overall system may therefore be summarized as</p><p><img src="VGM_for_Lorenz_Attractor_eq05988414051423361030.png" alt="$\mathbf{Y} = \mathbf{X} + \mathbf{E}$" style="width:56px;height:9px;">,</p><p>where</p><p><img src="VGM_for_Lorenz_Attractor_eq13585074033575565705.png" alt="$\mathbf{X} = [\mathbf{x}(t_1),\ldots,\mathbf{x}(t_N)] =  [\mathbf{x}_1,\ldots,\mathbf{x}_K]^T$" style="width:181px;height:13px;">,</p><p><img src="VGM_for_Lorenz_Attractor_eq05721247787040501698.png" alt="$\mathbf{Y} = [\mathbf{y}(t_1),\ldots,\mathbf{y}(t_N)] =  [\mathbf{y}_1,\ldots,\mathbf{y}_K]^T$" style="width:182px;height:13px;">,</p><p>and <img src="VGM_for_Lorenz_Attractor_eq15380491105506292187.png" alt="$\mathbf{x}_k = [x_k(t_1),\ldots,x_k(t_N)]^T$" style="width:122px;height:13px;"> is the <img src="VGM_for_Lorenz_Attractor_eq15636846968047188835.png" alt="$k$" style="width:6px;height:8px;">'th state sequence and <img src="VGM_for_Lorenz_Attractor_eq05965047012017685170.png" alt="$\mathbf{y}_k = [y_k(t_1),\ldots,y_k(t_N)]^T$" style="width:120px;height:13px;"> are the observations. Given the observations <img src="VGM_for_Lorenz_Attractor_eq00013651220649516337.png" alt="$\mathbf{Y}$" style="width:10px;height:8px;"> and the description of the dynamical system (1), the aim is to estimate both state variables <img src="VGM_for_Lorenz_Attractor_eq03397130480831257552.png" alt="$\mathbf{X}$" style="width:9px;height:8px;"> and parameters <img src="VGM_for_Lorenz_Attractor_eq14439582888765908571.png" alt="$\mathbf\theta$" style="width:5px;height:8px;">.</p><p>We consider only dynamical systems that are _*locally linear _*with respect to ODE parameters <img src="VGM_for_Lorenz_Attractor_eq14439582888765908571.png" alt="$\mathbf\theta$" style="width:5px;height:8px;"> and individual states <img src="VGM_for_Lorenz_Attractor_eq08291690262771002032.png" alt="$\mathbf{x}$" style="width:7px;height:5px;">. Such ODEs include mass-action kinetics and are given by:</p><p><img src="VGM_for_Lorenz_Attractor_eq07431399494256856402.png" alt="$f_{k}(\mathbf{x}(t),\theta) = \sum_{i=1} \theta_{ki} \prod_{j \in\mathcal{M}_{ki}} x_j \qquad (2)$" style="width:185px;height:13px;">,</p><p>with $\mathcal{M}_{ki} \subseteq \{ 1, \dots, K\}$describing the state variables in each factor of the equation (i.e. the functions are linear in parameters and contain arbitrary large products of monomials of the states).</p><h2 id="21">Simulate Trajectories</h2><pre class="codeinput">[simulation,obs_to_state_relation,fig_handle,plot_handle] = simulate_state_dynamics(simulation,state,symbols,ode,odes_path,time,plot_settings);
</pre><img vspace="5" hspace="5" src="VGM_for_Lorenz_Attractor_01.png" style="width:1600px;height:800px;" alt=""> <p>start timer</p><pre class="codeinput">tic;
</pre><h2 id="26">Prior on States and State Derivatives</h2><p>Gradient matching with Gaussian processes assumes a joint Gaussian process prior on states and their derivatives:</p><p><img src="VGM_for_Lorenz_Attractor_eq16994150251550570805.png" alt="$\left(\begin{array}{c} \mathbf{X} \\ \dot{\mathbf{X}} \end{array}\right) \sim \mathcal{N} \left(\begin{array}{c} \mathbf{X} \\ \dot{\mathbf{X}} \end{array}~;~\begin{array}{c} \mathbf{0} \\\mathbf{0} \end{array}~,~\begin{array}{cc} \mathbf{C}_{\mathbf\phi} &amp; \mathbf{C}_{\mathbf\phi}' \\ '\mathbf{C}_{\mathbf\phi} &amp; \mathbf{C}_{\mathbf\phi}'' \end{array} \right) \qquad (3)$" style="width:231px;height:28px;">,</p><p>with</p><p><img src="VGM_for_Lorenz_Attractor_eq06131563045232954702.png" alt="$\mathrm{cov}(x_k(t), x_k(t)) = C_{\mathbf\phi_k}(t,t')$" style="width:131px;height:12px;">,</p><p><img src="VGM_for_Lorenz_Attractor_eq09524647897482425600.png" alt="$\mathrm{cov}(\dot{x}_k(t), x_k(t)) = \frac{\partial C_{\mathbf{\phi}_k}(t,t')}{\partial t} =: C_{{\mathbf\phi}_k}'(t,t')$" style="width:185px;height:18px;">,</p><p><img src="VGM_for_Lorenz_Attractor_eq09454301398506501108.png" alt="$\mathrm{cov}(x_k(t), \dot{x}_k(t)) = \frac{\partial C_{\mathbf\phi_k}(t,t')}{\partial t'} =: {'C_{\mathbf\phi_k}(t,t')}$" style="width:187px;height:17px;">,</p><p>$\mathrm{cov}(\dot{x}_k(t), \dot{x}_k(t)) = \frac{\partialC_{\mathbf\phi_k}(t,t') }{\partial t \partial t'} =: C_{\mathbf\phi_k}''(t,t')$.</p><pre class="error">Error updating Text.

 Character vector must have valid interpreter syntax: 
$\mathrm{cov}(\dot{x}_k(t), \dot{x}_k(t)) = \frac{\partialC_{\mathbf\phi_k}(t,t') }{\partial t \partial t'} =: C_{\mathbf\phi_k}''(t,t')$
</pre><h2 id="27">Matching Gradients</h2><p>Given the joint distribution over states and their derivatives (3) as well as the ODEs (2), we therefore have two expressions for the state derivatives:</p><p><img src="VGM_for_Lorenz_Attractor_eq09779102347269268548.png" alt="$\dot{\mathbf{X}} = \mathbf{F} + \mathbf\epsilon_1, \mathbf\epsilon_1 \sim\mathcal{N}\left(\mathbf\epsilon_1;\mathbf{0}, \mathbf{I}\gamma \right)$" style="width:139px;height:13px;">,</p><p><img src="VGM_for_Lorenz_Attractor_eq10057857261610929409.png" alt="$\dot{\mathbf{X}} = {'\mathbf{C}_{\mathbf\phi}} \mathbf{C}_{\mathbf\phi}^{-1}~\mathbf{X} + \mathbf\epsilon_2, \mathbf\epsilon_2 \sim\mathcal{N}\left(\mathbf\epsilon_2;\mathbf{0}, \mathbf{A} \right)$" style="width:179px;height:16px;">,</p><p>where <img src="VGM_for_Lorenz_Attractor_eq00445188712889927684.png" alt="$\mathbf{F} := \mathbf{f}(\mathbf{X},\mathbf\theta)$" style="width:58px;height:11px;"> and <img src="VGM_for_Lorenz_Attractor_eq02754859548028378271.png" alt="$\mathbf{A} :=\mathbf{C}_{\mathbf\phi}'' -  {'\mathbf{C}_{\mathbf\phi}} \mathbf{C}_{\mathbf\phi}^{-1}\mathbf{C}_{\mathbf\phi}'$" style="width:105px;height:15px;"> and <img src="VGM_for_Lorenz_Attractor_eq17096441642737911057.png" alt="$\gamma$" style="width:6px;height:8px;"> is the error variance in the ODEs. Note that, in a deterministic system, the output of the ODEs <img src="VGM_for_Lorenz_Attractor_eq10805855639155619100.png" alt="$\mathbf{F}$" style="width:8px;height:8px;"> should equal the state derivatives <img src="VGM_for_Lorenz_Attractor_eq07315144976585928416.png" alt="$\dot{\mathbf{X}}$" style="width:9px;height:11px;">. However, in the first equation above we relax this contraint by adding stochasticity to the state derivatives <img src="VGM_for_Lorenz_Attractor_eq07315144976585928416.png" alt="$\dot{\mathbf{X}}$" style="width:9px;height:11px;"> in order to compensate for a</p><p>potential model mismatch. The second equation above is obtained by deriving the conditional distribution for <img src="VGM_for_Lorenz_Attractor_eq07315144976585928416.png" alt="$\dot{\mathbf{X}}$" style="width:9px;height:11px;"> from the joint distribution in equation (3). Equating the two expressions in the equations above we can eliminate the unknown state derivatives $\dot{\mathbf{X}$:</p><pre class="error">Error updating Text.

 Character vector must have valid interpreter syntax: 
$\dot{\mathbf{X}$
</pre><p><img src="VGM_for_Lorenz_Attractor_eq05797645052466703501.png" alt="$\mathbf{F} = {'\mathbf{C}_{\mathbf\phi}} \mathbf{C}_{\mathbf\phi}^{-1} ~\mathbf{X} +\mathbf\epsilon_0 \qquad (4)$" style="width:130px;height:15px;">,</p><p>with <img src="VGM_for_Lorenz_Attractor_eq15084253510815290366.png" alt="$\mathbf{\epsilon_0} := \mathbf{\epsilon_2} - \mathbf{\epsilon_1}$" style="width:58px;height:7px;">.</p><pre class="codeinput">[dC_times_invC,inv_C,A_plus_gamma_inv] = kernel_function(kernel,state,time.est);
</pre><h2 id="30">Rewrite ODEs as Linear Combination in Parameters</h2><p>Since, according to the mass action dynamics (equation 2), the ODEs are <i>*linear in the parameters *</i>$\mathbf\theta$ we can rewrite the ODEs in equation (2) as a linear combination in the parameters:</p><p><img src="VGM_for_Lorenz_Attractor_eq14681271264105480906.png" alt="$\mathbf{B}_{\mathbf{\theta} k} \mathbf{\theta} + \mathbf{b}_{\mathbf{\theta} k} \stackrel{!}{=}\mathbf{f}_k(\mathbf{X},\mathbf{\theta}) \qquad (5)$" style="width:139px;height:15px;">,</p><p>where matrices <img src="VGM_for_Lorenz_Attractor_eq14537279257693498981.png" alt="$\mathbf{B}_{\mathbf{\theta} k}$" style="width:17px;height:10px;"> and <img src="VGM_for_Lorenz_Attractor_eq04855563019719143218.png" alt="$\mathbf{b}_{\mathbf{\theta} k}$" style="width:15px;height:10px;"> and <img src="VGM_for_Lorenz_Attractor_eq06556310868311218058.png" alt="$\mathbf{b}_{\mathbf\theta k}$" style="width:15px;height:10px;"> are defined such that the ODEs <img src="VGM_for_Lorenz_Attractor_eq14967653859658475783.png" alt="$\mathbf{f}_k(\mathbf{X},\mathbf{\theta})$" style="width:36px;height:11px;"> are expressed as a linear combination in <img src="VGM_for_Lorenz_Attractor_eq00000731545789844591.png" alt="$\mathbf{\theta}$" style="width:5px;height:8px;">.</p><pre class="codeinput">[ode_param.lin_comb.B,ode_param.lin_comb.b] = rewrite_odes_as_linear_combination_in_parameters(ode,symbols);
</pre><h2 id="33">Posterior over ODE Parameters</h2><p>Inserting (5) into (4) and solving for <img src="VGM_for_Lorenz_Attractor_eq06840717269413420654.png" alt="$$\mathbf{\theta}$" style="width:5px;height:8px;">$ yields:</p><p><img src="VGM_for_Lorenz_Attractor_eq14707740594757803178.png" alt="$\mathbf{\theta} = \mathbf{B}_{\mathbf{\theta}}^+ \left( {'\mathbf{C}_{\mathbf{\phi}}}\mathbf{C}_{\mathbf{\phi}}^{-1} \mathbf{X} - \mathbf{b}_{\mathbf{\theta}} + \mathbf{\epsilon_0}\right)$" style="width:143px;height:20px;">,</p><p>where <img src="VGM_for_Lorenz_Attractor_eq05803544219715308982.png" alt="$\mathbf{B}_{\mathbf{\theta}}^+$" style="width:15px;height:13px;"> denotes the pseudo-inverse of <img src="VGM_for_Lorenz_Attractor_eq17686033574112270987.png" alt="$\mathbf{B}_{\mathbf{\theta}}$" style="width:13px;height:10px;">. Since <img src="VGM_for_Lorenz_Attractor_eq12476492725851578267.png" alt="$$\mathbf{C}_{\mathbf{\phi}}$" style="width:14px;height:11px;">$ is block diagonal we can rewrite the expression above as:</p><p><img src="VGM_for_Lorenz_Attractor_eq07476366998092137637.png" alt="$\mathbf{\theta} = \left( \mathbf{B}_{\mathbf{\theta}}^T \mathbf{B}_{\mathbf{\theta}} \right)^{-1} ~\mathbf{B}_{\mathbf{\theta}}^T  \left( \sum_k {'\mathbf{C}_{\mathbf{\phi}_k}}\mathbf{C}_{\mathbf{\phi}_k}^{-1} \mathbf{X}_k - \mathbf{b}_{\mathbf{\theta} k} + \mathbf{\epsilon_0}^{(k)} \right)\\ ~= \left( \mathbf{B}_{\mathbf{\theta}}^T \mathbf{B}_{\mathbf{\theta}} \right)^{-1} \left(\sum_k \mathbf{B}_{\mathbf{\theta} k}^T \left( {'\mathbf{C}_{\mathbf{\phi}_k}}\mathbf{C}_{\mathbf{\phi}_k}^{-1} \mathbf{X}_k - \mathbf{b}_{\mathbf{\theta} k} +\mathbf{\epsilon_0}^{(k)} \right) \right)$" style="width:491px;height:20px;">,</p><p>where we subsitute the Moore-Penrose inverse for the pseudo-inverse (i.e. <img src="VGM_for_Lorenz_Attractor_eq09091945040404351953.png" alt="$\mathbf{B}_{\mathbf{\theta}}^+ := \left( \mathbf{B}_{\mathbf{\theta}}^T \mathbf{B}_{\mathbf{\theta}}\right)^{-1} \mathbf{B}_{\mathbf{\theta}}^T$" style="width:99px;height:16px;">). We can therefore derive the posterior distribution over ODE parameters:</p><p><img src="VGM_for_Lorenz_Attractor_eq00860790021181510419.png" alt="$p(\mathbf{\theta} \mid \mathbf{X}, \mathbf{\phi}, \gamma) = \mathcal{N}\left(\mathbf{\theta} ; \left( \mathbf{B}_{\mathbf{\theta}}^T\mathbf{B}_{\mathbf{\theta}} \right)^{-1} \left( \sum_k \mathbf{B}_{\mathbf{\theta} k}^T ~\left( {'\mathbf{C}_{\mathbf{\phi} k}} \mathbf{C}_{\mathbf{\phi} k}^{-1} \mathbf{X}_k -\mathbf{b}_{\mathbf{\theta} k} \right) \right), ~ \mathbf{B}_{\mathbf{\theta}}^+ ~(\mathbf{A} + \mathbf{I}\gamma) ~ \mathbf{B}_{\mathbf{\theta}}^{+T}\right) \qquad (6)$" style="width:449px;height:20px;">.</p><h2 id="35">Rewrite ODEs as Linear Combination in Individual States</h2><p>Since, according to the mass action dynamics (equation 2), the ODEs are <i>*linear in the individual state</i>* <img src="VGM_for_Lorenz_Attractor_eq00398522576481313565.png" alt="$\mathbf{x}_u$" style="width:11px;height:7px;"> we can rewrite the ODE <img src="VGM_for_Lorenz_Attractor_eq11465620228287660888.png" alt="$\mathbf{f}_k(\mathbf{X},\mathbf\theta)$" style="width:36px;height:11px;"> as a linear combination in the individual state <img src="VGM_for_Lorenz_Attractor_eq00398522576481313565.png" alt="$\mathbf{x}_u$" style="width:11px;height:7px;">:</p><p><img src="VGM_for_Lorenz_Attractor_eq00583256486819676345.png" alt="$\mathbf{R}_{uk} \mathbf{x}_u + \mathbf{r}_{uk} \stackrel{!}{=}\mathbf{f}_k(\mathbf{X},\mathbf{\theta})$" style="width:109px;height:15px;">,</p><p>where matrices $ \mathbf{R}_{uk}$ and  <img src="VGM_for_Lorenz_Attractor_eq02299362643576640155.png" alt="$\mathbf{r}_{uk}$" style="width:14px;height:7px;"> are defined such that the ODE <img src="VGM_for_Lorenz_Attractor_eq14967653859658475783.png" alt="$\mathbf{f}_k(\mathbf{X},\mathbf{\theta})$" style="width:36px;height:11px;"> is expressed as a linear combination in the individual state <img src="VGM_for_Lorenz_Attractor_eq00398522576481313565.png" alt="$\mathbf{x}_u$" style="width:11px;height:7px;">.</p><pre class="codeinput">[state.lin_comb.R,state.lin_comb.r] = rewrite_odes_as_linear_combination_in_ind_states(ode,symbols,coupling_idx.states);
</pre><h2 id="38">Posterior over Individual States</h2><p>Given the linear combination of the ODEs w.r.t. an individual state, we define the matrices <img src="VGM_for_Lorenz_Attractor_eq11780124863314593273.png" alt="$\mathbf{B}_u$" style="width:13px;height:10px;"> and <img src="VGM_for_Lorenz_Attractor_eq15590211095018081680.png" alt="$\mathbf{b}_u$" style="width:11px;height:10px;"> such that the expression <img src="VGM_for_Lorenz_Attractor_eq15941870326266439714.png" alt="$$\mathbf{f}(\mathbf{X},\mathbf{\theta}) - {'\mathbf{C}}_{\mathbf{\phi}}\mathbf{C}_{\mathbf{\phi}}^{-1} \mathbf{X}$" style="width:93px;height:15px;">$ is rewritten as a linear combination in an individual state <img src="VGM_for_Lorenz_Attractor_eq00398522576481313565.png" alt="$\mathbf{x}_u$" style="width:11px;height:7px;">:</p><p><img src="VGM_for_Lorenz_Attractor_eq08018014595476126940.png" alt="$\mathbf{B}_{u} \mathbf{x}_u + \mathbf{b}_{u} \stackrel{!}{=}\mathbf{f}(\mathbf{X},\mathbf{\theta}) - {'\mathbf{C}}_{\mathbf{\phi}}\mathbf{C}_{\mathbf{\phi}}^{-1} \mathbf{X} \qquad (7)$" style="width:194px;height:17px;">.</p><p>Inserting (7) into (4) and solving for <img src="VGM_for_Lorenz_Attractor_eq06172288676100169704.png" alt="$$\mathbf{x}_u$" style="width:11px;height:7px;">$ yields:</p><p><img src="VGM_for_Lorenz_Attractor_eq04934047781206375234.png" alt="$$\mathbf{x}_u = \mathbf{B}_{u}^+ \left( \mathbf{\epsilon_0} -\mathbf{b}_{u}\right)$" style="width:86px;height:13px;">$,</p><p>where <img src="VGM_for_Lorenz_Attractor_eq00687478806446939535.png" alt="$$\mathbf{B}_{u}^+$" style="width:15px;height:13px;">$ denotes the pseudo-inverse of <img src="VGM_for_Lorenz_Attractor_eq05698187735710752536.png" alt="$$\mathbf{B}_{u}$" style="width:13px;height:10px;">$. Since <img src="VGM_for_Lorenz_Attractor_eq12476492725851578267.png" alt="$$\mathbf{C}_{\mathbf{\phi}}$" style="width:14px;height:11px;">$ is block diagonal we can rewrite the expression above as:</p><p><img src="VGM_for_Lorenz_Attractor_eq05588656242432596112.png" alt="$\mathbf{x}_u = \left( \mathbf{B}_{u} \mathbf{B}_{u}^T \right)^{-1}\mathbf{B}_{u}^T \sum_k \left(\mathbf{\epsilon_0}^{(k)} -\mathbf{b}_{uk} \right)\\ \quad= \left( \mathbf{B}_{u} \mathbf{B}_{u}^T \right)^{-1} \sum_k\mathbf{B}_{uk}^T \left(\mathbf{\epsilon_0}^{(k)} -\mathbf{b}_{uk} \right)$" style="width:349px;height:16px;">,</p><p>where we subsitute the Moore-Penrose inverse for the pseudo-inverse (i.e. <img src="VGM_for_Lorenz_Attractor_eq09091945040404351953.png" alt="$\mathbf{B}_{\mathbf{\theta}}^+ := \left( \mathbf{B}_{\mathbf{\theta}}^T \mathbf{B}_{\mathbf{\theta}}\right)^{-1} \mathbf{B}_{\mathbf{\theta}}^T$" style="width:99px;height:16px;">).  We can therefore derive the posterior distribution over an individual state <img src="VGM_for_Lorenz_Attractor_eq01182610398367661295.png" alt="$$\mathbf{x}_{u}$" style="width:11px;height:7px;">$:</p><p><img src="VGM_for_Lorenz_Attractor_eq14053813124642092754.png" alt="$p(\mathbf{x}_u \mid \mathbf{X}_{-u}, \mathbf{\phi}, \gamma)= \mathcal{N}\left(\mathbf{x}_u ; \left( \mathbf{B}_{u} \mathbf{B}_{u}^T\right)^{-1} \left( - \sum_k \mathbf{B}_{uk}^T \mathbf{b}_{uk} \right),~\mathbf{B}_{u}^{+} ~ (\mathbf{A} + \mathbf{I}\gamma) ~ \mathbf{B}_u^{+T}\right) \qquad (8)$" style="width:394px;height:20px;">,</p><p>with <img src="VGM_for_Lorenz_Attractor_eq09393849726171219045.png" alt="$$\mathbf{X}_{-u}$" style="width:20px;height:10px;">$ denoting the set of all states except state <img src="VGM_for_Lorenz_Attractor_eq06172288676100169704.png" alt="$$\mathbf{x}_u$" style="width:11px;height:7px;">$.</p><h2 id="40">Mean-field Variational Inference</h2><p>To infer the parameters <img src="VGM_for_Lorenz_Attractor_eq06840717269413420654.png" alt="$$\mathbf{\theta}$" style="width:5px;height:8px;">$, we want to find the maximum a posteriori estimate (MAP):</p><p><img src="VGM_for_Lorenz_Attractor_eq13511049836739271940.png" alt="$\mathbf{\theta}^* := \mathrm{arg} \max_{\mathbf{\theta}} ~ \ln p(\mathbf{\theta} \mid\mathbf{Y},\mathbf{\phi},\gamma,\mathbf \sigma)\\ \quad= \mathrm{arg}\max_{\mathbf{\theta}} ~ \ln \int  p(\mathbf{\theta},\mathbf{X} \mid\mathbf{Y},\mathbf{\phi},\gamma,\mathbf\sigma) d\mathbf{X}\\ \quad= \mathrm{arg}\max_{\mathbf{\theta}} ~ \ln \int p(\mathbf{\theta} \mid \mathbf{X},\mathbf{\phi},\gamma)p(\mathbf{X} \mid \mathbf{Y}, \mathbf{\phi},\mathbf\sigma) d\mathbf{X} \qquad(9)$" style="width:631px;height:13px;">.</p><p>However, the integral above is intractable due to the strong couplings induced by the nonlinear ODEs <img src="VGM_for_Lorenz_Attractor_eq07966868336018839913.png" alt="$$\mathbf{f}$" style="width:5px;height:8px;">$ which appear in the term <img src="VGM_for_Lorenz_Attractor_eq15455916038640034804.png" alt="$$p(\mathbf{\theta} \mid \mathbf{X},\mathbf{\phi},\gamma)$" style="width:60px;height:11px;">$.</p><p>We use mean-field variational inference to establish variational lower bounds that are analytically tractable by decoupling state variables from the ODE parameters as well as decoupling the state variables from each other. Note that, since the ODEs described by equation (2) are <i>*locally linear</i>*, both conditional distributions <img src="VGM_for_Lorenz_Attractor_eq04257617213050957075.png" alt="$$p(\mathbf{\theta} \mid\mathbf{X},\mathbf{Y},\mathbf{\phi},\gamma,\mathbf\sigma)$" style="width:87px;height:11px;">$ (equation (6)) and <img src="VGM_for_Lorenz_Attractor_eq02707094767016802694.png" alt="$$p(\mathbf{x}_u \mid \mathbf{\theta},\mathbf{X}_{-u},\mathbf{Y},\mathbf{\phi},\gamma,\mathbf\sigma)$" style="width:114px;height:11px;">$ (equation (8)) are analytically tractable and Gaussian distributed as mentioned previously. The decoupling is induced by designing a variational distribution <img src="VGM_for_Lorenz_Attractor_eq04524928483400281720.png" alt="$$Q(\mathbf{\theta},\mathbf{X})$" style="width:36px;height:11px;">$ which is restricted to the family of factorial distributions:</p><p><img src="VGM_for_Lorenz_Attractor_eq07116708179068120352.png" alt="$$\mathcal{Q} := \bigg{\{} Q : Q(\mathbf{\theta},\mathbf{X}) = q(\mathbf{\theta}) \prod_uq(\mathbf{x}_u) \bigg{\}}$" style="width:173px;height:29px;">$.</p><p>The particular form of <img src="VGM_for_Lorenz_Attractor_eq08886706959074116312.png" alt="$$q(\mathbf{\theta})$" style="width:19px;height:11px;">$ and <img src="VGM_for_Lorenz_Attractor_eq17420406007066013322.png" alt="$$q(\mathbf{x}_u)$" style="width:25px;height:11px;">$ are designed to be Gaussian distributed which places them in the same family as the true full conditional distributions. To find the optimal factorial distribution we minimize the Kullback-Leibler divergence between the variational and the true posterior distribution:</p><p><img src="VGM_for_Lorenz_Attractor_eq00221060080535114208.png" alt="$$\hat{Q} := \mathrm{arg} \min_{Q(\mathbf{\theta},\mathbf{X}) \in \mathcal{Q}} \mathrm{KL}\left[ Q(\mathbf{\theta},\mathbf{X}) \mid \mid p(\mathbf{\theta},\mathbf{X} \mid\mathbf{Y},\mathbf{\phi}, \gamma,\mathbf\mathbf{\sigma}) \right] \qquad (10)$" style="width:284px;height:21px;">$,</p><p>where <img src="VGM_for_Lorenz_Attractor_eq15743246986648076931.png" alt="$$\hat{Q}$" style="width:8px;height:13px;">$ is the proxy distribution. The proxy distribution that minimizes the KL-divergence (10) depends on the true full conditionals and is given by:</p><p><img src="VGM_for_Lorenz_Attractor_eq00524040698376027693.png" alt="$\hat{q}({\mathbf{\theta}}) \propto \exp \left(~ E_{Q_{-\mathbf{\theta}}} \ln p(\mathbf{\theta} \mid\mathbf{X},\mathbf{Y},\mathbf{\phi},\gamma,\mathbf\mathbf{\sigma}) ~\right) \qquad (11)\\\hat{q}(\mathbf{x}_u) \propto \exp\left( ~ E_{Q_{-u}} \ln p(\mathbf{x}_u\mid \mathbf{\theta}, \mathbf{X}_{-u},\mathbf{Y},\mathbf{\phi},\gamma,\mathbf{\sigma}) ~ \right)\qquad (12)$" style="width:502px;height:12px;">.</p><h2 id="42">Fitting observations of state trajectories</h2><p>We fit the observations of state trajectories by standard GP regression. The data-informed distribution$ <img src="VGM_for_Lorenz_Attractor_eq00607084938701956025.png" alt="$p(\mathbf{X} \mid \mathbf{Y}, \mathbf{\phi},\mathbf\mathbf{\sigma})$" style="width:65px;height:11px;">$ in euqation (9) can be determined analytically using Gaussian process regression with the GP prior <img src="VGM_for_Lorenz_Attractor_eq11165883262288439683.png" alt="$$p(\mathbf{X} \mid\mathbf{\phi}) = \prod_k \mathcal{N}(\mathbf{x}_k ;\mathbf{0},\mathbf{C}_{\mathbf{\phi}_k})$" style="width:134px;height:24px;">$:</p><p><img src="VGM_for_Lorenz_Attractor_eq07752023364341374350.png" alt="$$p(\mathbf{X} \mid \mathbf{Y}, \mathbf{\phi},\gamma) = \prod_k\mathcal{N}(\mathbf{x}_k ;\mathbf\mu_k(\mathbf{y}_k),\mathbf\mathbf{\sigma}_k)$" style="width:178px;height:24px;">$,</p><p>where <img src="VGM_for_Lorenz_Attractor_eq05139699893116141298.png" alt="$$\mathbf\mu_k(\mathbf{y}_k) := \mathbf{\sigma}_k^{-2} \left(\mathbf{\sigma}_k^{-2}\mathbf{I} + \mathbf{C}_{\mathbf{\phi}_k}^{-1} \right)^{-1} \mathbf{y}_k$" style="width:159px;height:23px;">$ and <img src="VGM_for_Lorenz_Attractor_eq02647485491785378212.png" alt="$$\mathbf{\sigma}_k ^{-1}:=\mathbf{\sigma}_k^{-2} \mathbf{I} +\mathbf{C}_{\mathbf\mathbf{\phi}_k}^{-1}$" style="width:89px;height:15px;">$.</p><pre class="codeinput">[mu,inv_sigma] = fitting_state_observations(inv_C,obs_to_state_relation,simulation,symbols);
</pre><h2 id="45">Coordinate Ascent Variational Gradient Matching</h2><p>We minimize the KL-divergence in equation (10) by coordinate descent (where each step is analytically tractable) by iterating between determining the proxy for the distribution over ODE parameters <img src="VGM_for_Lorenz_Attractor_eq13317601314667608785.png" alt="$$\hat{q}(\mathbf{\theta})$" style="width:19px;height:11px;">$ and the proxies for the distribution over individual states <img src="VGM_for_Lorenz_Attractor_eq11393795473504476532.png" alt="$$\hat{q}(\mathbf{x}_u)$" style="width:25px;height:11px;">$.</p><div><ul><li><b>Initialize the state estimation by the GP regression posterior</b></li></ul></div><p></p><pre class="codeinput">            state.proxy.mean = array2table([time.est',mu],<span class="string">'VariableNames'</span>,[<span class="string">'time'</span>,symbols.state_string]);
</pre><p></p><div><ul><li><b>Coordinate ascent</b></li></ul></div><p></p><pre class="codeinput">z=1;
            <span class="keyword">for</span> i = 1:opt_settings.coord_ascent_numb_iter
</pre><div><ul><li><b>Proxy for ODE parameters</b></li></ul></div><pre>                   Expanding the proxy distribution in equation (11) for
$$\mathbf{\theta}$$ yields:</pre><pre>                          $\hat{q}(\mathbf{\mathbf\theta}) \propto \exp
\left( ~E_{Q_{-\mathbf\theta}}     \ln p(\mathbf\theta \mid \mathbf{X},\mathbf{Y},\mathbf\phi,\gamma,\mathbf\sigma)
~     \right) \\ \qquad= \exp \left( ~E_{Q_{-\mathbf{\theta}}} \ln \mathcal{N}\left(\mathbf{\theta}
; \left(    \mathbf{B}_{\mathbf{\theta}}^T \mathbf{B}_{\mathbf{\theta}} \right)^{-1}
\left( \sum_k    \mathbf{B}_{\mathbf{\theta} k}^T ~ \left( {'\mathbf{C}_{\mathbf{\phi}
k}}    \mathbf{C}_{\mathbf{\phi} k}^{-1} \mathbf{X}_k - \mathbf{b}_{\mathbf{\theta}
k} \right)    \right), ~ \mathbf{B}_{\mathbf{\theta}}^+ ~ (\mathbf{A} + \mathbf{I}\gamma)
~    \mathbf{B}_{\mathbf{\theta}}^{+T} \right) ~\right)$,</pre><pre>                   where we substitute $$p(\mathbf{\theta} \mid \mathbf{X},\mathbf{\phi},\gamma)$$
with its density given in equation (6).</pre><pre class="codeinput">             [param_proxy_mean,param_proxy_inv_cov] = proxy_for_ode_parameters(state.proxy.mean{:,symbols.state_string},<span class="keyword">...</span>
                    dC_times_invC,ode_param.lin_comb,symbols,A_plus_gamma_inv,opt_settings);
</pre><p>Intermediate results</p><pre class="codeinput">            <span class="keyword">if</span> i==1 || ~mod(i,1)
                plot_results(fig_handle,state.proxy,simulation,param_proxy_mean,plot_handle,symbols,plot_settings,<span class="string">'not_final'</span>);
            <span class="keyword">end</span>

            z = z+1; print(1,[<span class="string">'video/demo_'</span> num2str(z) <span class="string">'.png'</span>],<span class="string">'-dpng'</span>,<span class="string">'-r0'</span>);
</pre><img vspace="5" hspace="5" src="VGM_for_Lorenz_Attractor_02.png" style="width:1600px;height:800px;" alt=""> <img vspace="5" hspace="5" src="VGM_for_Lorenz_Attractor_03.png" style="width:1600px;height:800px;" alt=""> <img vspace="5" hspace="5" src="VGM_for_Lorenz_Attractor_04.png" style="width:1600px;height:800px;" alt=""> <img vspace="5" hspace="5" src="VGM_for_Lorenz_Attractor_05.png" style="width:1600px;height:800px;" alt=""> <img vspace="5" hspace="5" src="VGM_for_Lorenz_Attractor_06.png" style="width:1600px;height:800px;" alt=""> <img vspace="5" hspace="5" src="VGM_for_Lorenz_Attractor_07.png" style="width:1600px;height:800px;" alt=""> <img vspace="5" hspace="5" src="VGM_for_Lorenz_Attractor_08.png" style="width:1600px;height:800px;" alt=""> <img vspace="5" hspace="5" src="VGM_for_Lorenz_Attractor_09.png" style="width:1600px;height:800px;" alt=""> <img vspace="5" hspace="5" src="VGM_for_Lorenz_Attractor_10.png" style="width:1600px;height:800px;" alt=""> <img vspace="5" hspace="5" src="VGM_for_Lorenz_Attractor_11.png" style="width:1600px;height:800px;" alt=""> <img vspace="5" hspace="5" src="VGM_for_Lorenz_Attractor_12.png" style="width:1600px;height:800px;" alt=""> <img vspace="5" hspace="5" src="VGM_for_Lorenz_Attractor_13.png" style="width:1600px;height:800px;" alt=""> <img vspace="5" hspace="5" src="VGM_for_Lorenz_Attractor_14.png" style="width:1600px;height:800px;" alt=""> <img vspace="5" hspace="5" src="VGM_for_Lorenz_Attractor_15.png" style="width:1600px;height:800px;" alt=""> <img vspace="5" hspace="5" src="VGM_for_Lorenz_Attractor_16.png" style="width:1600px;height:800px;" alt=""> <img vspace="5" hspace="5" src="VGM_for_Lorenz_Attractor_17.png" style="width:1600px;height:800px;" alt=""> <img vspace="5" hspace="5" src="VGM_for_Lorenz_Attractor_18.png" style="width:1600px;height:800px;" alt=""> <img vspace="5" hspace="5" src="VGM_for_Lorenz_Attractor_19.png" style="width:1600px;height:800px;" alt=""> <img vspace="5" hspace="5" src="VGM_for_Lorenz_Attractor_20.png" style="width:1600px;height:800px;" alt=""> <img vspace="5" hspace="5" src="VGM_for_Lorenz_Attractor_21.png" style="width:1600px;height:800px;" alt=""> <img vspace="5" hspace="5" src="VGM_for_Lorenz_Attractor_22.png" style="width:1600px;height:800px;" alt=""> <img vspace="5" hspace="5" src="VGM_for_Lorenz_Attractor_23.png" style="width:1600px;height:800px;" alt=""> <img vspace="5" hspace="5" src="VGM_for_Lorenz_Attractor_24.png" style="width:1600px;height:800px;" alt=""> <img vspace="5" hspace="5" src="VGM_for_Lorenz_Attractor_25.png" style="width:1600px;height:800px;" alt=""> <img vspace="5" hspace="5" src="VGM_for_Lorenz_Attractor_26.png" style="width:1600px;height:800px;" alt=""> <p></p><div><ul><li><b>Proxy for individual states</b></li></ul></div><pre>               Expanding the proxy distribution in equation (12) over
the individual state $$\mathbf{x}_u$$:</pre><pre>               $\hat{q}(\mathbf{x}_u) \stackrel{(a)}{\propto} \exp \left(
~ E_{Q_{-u}}  \ln ( p(\mathbf{x}_u \mid \mathbf\theta, \mathbf{X}_{-u},\mathbf\phi,\gamma)
p(\mathbf{x}_u  \mid\mathbf{Y},\mathbf\phi,\mathbf\sigma) ) ~ \right)\\ \qquad
~ \stackrel{(b)}{=} \exp\big( ~ E_{Q_{-u}} \ln     \mathcal{N}\left(\mathbf{x}_u
; -\mathbf{B}_{u}^+ \mathbf{b}_u,     ~\mathbf{B}_u^{+} ~ (\mathbf{A} + \mathbf{I}\gamma)
~     \mathbf{B}_u^{+T} \right) + E_{Q_{-u}} \ln    \mathcal{N}\left(\mathbf{x}_u
; \mathbf\mu_u(\mathbf{Y}), \mathbf\Sigma_u    \right) \big)\\ \qquad ~= \exp\big(
~ E_{Q_{-u}} \ln                \mathcal{N}\left(\mathbf{x}_u ; -\mathbf{B}_{u}^+
\mathbf{b}_u,                ~\mathbf{B}_u^{+} ~ (\mathbf{A} + \mathbf{I}\gamma)
~                \mathbf{B}_u^{+T} \right) + E_{Q_{-u}} \ln                \mathcal{N}\left(\mathbf{x}_u
; \mathbf\mu_u(\mathbf{Y}), \mathbf{\sigma}_u                \right) \big)$.</pre><pre>               In (a) we decompose the full conditional nto an ODE-informed
distribution and a data-informed distribution and in (b) we substitute the ODE-informed
distribution $p(\mathbf{x}_u \mid \mathbf\theta, \mathbf{X}_{-u},\mathbf\phi,\gamma)$
with its density given by equation (8).</pre><pre class="codeinput">            [state.proxy.mean{:,symbols.state_string},state.proxy.inv_cov] = proxy_for_ind_states(state.lin_comb,<span class="keyword">...</span>
                state.proxy.mean{:,symbols.state_string},param_proxy_mean',dC_times_invC,coupling_idx.states,symbols,mu,<span class="keyword">...</span>
                inv_sigma,simulation.observed_states,A_plus_gamma_inv,opt_settings);
</pre><pre class="codeinput"><span class="keyword">end</span>
</pre><p></p><p>Final results</p><p></p><pre class="codeinput">            plot_results(fig_handle,state.proxy,simulation,param_proxy_mean,plot_handle,symbols,plot_settings,<span class="string">'final'</span>);
</pre><img vspace="5" hspace="5" src="VGM_for_Lorenz_Attractor_27.png" style="width:1600px;height:800px;" alt=""> <h2 id="56">Time Taken</h2><pre class="codeinput">disp([<span class="string">'time taken: '</span> num2str(toc) <span class="string">' seconds'</span>])
</pre><pre class="codeoutput">time taken: 60.6568 seconds
</pre><h2 id="59">References</h2><p>Gorbach, N.S.  Validation and Inference of Structural Connectivity and Neural Dynamics with MRI data. 2018. ETH Z&uuml;rich Doctoral Thesis. <a href="https://www.research-collection.ethz.ch/handle/20.500.11850/261734">https://www.research-collection.ethz.ch/handle/20.500.11850/261734</a>.</p><p>*Gorbach, N.S. , Bauer, S. *and Buhmann, J.M., Scalable Variational Inference for Dynamical Systems. 2017a. Neural Information Processing Systems (NIPS). Link to NIPS paper <a href="https://papers.nips.cc/paper/7066-scalable-variational-inference-for-dynamical-systems.pdf">here</a> and arxiv paper <a href="https://arxiv.org/abs/1705.07079">here</a>.</p><p><b>Bauer, S. , Gorbach, N.S.</b> and Buhmann, J.M., Efficient and Flexible Inference for Stochastic Differential Equations. 2017b. Neural Information Processing Systems (NIPS). Link to NIPS paper <a href="https://papers.nips.cc/paper/7274-efficient-and-flexible-inference-for-stochastic-systems.pdf">here</a>.</p><p>Wenk, P., Gotovos, A., Bauer, S., Gorbach, N.S., Krause, A. and Buhmann, J.M., Fast Gaussian Process Based Gradient Matching for Parameters Identification in Systems of Nonlinear ODEs. 2018. In submission to Conference on Uncertainty in Artificial Intelligence (UAI). Link to arxiv paper <a href="https://arxiv.org/pdf/1804.04378.pdf">here</a>.</p><p>Calderhead, B., Girolami, M. and Lawrence. N.D., 2002. Accelerating Bayesian inference over nonlinear differential equation models. In Advances in Neural Information Processing Systems (NIPS) . 22.</p><p>The authors in bold font have contributed equally to their respective papers.</p><p class="footer"><br><a href="http://www.mathworks.com/products/matlab/">Published with MATLAB&reg; R2017a</a><br></p></div><!--
##### SOURCE BEGIN #####
%% Variational Gradient Matching for Dynamical Systems: Lorenz Attractor
% 
%% .
%% *Authors*: 
% *Nico Stephan Gorbach* and *Stefan Bauer*, email: nico.gorbach@gmail.com
% 
% 
%% Contents:
% Instructional code for the NIPS (2018) paper <https://papers.nips.cc/paper/7066-scalable-variational-inference-for-dynamical-systems.pdf 
% Scalable Variational Inference for Dynamical Systems> by Nico S. Gorbach, Stefan 
% Bauer and Joachim M. Buhmann. Please cite our paper if you use our program for 
% a further publication. The derivations in this document are also given in the 
% doctoral thesis <https://www.research-collection.ethz.ch/handle/20.500.11850/261734 
% https://www.research-collection.ethz.ch/handle/20.500.11850/261734> as well 
% as in parts of Wenk et al. (2018).
% 
% Example dynamical system used in this code:* Lorenz attractor* system with 
% the* y-dimension unobserved*. The ODE parameters are also unobserved.
% 
% 
% 
% 
% 
% 
% 
% 
% 
% 
%% User Input: Simulation Settings
% 
% 
% * *true ODE parameter*
% 
%                 Input a row vector of real numbers of size 1 x 3:
% 
% 
%%
        simulation.ode_param = [10,28,8/3];  
%% 
% **
% 
% * *final time for simulation*
% 
%                 Input a positive real number:
% 
% 

        simulation.final_time = 20;
%% 
% **
% 
% * *observation noise*
% 
%                 Input a function handle:
% 
% 

        simulation.state_obs_variance = @(mean)(bsxfun(@times,[2,2],ones(size(mean))));
%% 
% **
% 
% * *time interval between observations*
% 
%                 Input a positive real number:
% 
% 

        simulation.interval_between_observations = 0.1;
%% 
% **
%% User Input: Estimation Settings
% 
% 
% * *Kernel parameters *$\mathbf\phi$
% 
%                 Input a row vector of positive real numbers of size 1 x 
% 2:
% 
% 

        kernel.param = [10,0.2];
%% 
% **
% 
% * *Error variance on state derivatives (i.e.* $\gamma$*)*
% 
%                 Input a row vector of positive real numbers of size 1 x 
% 3:
% 
% 

        state.derivative_variance = [6,6,6];
%% 
% **
% 
% * *Estimation times*
% 
%                 Input a row vector of positive real numbers in ascending 
% order:
% 
% 

        time.est = 0:0.1:20;        
%% 
% 
% 
% 
% 
% Preliminary operations
%%
close all; clc; addpath('VGM_functions')
%% 
% 
% 
% 
%% Preprocessing
% 
%%
[symbols,simulation,ode,odes_path,coupling_idx,opt_settings,plot_settings] = preprocessing_Lorenz_Attractor (simulation);
%% 
% 
%% Mass Action Dynamical Systems
% 
% 
% A deterministic dynamical system is represented by a set of $K$ ordinary 
% differential equations (ODEs) with model parameters $\mathbf\theta \in \mathcal{R}^d$ 
% that describe the evolution of $K$ states $\mathbf{x}(t) = [x_1(t),\ldots, x_K(t)]^T$ 
% such that:  
% 
% $\dot{\mathbf{x}}(t) = \frac{d \mathbf{x}(t)}{d t} = \mathbf{f}(\mathbf{x}(t),\mathbf\theta) 
% \qquad (1)$,
% 
% A sequence of observations, $\mathbf{y}(t)$, is usually contaminated by 
% measurement error which we assume to be normally distributed with zero mean 
% and variance for each of the $K$ states, i.e. $\mathbf{E}\sim\mathcal{N}(\mathbf{E};\mathbf{0},\mathbf{D})$, 
% with $\mathbf{D}_{ik}=\sigma_k ^2 \delta_{ik}$. For $N$ distinct time points 
% the overall system may therefore be summarized as  
% 
% $\mathbf{Y} = \mathbf{X} + \mathbf{E}$,
% 
% where
% 
% $\mathbf{X} = [\mathbf{x}(t_1),\ldots,\mathbf{x}(t_N)] =  [\mathbf{x}_1,\ldots,\mathbf{x}_K]^T$,
% 
% $\mathbf{Y} = [\mathbf{y}(t_1),\ldots,\mathbf{y}(t_N)] =  [\mathbf{y}_1,\ldots,\mathbf{y}_K]^T$,
% 
% and $\mathbf{x}_k = [x_k(t_1),\ldots,x_k(t_N)]^T$ is the $k$'th state sequence 
% and $\mathbf{y}_k = [y_k(t_1),\ldots,y_k(t_N)]^T$ are the observations. Given 
% the observations $\mathbf{Y}$ and the description of the dynamical system (1), 
% the aim is to estimate both state variables $\mathbf{X}$ and parameters $\mathbf\theta$.
% 
% We consider only dynamical systems that are _*locally linear _*with respect 
% to ODE parameters $\mathbf\theta$ and individual states $\mathbf{x}$. Such ODEs 
% include mass-action kinetics and are given by:
% 
% $f_{k}(\mathbf{x}(t),\theta) = \sum_{i=1} \theta_{ki} \prod_{j \in\mathcal{M}_{ki}} 
% x_j \qquad (2)$,
% 
% with $\mathcal{M}_{ki} \subseteq \{ 1, \dots, K\}$describing the state 
% variables in each factor of the equation (i.e. the functions are linear in parameters 
% and contain arbitrary large products of monomials of the states).
%% 
% 
%% Simulate Trajectories
% 
%%
[simulation,obs_to_state_relation,fig_handle,plot_handle] = simulate_state_dynamics(simulation,state,symbols,ode,odes_path,time,plot_settings);
%% 
% 
% 
% start timer
%%
tic;
%% 
% 
%% Prior on States and State Derivatives
% 
% 
% Gradient matching with Gaussian processes assumes a joint Gaussian process 
% prior on states and their derivatives:
% 
% 
% 
% $\left(\begin{array}{c} \mathbf{X} \\ \dot{\mathbf{X}} \end{array}\right) 
% \sim \mathcal{N} \left(\begin{array}{c} \mathbf{X} \\ \dot{\mathbf{X}} \end{array}~;~\begin{array}{c} 
% \mathbf{0} \\\mathbf{0} \end{array}~,~\begin{array}{cc} \mathbf{C}_{\mathbf\phi} 
% & \mathbf{C}_{\mathbf\phi}' \\ '\mathbf{C}_{\mathbf\phi} & \mathbf{C}_{\mathbf\phi}'' 
% \end{array} \right) \qquad (3)$,
% 
% with         
% 
% $\mathrm{cov}(x_k(t), x_k(t)) = C_{\mathbf\phi_k}(t,t')$, 
% 
% $\mathrm{cov}(\dot{x}_k(t), x_k(t)) = \frac{\partial C_{\mathbf{\phi}_k}(t,t')}{\partial 
% t} =: C_{{\mathbf\phi}_k}'(t,t')$, 
% 
% $\mathrm{cov}(x_k(t), \dot{x}_k(t)) = \frac{\partial C_{\mathbf\phi_k}(t,t')}{\partial 
% t'} =: {'C_{\mathbf\phi_k}(t,t')}$, 
% 
% $\mathrm{cov}(\dot{x}_k(t), \dot{x}_k(t)) = \frac{\partialC_{\mathbf\phi_k}(t,t') 
% }{\partial t \partial t'} =: C_{\mathbf\phi_k}''(t,t')$.
% 
% 
% 
% 
%% Matching Gradients
% 
% 
% Given the joint distribution over states and their derivatives (3) as well 
% as the ODEs (2), we therefore have two expressions for the state derivatives:
% 
% $\dot{\mathbf{X}} = \mathbf{F} + \mathbf\epsilon_1, \mathbf\epsilon_1 \sim\mathcal{N}\left(\mathbf\epsilon_1;\mathbf{0}, 
% \mathbf{I}\gamma \right)$,
% 
% $\dot{\mathbf{X}} = {'\mathbf{C}_{\mathbf\phi}} \mathbf{C}_{\mathbf\phi}^{-1}~\mathbf{X} 
% + \mathbf\epsilon_2, \mathbf\epsilon_2 \sim\mathcal{N}\left(\mathbf\epsilon_2;\mathbf{0}, 
% \mathbf{A} \right)$,
% 
% where $\mathbf{F} := \mathbf{f}(\mathbf{X},\mathbf\theta)$ and $\mathbf{A} 
% :=\mathbf{C}_{\mathbf\phi}'' -  {'\mathbf{C}_{\mathbf\phi}} \mathbf{C}_{\mathbf\phi}^{-1}\mathbf{C}_{\mathbf\phi}'$ 
% and $\gamma$ is the error variance in the ODEs. Note that, in a deterministic 
% system, the output of the ODEs $\mathbf{F}$ should equal the state derivatives 
% $\dot{\mathbf{X}}$. However, in the first equation above we relax this contraint 
% by adding stochasticity to the state derivatives $\dot{\mathbf{X}}$ in order 
% to compensate for a
% 
% potential model mismatch. The second equation above is obtained by deriving 
% the conditional distribution for $\dot{\mathbf{X}}$ from the joint distribution 
% in equation (3). Equating the two expressions in the equations above we can 
% eliminate the unknown state derivatives $\dot{\mathbf{X}$:
% 
% $\mathbf{F} = {'\mathbf{C}_{\mathbf\phi}} \mathbf{C}_{\mathbf\phi}^{-1} 
% ~\mathbf{X} +\mathbf\epsilon_0 \qquad (4)$,
% 
% with $\mathbf{\epsilon_0} := \mathbf{\epsilon_2} - \mathbf{\epsilon_1}$.
% 
% 
%%
[dC_times_invC,inv_C,A_plus_gamma_inv] = kernel_function(kernel,state,time.est);
%% 
% 
%% Rewrite ODEs as Linear Combination in Parameters
% 
% 
% Since, according to the mass action dynamics (equation 2), the ODEs are 
% _*linear in the parameters *_$\mathbf\theta$ we can rewrite the ODEs in equation 
% (2) as a linear combination in the parameters:
% 
% $\mathbf{B}_{\mathbf{\theta} k} \mathbf{\theta} + \mathbf{b}_{\mathbf{\theta} 
% k} \stackrel{!}{=}\mathbf{f}_k(\mathbf{X},\mathbf{\theta}) \qquad (5)$,
% 
% where matrices $\mathbf{B}_{\mathbf{\theta} k}$ and $\mathbf{b}_{\mathbf{\theta} 
% k}$ and $\mathbf{b}_{\mathbf\theta k}$ are defined such that the ODEs $\mathbf{f}_k(\mathbf{X},\mathbf{\theta})$ 
% are expressed as a linear combination in $\mathbf{\theta}$.
% 
% 
%%
[ode_param.lin_comb.B,ode_param.lin_comb.b] = rewrite_odes_as_linear_combination_in_parameters(ode,symbols);
%% 
% 
%% Posterior over ODE Parameters
% 
% 
% Inserting (5) into (4) and solving for $$\mathbf{\theta}$$ yields:
% 
% $\mathbf{\theta} = \mathbf{B}_{\mathbf{\theta}}^+ \left( {'\mathbf{C}_{\mathbf{\phi}}}\mathbf{C}_{\mathbf{\phi}}^{-1} 
% \mathbf{X} - \mathbf{b}_{\mathbf{\theta}} + \mathbf{\epsilon_0}\right)$,
% 
% where $\mathbf{B}_{\mathbf{\theta}}^+$ denotes the pseudo-inverse of $\mathbf{B}_{\mathbf{\theta}}$. 
% Since $$\mathbf{C}_{\mathbf{\phi}}$$ is block diagonal we can rewrite the expression 
% above as:
% 
% $\mathbf{\theta} = \left( \mathbf{B}_{\mathbf{\theta}}^T \mathbf{B}_{\mathbf{\theta}} 
% \right)^{-1} ~\mathbf{B}_{\mathbf{\theta}}^T  \left( \sum_k {'\mathbf{C}_{\mathbf{\phi}_k}}\mathbf{C}_{\mathbf{\phi}_k}^{-1} 
% \mathbf{X}_k - \mathbf{b}_{\mathbf{\theta} k} + \mathbf{\epsilon_0}^{(k)} \right)\\ 
% ~= \left( \mathbf{B}_{\mathbf{\theta}}^T \mathbf{B}_{\mathbf{\theta}} \right)^{-1} 
% \left(\sum_k \mathbf{B}_{\mathbf{\theta} k}^T \left( {'\mathbf{C}_{\mathbf{\phi}_k}}\mathbf{C}_{\mathbf{\phi}_k}^{-1} 
% \mathbf{X}_k - \mathbf{b}_{\mathbf{\theta} k} +\mathbf{\epsilon_0}^{(k)} \right) 
% \right)$,
% 
% where we subsitute the Moore-Penrose inverse for the pseudo-inverse (i.e. 
% $\mathbf{B}_{\mathbf{\theta}}^+ := \left( \mathbf{B}_{\mathbf{\theta}}^T \mathbf{B}_{\mathbf{\theta}}\right)^{-1} 
% \mathbf{B}_{\mathbf{\theta}}^T$). We can therefore derive the posterior distribution 
% over ODE parameters:
% 
% $p(\mathbf{\theta} \mid \mathbf{X}, \mathbf{\phi}, \gamma) = \mathcal{N}\left(\mathbf{\theta} 
% ; \left( \mathbf{B}_{\mathbf{\theta}}^T\mathbf{B}_{\mathbf{\theta}} \right)^{-1} 
% \left( \sum_k \mathbf{B}_{\mathbf{\theta} k}^T ~\left( {'\mathbf{C}_{\mathbf{\phi} 
% k}} \mathbf{C}_{\mathbf{\phi} k}^{-1} \mathbf{X}_k -\mathbf{b}_{\mathbf{\theta} 
% k} \right) \right), ~ \mathbf{B}_{\mathbf{\theta}}^+ ~(\mathbf{A} + \mathbf{I}\gamma) 
% ~ \mathbf{B}_{\mathbf{\theta}}^{+T}\right) \qquad (6)$.
%% 
% 
%% Rewrite ODEs as Linear Combination in Individual States
% 
% 
% Since, according to the mass action dynamics (equation 2), the ODEs are 
% _*linear in the individual state_* $\mathbf{x}_u$ we can rewrite the ODE $\mathbf{f}_k(\mathbf{X},\mathbf\theta)$ 
% as a linear combination in the individual state $\mathbf{x}_u$:
% 
% $\mathbf{R}_{uk} \mathbf{x}_u + \mathbf{r}_{uk} \stackrel{!}{=}\mathbf{f}_k(\mathbf{X},\mathbf{\theta})$,
% 
% where matrices $ \mathbf{R}_{uk}$ and  $\mathbf{r}_{uk}$ are defined such 
% that the ODE $\mathbf{f}_k(\mathbf{X},\mathbf{\theta})$ is expressed as a linear 
% combination in the individual state $\mathbf{x}_u$.
% 
% 
%%
[state.lin_comb.R,state.lin_comb.r] = rewrite_odes_as_linear_combination_in_ind_states(ode,symbols,coupling_idx.states);
%% 
% 
%% Posterior over Individual States
% 
% 
% Given the linear combination of the ODEs w.r.t. an individual state, we 
% define the matrices $\mathbf{B}_u$ and $\mathbf{b}_u$ such that the expression 
% $$\mathbf{f}(\mathbf{X},\mathbf{\theta}) - {'\mathbf{C}}_{\mathbf{\phi}}\mathbf{C}_{\mathbf{\phi}}^{-1} 
% \mathbf{X}$$ is rewritten as a linear combination in an individual state $\mathbf{x}_u$:
% 
% $\mathbf{B}_{u} \mathbf{x}_u + \mathbf{b}_{u} \stackrel{!}{=}\mathbf{f}(\mathbf{X},\mathbf{\theta}) 
% - {'\mathbf{C}}_{\mathbf{\phi}}\mathbf{C}_{\mathbf{\phi}}^{-1} \mathbf{X} \qquad 
% (7)$.
% 
% Inserting (7) into (4) and solving for $$\mathbf{x}_u$$ yields:
% 
% $$\mathbf{x}_u = \mathbf{B}_{u}^+ \left( \mathbf{\epsilon_0} -\mathbf{b}_{u}\right)$$,
% 
% where $$\mathbf{B}_{u}^+$$ denotes the pseudo-inverse of $$\mathbf{B}_{u}$$. 
% Since $$\mathbf{C}_{\mathbf{\phi}}$$ is block diagonal we can rewrite the expression 
% above as:
% 
% $\mathbf{x}_u = \left( \mathbf{B}_{u} \mathbf{B}_{u}^T \right)^{-1}\mathbf{B}_{u}^T 
% \sum_k \left(\mathbf{\epsilon_0}^{(k)} -\mathbf{b}_{uk} \right)\\ \quad= \left( 
% \mathbf{B}_{u} \mathbf{B}_{u}^T \right)^{-1} \sum_k\mathbf{B}_{uk}^T \left(\mathbf{\epsilon_0}^{(k)} 
% -\mathbf{b}_{uk} \right)$,
% 
% where we subsitute the Moore-Penrose inverse for the pseudo-inverse (i.e. 
% $\mathbf{B}_{\mathbf{\theta}}^+ := \left( \mathbf{B}_{\mathbf{\theta}}^T \mathbf{B}_{\mathbf{\theta}}\right)^{-1} 
% \mathbf{B}_{\mathbf{\theta}}^T$).  We can therefore derive the posterior distribution 
% over an individual state $$\mathbf{x}_{u}$$:
% 
% $p(\mathbf{x}_u \mid \mathbf{X}_{-u}, \mathbf{\phi}, \gamma)= \mathcal{N}\left(\mathbf{x}_u 
% ; \left( \mathbf{B}_{u} \mathbf{B}_{u}^T\right)^{-1} \left( - \sum_k \mathbf{B}_{uk}^T 
% \mathbf{b}_{uk} \right),~\mathbf{B}_{u}^{+} ~ (\mathbf{A} + \mathbf{I}\gamma) 
% ~ \mathbf{B}_u^{+T}\right) \qquad (8)$,
% 
% with $$\mathbf{X}_{-u}$$ denoting the set of all states except state $$\mathbf{x}_u$$. 
%% 
% 
%% Mean-field Variational Inference
% 
% 
% To infer the parameters $$\mathbf{\theta}$$, we want to find the maximum 
% a posteriori estimate (MAP):
% 
% $\mathbf{\theta}^* := \mathrm{arg} \max_{\mathbf{\theta}} ~ \ln p(\mathbf{\theta} 
% \mid\mathbf{Y},\mathbf{\phi},\gamma,\mathbf \sigma)\\ \quad= \mathrm{arg}\max_{\mathbf{\theta}} 
% ~ \ln \int  p(\mathbf{\theta},\mathbf{X} \mid\mathbf{Y},\mathbf{\phi},\gamma,\mathbf\sigma) 
% d\mathbf{X}\\ \quad= \mathrm{arg}\max_{\mathbf{\theta}} ~ \ln \int p(\mathbf{\theta} 
% \mid \mathbf{X},\mathbf{\phi},\gamma)p(\mathbf{X} \mid \mathbf{Y}, \mathbf{\phi},\mathbf\sigma) 
% d\mathbf{X} \qquad(9)$.
% 
% However, the integral above is intractable due to the strong couplings 
% induced by the nonlinear ODEs $$\mathbf{f}$$ which appear in the term $$p(\mathbf{\theta} 
% \mid \mathbf{X},\mathbf{\phi},\gamma)$$.
% 
% We use mean-field variational inference to establish variational lower 
% bounds that are analytically tractable by decoupling state variables from the 
% ODE parameters as well as decoupling the state variables from each other. Note 
% that, since the ODEs described by equation (2) are _*locally linear_*, both 
% conditional distributions $$p(\mathbf{\theta} \mid\mathbf{X},\mathbf{Y},\mathbf{\phi},\gamma,\mathbf\sigma)$$ 
% (equation (6)) and $$p(\mathbf{x}_u \mid \mathbf{\theta},\mathbf{X}_{-u},\mathbf{Y},\mathbf{\phi},\gamma,\mathbf\sigma)$$ 
% (equation (8)) are analytically tractable and Gaussian distributed as mentioned 
% previously. The decoupling is induced by designing a variational distribution 
% $$Q(\mathbf{\theta},\mathbf{X})$$ which is restricted to the family of factorial 
% distributions:
% 
% $$\mathcal{Q} := \bigg{\{} Q : Q(\mathbf{\theta},\mathbf{X}) = q(\mathbf{\theta}) 
% \prod_uq(\mathbf{x}_u) \bigg{\}}$$.
% 
% The particular form of $$q(\mathbf{\theta})$$ and $$q(\mathbf{x}_u)$$ are 
% designed to be Gaussian distributed which places them in the same family as 
% the true full conditional distributions. To find the optimal factorial distribution 
% we minimize the Kullback-Leibler divergence between the variational and the 
% true posterior distribution:
% 
% $$\hat{Q} := \mathrm{arg} \min_{Q(\mathbf{\theta},\mathbf{X}) \in \mathcal{Q}} 
% \mathrm{KL}\left[ Q(\mathbf{\theta},\mathbf{X}) \mid \mid p(\mathbf{\theta},\mathbf{X} 
% \mid\mathbf{Y},\mathbf{\phi}, \gamma,\mathbf\mathbf{\sigma}) \right] \qquad 
% (10)$$,
% 
% where $$\hat{Q}$$ is the proxy distribution. The proxy distribution that 
% minimizes the KL-divergence (10) depends on the true full conditionals and is 
% given by:
% 
% $\hat{q}({\mathbf{\theta}}) \propto \exp \left(~ E_{Q_{-\mathbf{\theta}}} 
% \ln p(\mathbf{\theta} \mid\mathbf{X},\mathbf{Y},\mathbf{\phi},\gamma,\mathbf\mathbf{\sigma}) 
% ~\right) \qquad (11)\\\hat{q}(\mathbf{x}_u) \propto \exp\left( ~ E_{Q_{-u}} 
% \ln p(\mathbf{x}_u\mid \mathbf{\theta}, \mathbf{X}_{-u},\mathbf{Y},\mathbf{\phi},\gamma,\mathbf{\sigma}) 
% ~ \right)\qquad (12)$.
%% 
% 
%% Fitting observations of state trajectories
% 
% 
% We fit the observations of state trajectories by standard GP regression. 
% The data-informed distribution$ $p(\mathbf{X} \mid \mathbf{Y}, \mathbf{\phi},\mathbf\mathbf{\sigma})$$ 
% in euqation (9) can be determined analytically using Gaussian process regression 
% with the GP prior $$p(\mathbf{X} \mid\mathbf{\phi}) = \prod_k \mathcal{N}(\mathbf{x}_k 
% ;\mathbf{0},\mathbf{C}_{\mathbf{\phi}_k})$$:
% 
% $$p(\mathbf{X} \mid \mathbf{Y}, \mathbf{\phi},\gamma) = \prod_k\mathcal{N}(\mathbf{x}_k 
% ;\mathbf\mu_k(\mathbf{y}_k),\mathbf\mathbf{\sigma}_k)$$,
% 
% where $$\mathbf\mu_k(\mathbf{y}_k) := \mathbf{\sigma}_k^{-2} \left(\mathbf{\sigma}_k^{-2}\mathbf{I} 
% + \mathbf{C}_{\mathbf{\phi}_k}^{-1} \right)^{-1} \mathbf{y}_k$$ and $$\mathbf{\sigma}_k 
% ^{-1}:=\mathbf{\sigma}_k^{-2} \mathbf{I} +\mathbf{C}_{\mathbf\mathbf{\phi}_k}^{-1}$$.
% 
% 
%%
[mu,inv_sigma] = fitting_state_observations(inv_C,obs_to_state_relation,simulation,symbols);
%% 
% 
%% Coordinate Ascent Variational Gradient Matching
% 
% 
% We minimize the KL-divergence in equation (10) by coordinate descent (where 
% each step is analytically tractable) by iterating between determining the proxy 
% for the distribution over ODE parameters $$\hat{q}(\mathbf{\theta})$$ and the 
% proxies for the distribution over individual states $$\hat{q}(\mathbf{x}_u)$$.
% 
% 
% 
% * *Initialize the state estimation by the GP regression posterior*
% 
% **
%%
            state.proxy.mean = array2table([time.est',mu],'VariableNames',['time',symbols.state_string]);
%% 
% **
% 
% * *Coordinate ascent*
% 
% **
z=1;
            for i = 1:opt_settings.coord_ascent_numb_iter
%% 
% 
% 
% * *Proxy for ODE parameters*
% 
%                     Expanding the proxy distribution in equation (11) for 
% $$\mathbf{\theta}$$ yields:
% 
%                            $\hat{q}(\mathbf{\mathbf\theta}) \propto \exp 
% \left( ~E_{Q_{-\mathbf\theta}}     \ln p(\mathbf\theta \mid \mathbf{X},\mathbf{Y},\mathbf\phi,\gamma,\mathbf\sigma) 
% ~     \right) \\ \qquad= \exp \left( ~E_{Q_{-\mathbf{\theta}}} \ln \mathcal{N}\left(\mathbf{\theta} 
% ; \left(    \mathbf{B}_{\mathbf{\theta}}^T \mathbf{B}_{\mathbf{\theta}} \right)^{-1} 
% \left( \sum_k    \mathbf{B}_{\mathbf{\theta} k}^T ~ \left( {'\mathbf{C}_{\mathbf{\phi} 
% k}}    \mathbf{C}_{\mathbf{\phi} k}^{-1} \mathbf{X}_k - \mathbf{b}_{\mathbf{\theta} 
% k} \right)    \right), ~ \mathbf{B}_{\mathbf{\theta}}^+ ~ (\mathbf{A} + \mathbf{I}\gamma) 
% ~    \mathbf{B}_{\mathbf{\theta}}^{+T} \right) ~\right)$,
% 
%                     where we substitute $$p(\mathbf{\theta} \mid \mathbf{X},\mathbf{\phi},\gamma)$$ 
% with its density given in equation (6).
% 
% 

             [param_proxy_mean,param_proxy_inv_cov] = proxy_for_ode_parameters(state.proxy.mean{:,symbols.state_string},...
                    dC_times_invC,ode_param.lin_comb,symbols,A_plus_gamma_inv,opt_settings);
%% 
% 
% 
% Intermediate results
% 
% 

            if i==1 || ~mod(i,1)
                plot_results(fig_handle,state.proxy,simulation,param_proxy_mean,plot_handle,symbols,plot_settings,'not_final');
            end           
            
            z = z+1; print(1,['video/demo_' num2str(z) '.png'],'-dpng','-r0');
%% 
% **
% 
% * *Proxy for individual states*
% 
%                 Expanding the proxy distribution in equation (12) over 
% the individual state $$\mathbf{x}_u$$:
% 
%                 $\hat{q}(\mathbf{x}_u) \stackrel{(a)}{\propto} \exp \left( 
% ~ E_{Q_{-u}}  \ln ( p(\mathbf{x}_u \mid \mathbf\theta, \mathbf{X}_{-u},\mathbf\phi,\gamma) 
% p(\mathbf{x}_u  \mid\mathbf{Y},\mathbf\phi,\mathbf\sigma) ) ~ \right)\\ \qquad 
% ~ \stackrel{(b)}{=} \exp\big( ~ E_{Q_{-u}} \ln     \mathcal{N}\left(\mathbf{x}_u 
% ; -\mathbf{B}_{u}^+ \mathbf{b}_u,     ~\mathbf{B}_u^{+} ~ (\mathbf{A} + \mathbf{I}\gamma) 
% ~     \mathbf{B}_u^{+T} \right) + E_{Q_{-u}} \ln    \mathcal{N}\left(\mathbf{x}_u 
% ; \mathbf\mu_u(\mathbf{Y}), \mathbf\Sigma_u    \right) \big)\\ \qquad ~= \exp\big( 
% ~ E_{Q_{-u}} \ln                \mathcal{N}\left(\mathbf{x}_u ; -\mathbf{B}_{u}^+ 
% \mathbf{b}_u,                ~\mathbf{B}_u^{+} ~ (\mathbf{A} + \mathbf{I}\gamma) 
% ~                \mathbf{B}_u^{+T} \right) + E_{Q_{-u}} \ln                \mathcal{N}\left(\mathbf{x}_u 
% ; \mathbf\mu_u(\mathbf{Y}), \mathbf{\sigma}_u                \right) \big)$.
% 
%                 In (a) we decompose the full conditional nto an ODE-informed 
% distribution and a data-informed distribution and in (b) we substitute the ODE-informed 
% distribution $p(\mathbf{x}_u \mid \mathbf\theta, \mathbf{X}_{-u},\mathbf\phi,\gamma)$ 
% with its density given by equation (8).
% 
% 

            [state.proxy.mean{:,symbols.state_string},state.proxy.inv_cov] = proxy_for_ind_states(state.lin_comb,...
                state.proxy.mean{:,symbols.state_string},param_proxy_mean',dC_times_invC,coupling_idx.states,symbols,mu,...
                inv_sigma,simulation.observed_states,A_plus_gamma_inv,opt_settings);
%% 
% 

end
%% 
% **
% 
% Final results
% 
% **
%%
            plot_results(fig_handle,state.proxy,simulation,param_proxy_mean,plot_handle,symbols,plot_settings,'final');
%% 
% 
%% Time Taken
% 
%%
disp(['time taken: ' num2str(toc) ' seconds'])
%% 
% 
%% References
% 
% 
% Gorbach, N.S.  Validation and Inference of Structural Connectivity and 
% Neural Dynamics with MRI data. 2018. ETH Zürich Doctoral Thesis. <https://www.research-collection.ethz.ch/handle/20.500.11850/261734 
% https://www.research-collection.ethz.ch/handle/20.500.11850/261734>. 
% 
% *Gorbach, N.S. , Bauer, S. *and Buhmann, J.M., Scalable Variational Inference 
% for Dynamical Systems. 2017a. Neural Information Processing Systems (NIPS). 
% Link to NIPS paper <https://papers.nips.cc/paper/7066-scalable-variational-inference-for-dynamical-systems.pdf 
% here> and arxiv paper <https://arxiv.org/abs/1705.07079 here>.
% 
% *Bauer, S. , Gorbach, N.S.* and Buhmann, J.M., Efficient and Flexible Inference 
% for Stochastic Differential Equations. 2017b. Neural Information Processing 
% Systems (NIPS). Link to NIPS paper <https://papers.nips.cc/paper/7274-efficient-and-flexible-inference-for-stochastic-systems.pdf 
% here>.
% 
% Wenk, P., Gotovos, A., Bauer, S., Gorbach, N.S., Krause, A. and Buhmann, 
% J.M., Fast Gaussian Process Based Gradient Matching for Parameters Identification 
% in Systems of Nonlinear ODEs. 2018. In submission to Conference on Uncertainty 
% in Artificial Intelligence (UAI). Link to arxiv paper <https://arxiv.org/pdf/1804.04378.pdf 
% here>.
% 
% Calderhead, B., Girolami, M. and Lawrence. N.D., 2002. Accelerating Bayesian 
% inference over nonlinear differential equation models. In Advances in Neural 
% Information Processing Systems (NIPS) . 22.
% 
% 
% 
% The authors in bold font have contributed equally to their respective papers.
##### SOURCE END #####
--></body></html>